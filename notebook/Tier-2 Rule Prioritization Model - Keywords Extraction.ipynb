{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jdv_conn import query_JDV\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.colheader_justify\",\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load kcs and cases from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "kcs_rule_dict = json.load(open( \"../data/kcs_rule_dict.json\"))\n",
    "kcs_string = ', '.join(kcs_rule_dict.keys())\n",
    "#select 10 for tests\n",
    "kcs_string = '740323, 24651, 1614393, 2065483, 797553, 3415331, 3446331, 4455551, 3991451, 29894'\n",
    "kcs_rule_dict = {\"740323\": \"filesystem/mounting_fs_with_errors.py\", \"24651\": \"filesystem/ext3_lookup_logs_unlinked_inode_error.py\", \"1614393\": \"filesystem/xfs_blocksize_issue.py\", \"2065483\": \"filesystem/nfs_fragment_too_large_error.py\", \"797553\": \"filesystem/nt_status_account_locked_out.py\", \"3415331\": \"filesystem/smb2_ntlm_mount_issue.py\", \"3446331\": \"filesystem/system_crash_is_size_safe_to_change.py\", \"4455551\": \"filesystem/smb3_systemd_issue.py\", \"3991451\": \"filesystem/cifs_fips_issue.py\", \"29894\": \"filesystem/ext_dx_add_entry.py\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql =\"\"\"\n",
    "        SELECT kcs.resource_display_id__c, c.casenumber, c.subject, c.description\n",
    "        FROM (SELECT  id, casenumber, subject, description\n",
    "              FROM stg_gss_case\n",
    "              WHERE stg_curr_flg = true\n",
    "                  AND case_language__c='en'\n",
    "                  AND isdeleted = false\n",
    "                  AND ownerid != '00GA0000000XxxNMAS') c\n",
    "            INNER JOIN stg_gss_case_rsrc_rltnshp kcs ON kcs.case__c = c.id\n",
    "        WHERE stg_curr_flg = true\n",
    "            AND isdeleted = false\n",
    "            AND type__c = 'Link'  \n",
    "            AND kcs.resource_display_id__c IN (\"\"\" + kcs_string + \"\"\")\n",
    "        ORDER BY kcs.resource_display_id__c\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = query_JDV(queryString=sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resource_display_id__c</th>\n",
       "      <th>casenumber</th>\n",
       "      <th>subject</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02144975</td>\n",
       "      <td>mount XFS failed: Function not implemented wit...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02168396</td>\n",
       "      <td>Disk partitioning - Failed if I use 16K blocksize</td>\n",
       "      <td>Hi Team,\\n\\nWe have RHEL 7.5 VM and attached m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02532866</td>\n",
       "      <td>We set the block size doesn't reflect to the x...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02410915</td>\n",
       "      <td>Want to know correct syntax to format a logica...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01734684</td>\n",
       "      <td>Failed to mount XFS file system with 16K block...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01906558</td>\n",
       "      <td>Unable to increase XFS block size to 64KB beca...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02073948</td>\n",
       "      <td>mkfs.xfs -f -b size=32k produces \"Function not...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02122142</td>\n",
       "      <td>Unable to mount xfs file system if  the volume...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02468840</td>\n",
       "      <td>Is possible reduce the bs in xfs?</td>\n",
       "      <td>¿Qué problema/comportamiento le está causando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02333745</td>\n",
       "      <td>Partition Table Issue for UEFI and  16 Bloxk Size</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02288182</td>\n",
       "      <td>Kickstart block size</td>\n",
       "      <td>Hi\\nis there a way to set block size (default ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01802828</td>\n",
       "      <td>mount a xfs that has bigger block size than 4K</td>\n",
       "      <td>we are setting a box for a backup agent. the h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01811136</td>\n",
       "      <td>Need Oracle DB on XFS (not ASM) Best Practise ...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02275628</td>\n",
       "      <td>Need to create a XFS FS with 16K block size</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02297397</td>\n",
       "      <td>8k format OS and application related FS</td>\n",
       "      <td>We got the new request from Client , client wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01853201</td>\n",
       "      <td>How can I change the block size to 16Kb in RHE...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02099567</td>\n",
       "      <td>Requirement to increase block size of logical ...</td>\n",
       "      <td>We tried to assign block size of 8192 bytes to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01720996</td>\n",
       "      <td>Oracle database on file system with 8k blocks ...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02192105</td>\n",
       "      <td>cannot mount xfs if 64k block size specified -...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02461025</td>\n",
       "      <td>XFS - RHEL6 blocksize</td>\n",
       "      <td>Hi,\\nwe are not able to mount a XFS FS (blocks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01790915</td>\n",
       "      <td>Can we set block size larger than 4096 for ext...</td>\n",
       "      <td>We experience some performance issue, and we w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01752984</td>\n",
       "      <td>XFS Tuning</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01909456</td>\n",
       "      <td>mount xfs failed in Function not implemented e...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02378022</td>\n",
       "      <td>Page size increase: on rhel cls589</td>\n",
       "      <td>Hi team,\\n\\nwe are planning to increase the bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02475590</td>\n",
       "      <td>Can not mount xfs filesystem after specifying ...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02107908</td>\n",
       "      <td>Change block size of filesystem to 16K</td>\n",
       "      <td>Hi Red Hat Support,\\n\\nSimilar ticket has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1614393</td>\n",
       "      <td>01859381</td>\n",
       "      <td>xfs: mount with 32k blockzise failed: Function...</td>\n",
       "      <td>Was bzw. welches Verhalten bereitet Ihnen Prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02088195</td>\n",
       "      <td>XFS recommended block size for 100TB file syst...</td>\n",
       "      <td>XFS recommended block size for 100TB file syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1614393</td>\n",
       "      <td>02005935</td>\n",
       "      <td>is redhat 7 file system support 64k block size?</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02007816</td>\n",
       "      <td>pldandbd3 kernel: RPC: fragment too large: 131...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01761997</td>\n",
       "      <td>General performance issues on VM eseordmgtapp3/4</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01963131</td>\n",
       "      <td>kernel: FILEACCESS_ERROR   : Failed to find LR...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02382482</td>\n",
       "      <td>Unable to connect with normal user intermitten...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02104404</td>\n",
       "      <td>Fencing Issue in a Production Server</td>\n",
       "      <td>Assistance to determine why a node server in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02433188</td>\n",
       "      <td>On NFS server get RPC: fragment too large mess...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02291885</td>\n",
       "      <td>Soft Lockups</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01936433</td>\n",
       "      <td>rpcbind suddenly dead after running well for a...</td>\n",
       "      <td>rpcbind suddenly died after running well for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02418936</td>\n",
       "      <td>NFS utility hungs when large file being copied</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02166929</td>\n",
       "      <td>please suggest how can i add VMs to export dom...</td>\n",
       "      <td>I have added export domain successfully on RHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02460104</td>\n",
       "      <td>server got hung</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02420696</td>\n",
       "      <td>RPC: fragment too large</td>\n",
       "      <td>Hi\\nNeed help with resolving RPC: fragment too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01870112</td>\n",
       "      <td>server was hung, did power drain to recover it...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02306207</td>\n",
       "      <td>Getting RPC: fragment too large erros in /var/...</td>\n",
       "      <td>We are getting RPC: fragment too large erros i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02280416</td>\n",
       "      <td>We are getting \"Dec 16 08:50:11 tlnckapv0018 k...</td>\n",
       "      <td>We are getting \"Dec 16 08:50:11 tlnckapv0018 k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02510806</td>\n",
       "      <td>NFS share was inaccessible from cluster.</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02453510</td>\n",
       "      <td>Server getting RPC: fragment too large xxx</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02445568</td>\n",
       "      <td>Need RCA for server inaccessible on 3rd Aug at 6:</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02446720</td>\n",
       "      <td>The server vmwplsapp07-prd went hung and had t...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02014278</td>\n",
       "      <td>Repeated \"RPC: fragment too large:\" every 7 days</td>\n",
       "      <td>We are seeing repeated \"RPC: fragment too larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01674498</td>\n",
       "      <td>RPC: fragment too large</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02390587</td>\n",
       "      <td>Bad packet length . Disconnecting: Packet corr...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02388158</td>\n",
       "      <td>We are observing latency issue on our from pro...</td>\n",
       "      <td>We are observing latency issue on our from pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01998237</td>\n",
       "      <td>RPC: fragment too large errors</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02045308</td>\n",
       "      <td>Server unresponsive</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02296545</td>\n",
       "      <td>Server got rebooted automatically. need to kno...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02363970</td>\n",
       "      <td>directory listing under nfs mount of gluster v...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02185835</td>\n",
       "      <td>user's NAS home directories are not getting mo...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02424456</td>\n",
       "      <td>Processo rpciod com consumo elevado de CPU</td>\n",
       "      <td>Que tipo de problema/comportamento você está e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02257895</td>\n",
       "      <td>we are facing the issue with SSH on our Linux ...</td>\n",
       "      <td>We are continuing to have sporadic problems us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02234282</td>\n",
       "      <td>System crashed and a vmcore file was generated</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02150176</td>\n",
       "      <td>NFS overload  Handling  during scan: RPC: frag...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01910584</td>\n",
       "      <td>Regulary getting RPC: fragment too large error...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02280914</td>\n",
       "      <td>syslog server salogp12(NFS client-RHEL5.11) ha...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02257668</td>\n",
       "      <td>NFS version 3 is hanging</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02102349</td>\n",
       "      <td>vendor is reporting they believe they are seei...</td>\n",
       "      <td>We have a VM with RedHat 6.9 installed.  This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02163089</td>\n",
       "      <td>kernel: RPC: fragment too large</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02502364</td>\n",
       "      <td>kernel: nfsd: peername failed (err 107)!</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02002308</td>\n",
       "      <td>fragment too large errors in logs due to that ...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02364016</td>\n",
       "      <td>A NFS that is mounted is causing latency probl...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02171286</td>\n",
       "      <td>Error message in /var/log/messages (INC0043423)</td>\n",
       "      <td>The error message below is seen repeatedly bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02088085</td>\n",
       "      <td>RPC fragment too large</td>\n",
       "      <td>We are seeing an RPC fragment too larger error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02083025</td>\n",
       "      <td>Apr 17 00:18:29 xczzqa0033 kernel: RPC: fragme...</td>\n",
       "      <td>We are receiving 'kernel: RPC: fragment too la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02393181</td>\n",
       "      <td>RPC: fragment too large:</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2065483</td>\n",
       "      <td>02078244</td>\n",
       "      <td>NFS: RPC: fragment too large</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2065483</td>\n",
       "      <td>01947984</td>\n",
       "      <td>Server facing performance issue.</td>\n",
       "      <td>Server facing performance issue, and it is tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>24651</td>\n",
       "      <td>02178396</td>\n",
       "      <td>multiple issues with server</td>\n",
       "      <td>Server is hosted on xen host. We tried booting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>24651</td>\n",
       "      <td>02044499</td>\n",
       "      <td>file system  keeps on getting corrupted</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>24651</td>\n",
       "      <td>01742082</td>\n",
       "      <td>File system frequently going to read-only mode</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>24651</td>\n",
       "      <td>02079037</td>\n",
       "      <td>Cluster Unexpectedly getting Down</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>24651</td>\n",
       "      <td>01715731</td>\n",
       "      <td>Cluster service crashed</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>24651</td>\n",
       "      <td>02040890</td>\n",
       "      <td>The file system became read-only</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>24651</td>\n",
       "      <td>01698639</td>\n",
       "      <td>[FIlesystem entered in read-only] Filesystem e...</td>\n",
       "      <td>[English]\\n\\nServer has a volume from a IBM DS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>24651</td>\n",
       "      <td>01804381</td>\n",
       "      <td>Filesystem showing EXT3-fs error</td>\n",
       "      <td>We are getting the following filesystem error ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>24651</td>\n",
       "      <td>01720553</td>\n",
       "      <td>Customer has a mount that keeps going readonly.</td>\n",
       "      <td>Reboots server and comes up fine, but the moun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>24651</td>\n",
       "      <td>02374105</td>\n",
       "      <td>There Disk Missing</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>24651</td>\n",
       "      <td>02376102</td>\n",
       "      <td>root cause for disk corruption</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>24651</td>\n",
       "      <td>02353987</td>\n",
       "      <td>One of FS goes  in read only mode  frequntly  ...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>24651</td>\n",
       "      <td>02103846</td>\n",
       "      <td>Filesystem goes in Read only mode while perfor...</td>\n",
       "      <td>On server RG546 one of the filesystem goes in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>24651</td>\n",
       "      <td>01791831</td>\n",
       "      <td>file system went to read-only mode</td>\n",
       "      <td>file system went to read-only mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>24651</td>\n",
       "      <td>01781952</td>\n",
       "      <td>Deleted inode error</td>\n",
       "      <td>We are getting  ext3_lookup : deleted inode er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>24651</td>\n",
       "      <td>02399658</td>\n",
       "      <td>We are getting error &gt;&gt;&gt;&gt; device dm-37): ext3_...</td>\n",
       "      <td>Hi Team ,\\n\\nCould you please let us know why ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>24651</td>\n",
       "      <td>02367635</td>\n",
       "      <td>Server is getting hung very frequently, attach...</td>\n",
       "      <td>Team,\\n\\nsever is getting very frequently and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>24651</td>\n",
       "      <td>02069132</td>\n",
       "      <td>Server not booting</td>\n",
       "      <td>We have rebooted the server and it is not comi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>24651</td>\n",
       "      <td>02328123</td>\n",
       "      <td>/ is reached to 100% unable to delete files</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>24651</td>\n",
       "      <td>02099854</td>\n",
       "      <td>EXT3 Filesystem keep on changing to read only ...</td>\n",
       "      <td>Team,\\n   We are facing an issue on below file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>24651</td>\n",
       "      <td>01808816</td>\n",
       "      <td>File system is going in Read only mode</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>24651</td>\n",
       "      <td>01801492</td>\n",
       "      <td>touch: cannot touch `test': Read-only file system</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>24651</td>\n",
       "      <td>02090500</td>\n",
       "      <td>Inodes No.'s are missing of some files &amp; getti...</td>\n",
       "      <td>We are getting continuously inodes Error Logs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>24651</td>\n",
       "      <td>01766714</td>\n",
       "      <td>mount point /var read only fs</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>24651</td>\n",
       "      <td>02304397</td>\n",
       "      <td>filesystem became readonly</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>24651</td>\n",
       "      <td>02299955</td>\n",
       "      <td>unable to rename volume group</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>29894</td>\n",
       "      <td>01860065</td>\n",
       "      <td>RCA from SOSREPORT</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>29894</td>\n",
       "      <td>02402109</td>\n",
       "      <td>continous</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>29894</td>\n",
       "      <td>02035374</td>\n",
       "      <td>We are seeing lot of error message in /var/log...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>29894</td>\n",
       "      <td>01872978</td>\n",
       "      <td>Receiving EXT4-fs warnings in /var/log/messages</td>\n",
       "      <td>We are seeing the following in /var/log/messag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>29894</td>\n",
       "      <td>01688050</td>\n",
       "      <td>We get issues regarding some jobs to be run on...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>29894</td>\n",
       "      <td>01945036</td>\n",
       "      <td>kernel: EXT4-fs warning (device dm-1): ext4_dx...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>29894</td>\n",
       "      <td>01703146</td>\n",
       "      <td>there is large amount of open files on the ser...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>29894</td>\n",
       "      <td>01769617</td>\n",
       "      <td>email-node1 kernel: EXT3-fs warning (device dm...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>29894</td>\n",
       "      <td>02294449</td>\n",
       "      <td>No space left on a device error</td>\n",
       "      <td>Hi there,\\n\\nWe have an issue where users get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>29894</td>\n",
       "      <td>01788530</td>\n",
       "      <td>m222218dbss3001 Server Hung--need Root Cause A...</td>\n",
       "      <td>m222218dbss3001 hung and needed to be power cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>29894</td>\n",
       "      <td>02073758</td>\n",
       "      <td>kernel: EXT4-fs warning (device dm-20): ext4_d...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>29894</td>\n",
       "      <td>02456466</td>\n",
       "      <td>EXT4-fs warning (device dm-326): ext4_dx_add_e...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>29894</td>\n",
       "      <td>01792679</td>\n",
       "      <td>not able to create or delete files under a dir...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>29894</td>\n",
       "      <td>01959878</td>\n",
       "      <td>FileNotFoundException - no space left on device</td>\n",
       "      <td>We are getting the error  java.io.FileNotFound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>29894</td>\n",
       "      <td>02478554</td>\n",
       "      <td>we are seeing XT4-fs warning (device dm-1): ex...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>29894</td>\n",
       "      <td>02003664</td>\n",
       "      <td>ext4_dx_add_entry: Directory index full!</td>\n",
       "      <td>kernel: EXT4-fs warning (device dm-1): ext4_dx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>29894</td>\n",
       "      <td>02124863</td>\n",
       "      <td>Constant \"kernel: EXT4-fs warning (device dm-2...</td>\n",
       "      <td>We are getting a lot of \"kernel: EXT4-fs warni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>29894</td>\n",
       "      <td>01984828</td>\n",
       "      <td>receiveing Nov 29 17:28:23 lrdna1gx kernel: EX...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>29894</td>\n",
       "      <td>01984076</td>\n",
       "      <td>Crashed &amp; came up automatically</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>29894</td>\n",
       "      <td>02051766</td>\n",
       "      <td>we have observed one of mount point provide fr...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>29894</td>\n",
       "      <td>02087164</td>\n",
       "      <td>NFS issue</td>\n",
       "      <td>Dear Concern:\\nFacing NFS issue in server end....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>29894</td>\n",
       "      <td>01717826</td>\n",
       "      <td>Server Got Rebooted - lrau1p17</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>29894</td>\n",
       "      <td>01744009</td>\n",
       "      <td>mv: cannot create regular file `/var/www/html/...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>29894</td>\n",
       "      <td>01997345</td>\n",
       "      <td>kernel: EXT4-fs warning (device dm-4): ext4_dx...</td>\n",
       "      <td>We are getting continues an errors in logs, ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>29894</td>\n",
       "      <td>01705654</td>\n",
       "      <td>OS panic reboot</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>29894</td>\n",
       "      <td>01852969</td>\n",
       "      <td>No space left on device - dayrhedsgp001 - /dsd_0</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>29894</td>\n",
       "      <td>01757039</td>\n",
       "      <td>Performance issue - Hardware error in dmesg</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>29894</td>\n",
       "      <td>02364858</td>\n",
       "      <td>dvice error</td>\n",
       "      <td>I started seeing this error on our server.  Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>29894</td>\n",
       "      <td>02076874</td>\n",
       "      <td>EXT4-fs warning (device dm-2): ext4_dx_add_ent...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>29894</td>\n",
       "      <td>01787276</td>\n",
       "      <td>gpnuatnap01: OS daemons went down including al...</td>\n",
       "      <td>Need to check what have caused the daemons to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>29894</td>\n",
       "      <td>02001007</td>\n",
       "      <td>messages file getting filled</td>\n",
       "      <td>Messages file is getting filled due to below e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>29894</td>\n",
       "      <td>01894225</td>\n",
       "      <td>cannot move no space left on device</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>29894</td>\n",
       "      <td>02271184</td>\n",
       "      <td>dfw-dbblx07c-05 got rebooted and need RCA</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>29894</td>\n",
       "      <td>01894802</td>\n",
       "      <td>The oracle database server crashed suddenly</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>29894</td>\n",
       "      <td>01759421</td>\n",
       "      <td>Unable to create a file OR directory.</td>\n",
       "      <td>It is ext4 filesystem.\\n\\nContact number: +1 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>29894</td>\n",
       "      <td>02326437</td>\n",
       "      <td>Server does not boot. Black screen comes up wi...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>29894</td>\n",
       "      <td>01856932</td>\n",
       "      <td>Dmesg Error_EXT4-fs warning (device dm-57): ex...</td>\n",
       "      <td>Hi Team , \\n\\nWe have observered multiple erro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>29894</td>\n",
       "      <td>01843291</td>\n",
       "      <td>Performance issue on application server</td>\n",
       "      <td>We have one application server on which oracle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>29894</td>\n",
       "      <td>02393633</td>\n",
       "      <td>EXT3-fs (dm-126): warning: ext3_dx_add_entry: ...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>29894</td>\n",
       "      <td>01694970</td>\n",
       "      <td>Application not able to start on dcaldd145 server</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>29894</td>\n",
       "      <td>01706752</td>\n",
       "      <td>No tengo mas inodos libres</td>\n",
       "      <td>Nos muestra error de que no hay mas inodos lib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>29894</td>\n",
       "      <td>01813420</td>\n",
       "      <td>After rebooting server nzxpdb451 database not ...</td>\n",
       "      <td>Getting error as below \\n\\nMar 18 09:32:00 nzx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>29894</td>\n",
       "      <td>02262758</td>\n",
       "      <td>Warning message related to ext4 file system</td>\n",
       "      <td>We can see warning message on the server \\nEXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>29894</td>\n",
       "      <td>02434900</td>\n",
       "      <td>Systems Were Unresponsive</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>29894</td>\n",
       "      <td>01819663</td>\n",
       "      <td>Performance issue (I/O slowness) - lrau1p17 an...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>29894</td>\n",
       "      <td>02373418</td>\n",
       "      <td>Directory Index Full when recovering from backup</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>29894</td>\n",
       "      <td>01923971</td>\n",
       "      <td>Samba service is running &amp; port 445 &amp; 139 was ...</td>\n",
       "      <td>Continuously getting directory index full mess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>29894</td>\n",
       "      <td>01833866</td>\n",
       "      <td>Erro [11596368.850363] EXT4-fs warning (device...</td>\n",
       "      <td>Que tipo de problema/comportamento você está e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>29894</td>\n",
       "      <td>02253193</td>\n",
       "      <td>Recent problem suggesting file system space issue</td>\n",
       "      <td>Developers on our staff related an issue in wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>29894</td>\n",
       "      <td>01791022</td>\n",
       "      <td>rsync is failing with no space left on device</td>\n",
       "      <td>rsync is failing with no space left on device ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>29894</td>\n",
       "      <td>02262694</td>\n",
       "      <td>kernel: EXT3-fs warning (device dm-4): ext3_dx...</td>\n",
       "      <td>We are getting below error in messages logs co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>29894</td>\n",
       "      <td>02000622</td>\n",
       "      <td>EXT4-fs Directory index full! warnings caused ...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>29894</td>\n",
       "      <td>01907700</td>\n",
       "      <td>HIGH Memory utilization reported in the server</td>\n",
       "      <td>HIGH Memory utilization reported in the server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>29894</td>\n",
       "      <td>01947705</td>\n",
       "      <td>Messages logs flooded with warning (device dm-...</td>\n",
       "      <td>The /var/log/messages is flooded with below er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>29894</td>\n",
       "      <td>02340668</td>\n",
       "      <td>Mar 18 07:45:23 betapmdmz4 kernel: EXT4-fs war...</td>\n",
       "      <td>Although the files/dirs. have been cleaned up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>29894</td>\n",
       "      <td>02254034</td>\n",
       "      <td>no space left” error while creating subfolders...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>29894</td>\n",
       "      <td>02425826</td>\n",
       "      <td>System File systems went into read-only mode</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>29894</td>\n",
       "      <td>02495596</td>\n",
       "      <td>/var/log/messages is getting keep growing due ...</td>\n",
       "      <td>/var/log/messages is getting keep growing/full...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>29894</td>\n",
       "      <td>02173889</td>\n",
       "      <td>MPAPSFTLMV04 kernel: EXT4-fs warning (device d...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>29894</td>\n",
       "      <td>02286044</td>\n",
       "      <td>Server rebooted and looking for RCA</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>29894</td>\n",
       "      <td>02466809</td>\n",
       "      <td>EXT4-fs warning (device dm-28): ext4_dx_add_en...</td>\n",
       "      <td>Hello Support,\\nWe  are receiving following ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>29894</td>\n",
       "      <td>02135275</td>\n",
       "      <td>ERROR - Standby Synchro on rdc1vldcora102.d1.a...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>29894</td>\n",
       "      <td>02005835</td>\n",
       "      <td>ext4_dx_add_entry: Directory index full!</td>\n",
       "      <td>[root@crmdbn2 log]# df -TH\\nFilesystem        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>29894</td>\n",
       "      <td>01731431</td>\n",
       "      <td>seeing</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>29894</td>\n",
       "      <td>02201107</td>\n",
       "      <td>Server running with High Load</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>29894</td>\n",
       "      <td>02482059</td>\n",
       "      <td>kernel: EXT4-fs warning (device dm-11): ext4_d...</td>\n",
       "      <td>Getting warning in the logs regarding index fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>29894</td>\n",
       "      <td>02305783</td>\n",
       "      <td>unable to run rm -rf command ext4_dx_add_entry...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>29894</td>\n",
       "      <td>01821317</td>\n",
       "      <td>ext[3/4]_dx_add_entry: Directory index full!</td>\n",
       "      <td>below error getting while file generating thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>29894</td>\n",
       "      <td>01890361</td>\n",
       "      <td>kernel: EXT4-fs warning (device dm-49): ext4_d...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>29894</td>\n",
       "      <td>02326820</td>\n",
       "      <td>Are there limitations on the count of files in...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>29894</td>\n",
       "      <td>02235835</td>\n",
       "      <td>EXT4 Directory index full!</td>\n",
       "      <td>Hello,\\n\\nI have a lot of warnings, related to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>29894</td>\n",
       "      <td>01926822</td>\n",
       "      <td>Server lrau1p18 was hung . Rebooted the manual...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3415331</td>\n",
       "      <td>02493305</td>\n",
       "      <td>nfs mount points and cifs mount points not ava...</td>\n",
       "      <td>¿Qué problema/comportamiento le está causando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3415331</td>\n",
       "      <td>02254090</td>\n",
       "      <td>After patching server went to Emergency mainte...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3415331</td>\n",
       "      <td>02510697</td>\n",
       "      <td>CIFS sec=ntlmv2 mounting issue</td>\n",
       "      <td>while mounting the CIFS share getting permissi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3446331</td>\n",
       "      <td>02480468</td>\n",
       "      <td>CIFS bug (crash in is_size_safe_to_change - bu...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3446331</td>\n",
       "      <td>02165480</td>\n",
       "      <td>[abrt] kernel: general protection fault: 0000 ...</td>\n",
       "      <td>Description of problem:\\nNothing special was r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3446331</td>\n",
       "      <td>02069191</td>\n",
       "      <td>[bz1757872][abrt] kernel: general protection f...</td>\n",
       "      <td>Description of problem:\\nduring compiling/link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3446331</td>\n",
       "      <td>02444742</td>\n",
       "      <td>Kernel panic occurs very night - possibly rela...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3991451</td>\n",
       "      <td>02445634</td>\n",
       "      <td>cifs mount error</td>\n",
       "      <td>Was bzw. welches Verhalten bereitet Ihnen Prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3991451</td>\n",
       "      <td>02232626</td>\n",
       "      <td>[BZ 1710421] CIFS mounts stopped working after...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>740323</td>\n",
       "      <td>02444918</td>\n",
       "      <td>Once of the filesystem is utilizing heavily</td>\n",
       "      <td>Hello team,\\n\\nOnce of the file system /opt/zi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>740323</td>\n",
       "      <td>02047491</td>\n",
       "      <td>server rebooted</td>\n",
       "      <td>Where are you experiencing the behavior?  What...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>740323</td>\n",
       "      <td>02088794</td>\n",
       "      <td>Both servers unable to login</td>\n",
       "      <td>Hi Team,\\n\\nBelow servers unable to login. Che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>740323</td>\n",
       "      <td>02340161</td>\n",
       "      <td>a0310pvasdb01: Filesystem Errors in Logs and H...</td>\n",
       "      <td>We are running a 2 node active-failover cluste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>740323</td>\n",
       "      <td>01757039</td>\n",
       "      <td>Performance issue - Hardware error in dmesg</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>740323</td>\n",
       "      <td>01705652</td>\n",
       "      <td>Strange Errors in message.log.</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>740323</td>\n",
       "      <td>02415747</td>\n",
       "      <td>Possible currupt file system need to run fsck</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>740323</td>\n",
       "      <td>01838579</td>\n",
       "      <td>issue with root file system</td>\n",
       "      <td>The machine went to maintenance mode and i hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>740323</td>\n",
       "      <td>02068652</td>\n",
       "      <td>ログ調査依頼</td>\n",
       "      <td>問題となっている不具合や動作は何ですか? 期待される動作はどのようなものですか?\\n\\n以下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>740323</td>\n",
       "      <td>02420863</td>\n",
       "      <td>NFS mounts cannot mount on boot, dropping host...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>740323</td>\n",
       "      <td>02250747</td>\n",
       "      <td>在menssage有报错， kernel: EXT4-fs (dm-8): warning:...</td>\n",
       "      <td>在menssage有报错，麻烦帮忙看一下，请帮忙看一下是否影响系统</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>740323</td>\n",
       "      <td>02475104</td>\n",
       "      <td>Alex01dc01(Virtual Machine Disk Unavailable)</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>740323</td>\n",
       "      <td>02349010</td>\n",
       "      <td>Unable to extend the fileystem</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>740323</td>\n",
       "      <td>01932366</td>\n",
       "      <td>ext4文件系统报错</td>\n",
       "      <td>\"您遇到了什么问题？您所期望获得的结果是什么？\"\\n\\nEXT4-fs (dm-2): wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>740323</td>\n",
       "      <td>02451641</td>\n",
       "      <td>Server was rebooted on 03rd August 2019 and st...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>740323</td>\n",
       "      <td>01887281</td>\n",
       "      <td>Multiple filesystem errors</td>\n",
       "      <td>dmesg output.\\n\\nXT4-fs error (device dm-2): _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>740323</td>\n",
       "      <td>02493552</td>\n",
       "      <td>/toms_share became read-only</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>740323</td>\n",
       "      <td>01854880</td>\n",
       "      <td>filesystem wen in to readonly, need RCA</td>\n",
       "      <td>Can you please look in to the below issue, as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>740323</td>\n",
       "      <td>02441407</td>\n",
       "      <td>Linux boxes not booting after storage loss</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>740323</td>\n",
       "      <td>02485497</td>\n",
       "      <td>requalar boot fail every weekend</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>740323</td>\n",
       "      <td>02017886</td>\n",
       "      <td>EXT4-fs warning: checktime reached, running e2...</td>\n",
       "      <td>This is the warning message received in dmesg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>740323</td>\n",
       "      <td>01955566</td>\n",
       "      <td>ext4文件系统EXT4-fs error (device dm-4)</td>\n",
       "      <td>Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>740323</td>\n",
       "      <td>01854822</td>\n",
       "      <td>Bad entry in directory (problem in dirs and file)</td>\n",
       "      <td>Displays the Ext4-fs error message (device sc1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>797553</td>\n",
       "      <td>01814280</td>\n",
       "      <td>Remote CIFS directory refuses to mount. Permis...</td>\n",
       "      <td>CIFS mount is no longer working.\\nWhen trying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>797553</td>\n",
       "      <td>02379980</td>\n",
       "      <td>Mount point issue | NT_STATUS_ACCOUNT_LOCKED_OUT</td>\n",
       "      <td>Hello,\\n\\nsosreport of hslep-dbcia server atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>797553</td>\n",
       "      <td>02097524</td>\n",
       "      <td>CIFS Mounts are not mounting automatically aft...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>797553</td>\n",
       "      <td>02088927</td>\n",
       "      <td>CIFS cannot be mounted</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>797553</td>\n",
       "      <td>02464919</td>\n",
       "      <td>Update Kernel Error</td>\n",
       "      <td>¿Qué problema/comportamiento le está causando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>797553</td>\n",
       "      <td>01928324</td>\n",
       "      <td>Unable to mount CIFS share on RHEL 6.x servers</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>797553</td>\n",
       "      <td>01710089</td>\n",
       "      <td>Unable to list the samba share  smbstatus,</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>797553</td>\n",
       "      <td>02200831</td>\n",
       "      <td>Intermittent connection issue while sending th...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>797553</td>\n",
       "      <td>02021034</td>\n",
       "      <td>CIFS umount from particular server</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>797553</td>\n",
       "      <td>02175580</td>\n",
       "      <td>Unable to mount the cifs mount point getting e...</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>797553</td>\n",
       "      <td>02178675</td>\n",
       "      <td>CIFS account gets locked out post reboot</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>797553</td>\n",
       "      <td>01758608</td>\n",
       "      <td>CIFS filesystem inconsistency</td>\n",
       "      <td>What problem/issue/behavior are you having tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>797553</td>\n",
       "      <td>02237349</td>\n",
       "      <td>Montage CIFS fait planter le systeme</td>\n",
       "      <td>Quelle sorte de problème/comportement inattend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    resource_display_id__c casenumber  \\\n",
       "0                  1614393   02144975   \n",
       "1                  1614393   02168396   \n",
       "2                  1614393   02532866   \n",
       "3                  1614393   02410915   \n",
       "4                  1614393   01734684   \n",
       "5                  1614393   01906558   \n",
       "6                  1614393   02073948   \n",
       "7                  1614393   02122142   \n",
       "8                  1614393   02468840   \n",
       "9                  1614393   02333745   \n",
       "10                 1614393   02288182   \n",
       "11                 1614393   01802828   \n",
       "12                 1614393   01811136   \n",
       "13                 1614393   02275628   \n",
       "14                 1614393   02297397   \n",
       "15                 1614393   01853201   \n",
       "16                 1614393   02099567   \n",
       "17                 1614393   01720996   \n",
       "18                 1614393   02192105   \n",
       "19                 1614393   02461025   \n",
       "20                 1614393   01790915   \n",
       "21                 1614393   01752984   \n",
       "22                 1614393   01909456   \n",
       "23                 1614393   02378022   \n",
       "24                 1614393   02475590   \n",
       "25                 1614393   02107908   \n",
       "26                 1614393   01859381   \n",
       "27                 1614393   02088195   \n",
       "28                 1614393   02005935   \n",
       "29                 2065483   02007816   \n",
       "30                 2065483   01761997   \n",
       "31                 2065483   01963131   \n",
       "32                 2065483   02382482   \n",
       "33                 2065483   02104404   \n",
       "34                 2065483   02433188   \n",
       "35                 2065483   02291885   \n",
       "36                 2065483   01936433   \n",
       "37                 2065483   02418936   \n",
       "38                 2065483   02166929   \n",
       "39                 2065483   02460104   \n",
       "40                 2065483   02420696   \n",
       "41                 2065483   01870112   \n",
       "42                 2065483   02306207   \n",
       "43                 2065483   02280416   \n",
       "44                 2065483   02510806   \n",
       "45                 2065483   02453510   \n",
       "46                 2065483   02445568   \n",
       "47                 2065483   02446720   \n",
       "48                 2065483   02014278   \n",
       "49                 2065483   01674498   \n",
       "50                 2065483   02390587   \n",
       "51                 2065483   02388158   \n",
       "52                 2065483   01998237   \n",
       "53                 2065483   02045308   \n",
       "54                 2065483   02296545   \n",
       "55                 2065483   02363970   \n",
       "56                 2065483   02185835   \n",
       "57                 2065483   02424456   \n",
       "58                 2065483   02257895   \n",
       "59                 2065483   02234282   \n",
       "60                 2065483   02150176   \n",
       "61                 2065483   01910584   \n",
       "62                 2065483   02280914   \n",
       "63                 2065483   02257668   \n",
       "64                 2065483   02102349   \n",
       "65                 2065483   02163089   \n",
       "66                 2065483   02502364   \n",
       "67                 2065483   02002308   \n",
       "68                 2065483   02364016   \n",
       "69                 2065483   02171286   \n",
       "70                 2065483   02088085   \n",
       "71                 2065483   02083025   \n",
       "72                 2065483   02393181   \n",
       "73                 2065483   02078244   \n",
       "74                 2065483   01947984   \n",
       "75                   24651   02178396   \n",
       "76                   24651   02044499   \n",
       "77                   24651   01742082   \n",
       "78                   24651   02079037   \n",
       "79                   24651   01715731   \n",
       "80                   24651   02040890   \n",
       "81                   24651   01698639   \n",
       "82                   24651   01804381   \n",
       "83                   24651   01720553   \n",
       "84                   24651   02374105   \n",
       "85                   24651   02376102   \n",
       "86                   24651   02353987   \n",
       "87                   24651   02103846   \n",
       "88                   24651   01791831   \n",
       "89                   24651   01781952   \n",
       "90                   24651   02399658   \n",
       "91                   24651   02367635   \n",
       "92                   24651   02069132   \n",
       "93                   24651   02328123   \n",
       "94                   24651   02099854   \n",
       "95                   24651   01808816   \n",
       "96                   24651   01801492   \n",
       "97                   24651   02090500   \n",
       "98                   24651   01766714   \n",
       "99                   24651   02304397   \n",
       "100                  24651   02299955   \n",
       "101                  29894   01860065   \n",
       "102                  29894   02402109   \n",
       "103                  29894   02035374   \n",
       "104                  29894   01872978   \n",
       "105                  29894   01688050   \n",
       "106                  29894   01945036   \n",
       "107                  29894   01703146   \n",
       "108                  29894   01769617   \n",
       "109                  29894   02294449   \n",
       "110                  29894   01788530   \n",
       "111                  29894   02073758   \n",
       "112                  29894   02456466   \n",
       "113                  29894   01792679   \n",
       "114                  29894   01959878   \n",
       "115                  29894   02478554   \n",
       "116                  29894   02003664   \n",
       "117                  29894   02124863   \n",
       "118                  29894   01984828   \n",
       "119                  29894   01984076   \n",
       "120                  29894   02051766   \n",
       "121                  29894   02087164   \n",
       "122                  29894   01717826   \n",
       "123                  29894   01744009   \n",
       "124                  29894   01997345   \n",
       "125                  29894   01705654   \n",
       "126                  29894   01852969   \n",
       "127                  29894   01757039   \n",
       "128                  29894   02364858   \n",
       "129                  29894   02076874   \n",
       "130                  29894   01787276   \n",
       "131                  29894   02001007   \n",
       "132                  29894   01894225   \n",
       "133                  29894   02271184   \n",
       "134                  29894   01894802   \n",
       "135                  29894   01759421   \n",
       "136                  29894   02326437   \n",
       "137                  29894   01856932   \n",
       "138                  29894   01843291   \n",
       "139                  29894   02393633   \n",
       "140                  29894   01694970   \n",
       "141                  29894   01706752   \n",
       "142                  29894   01813420   \n",
       "143                  29894   02262758   \n",
       "144                  29894   02434900   \n",
       "145                  29894   01819663   \n",
       "146                  29894   02373418   \n",
       "147                  29894   01923971   \n",
       "148                  29894   01833866   \n",
       "149                  29894   02253193   \n",
       "150                  29894   01791022   \n",
       "151                  29894   02262694   \n",
       "152                  29894   02000622   \n",
       "153                  29894   01907700   \n",
       "154                  29894   01947705   \n",
       "155                  29894   02340668   \n",
       "156                  29894   02254034   \n",
       "157                  29894   02425826   \n",
       "158                  29894   02495596   \n",
       "159                  29894   02173889   \n",
       "160                  29894   02286044   \n",
       "161                  29894   02466809   \n",
       "162                  29894   02135275   \n",
       "163                  29894   02005835   \n",
       "164                  29894   01731431   \n",
       "165                  29894   02201107   \n",
       "166                  29894   02482059   \n",
       "167                  29894   02305783   \n",
       "168                  29894   01821317   \n",
       "169                  29894   01890361   \n",
       "170                  29894   02326820   \n",
       "171                  29894   02235835   \n",
       "172                  29894   01926822   \n",
       "173                3415331   02493305   \n",
       "174                3415331   02254090   \n",
       "175                3415331   02510697   \n",
       "176                3446331   02480468   \n",
       "177                3446331   02165480   \n",
       "178                3446331   02069191   \n",
       "179                3446331   02444742   \n",
       "180                3991451   02445634   \n",
       "181                3991451   02232626   \n",
       "182                 740323   02444918   \n",
       "183                 740323   02047491   \n",
       "184                 740323   02088794   \n",
       "185                 740323   02340161   \n",
       "186                 740323   01757039   \n",
       "187                 740323   01705652   \n",
       "188                 740323   02415747   \n",
       "189                 740323   01838579   \n",
       "190                 740323   02068652   \n",
       "191                 740323   02420863   \n",
       "192                 740323   02250747   \n",
       "193                 740323   02475104   \n",
       "194                 740323   02349010   \n",
       "195                 740323   01932366   \n",
       "196                 740323   02451641   \n",
       "197                 740323   01887281   \n",
       "198                 740323   02493552   \n",
       "199                 740323   01854880   \n",
       "200                 740323   02441407   \n",
       "201                 740323   02485497   \n",
       "202                 740323   02017886   \n",
       "203                 740323   01955566   \n",
       "204                 740323   01854822   \n",
       "205                 797553   01814280   \n",
       "206                 797553   02379980   \n",
       "207                 797553   02097524   \n",
       "208                 797553   02088927   \n",
       "209                 797553   02464919   \n",
       "210                 797553   01928324   \n",
       "211                 797553   01710089   \n",
       "212                 797553   02200831   \n",
       "213                 797553   02021034   \n",
       "214                 797553   02175580   \n",
       "215                 797553   02178675   \n",
       "216                 797553   01758608   \n",
       "217                 797553   02237349   \n",
       "\n",
       "                                               subject  \\\n",
       "0    mount XFS failed: Function not implemented wit...   \n",
       "1    Disk partitioning - Failed if I use 16K blocksize   \n",
       "2    We set the block size doesn't reflect to the x...   \n",
       "3    Want to know correct syntax to format a logica...   \n",
       "4    Failed to mount XFS file system with 16K block...   \n",
       "5    Unable to increase XFS block size to 64KB beca...   \n",
       "6    mkfs.xfs -f -b size=32k produces \"Function not...   \n",
       "7    Unable to mount xfs file system if  the volume...   \n",
       "8                    Is possible reduce the bs in xfs?   \n",
       "9    Partition Table Issue for UEFI and  16 Bloxk Size   \n",
       "10                                Kickstart block size   \n",
       "11      mount a xfs that has bigger block size than 4K   \n",
       "12   Need Oracle DB on XFS (not ASM) Best Practise ...   \n",
       "13         Need to create a XFS FS with 16K block size   \n",
       "14             8k format OS and application related FS   \n",
       "15   How can I change the block size to 16Kb in RHE...   \n",
       "16   Requirement to increase block size of logical ...   \n",
       "17   Oracle database on file system with 8k blocks ...   \n",
       "18   cannot mount xfs if 64k block size specified -...   \n",
       "19                               XFS - RHEL6 blocksize   \n",
       "20   Can we set block size larger than 4096 for ext...   \n",
       "21                                          XFS Tuning   \n",
       "22   mount xfs failed in Function not implemented e...   \n",
       "23                  Page size increase: on rhel cls589   \n",
       "24   Can not mount xfs filesystem after specifying ...   \n",
       "25              Change block size of filesystem to 16K   \n",
       "26   xfs: mount with 32k blockzise failed: Function...   \n",
       "27   XFS recommended block size for 100TB file syst...   \n",
       "28     is redhat 7 file system support 64k block size?   \n",
       "29   pldandbd3 kernel: RPC: fragment too large: 131...   \n",
       "30    General performance issues on VM eseordmgtapp3/4   \n",
       "31   kernel: FILEACCESS_ERROR   : Failed to find LR...   \n",
       "32   Unable to connect with normal user intermitten...   \n",
       "33                Fencing Issue in a Production Server   \n",
       "34   On NFS server get RPC: fragment too large mess...   \n",
       "35                                        Soft Lockups   \n",
       "36   rpcbind suddenly dead after running well for a...   \n",
       "37      NFS utility hungs when large file being copied   \n",
       "38   please suggest how can i add VMs to export dom...   \n",
       "39                                     server got hung   \n",
       "40                             RPC: fragment too large   \n",
       "41   server was hung, did power drain to recover it...   \n",
       "42   Getting RPC: fragment too large erros in /var/...   \n",
       "43   We are getting \"Dec 16 08:50:11 tlnckapv0018 k...   \n",
       "44            NFS share was inaccessible from cluster.   \n",
       "45          Server getting RPC: fragment too large xxx   \n",
       "46   Need RCA for server inaccessible on 3rd Aug at 6:   \n",
       "47   The server vmwplsapp07-prd went hung and had t...   \n",
       "48    Repeated \"RPC: fragment too large:\" every 7 days   \n",
       "49                             RPC: fragment too large   \n",
       "50   Bad packet length . Disconnecting: Packet corr...   \n",
       "51   We are observing latency issue on our from pro...   \n",
       "52                      RPC: fragment too large errors   \n",
       "53                                 Server unresponsive   \n",
       "54   Server got rebooted automatically. need to kno...   \n",
       "55   directory listing under nfs mount of gluster v...   \n",
       "56   user's NAS home directories are not getting mo...   \n",
       "57          Processo rpciod com consumo elevado de CPU   \n",
       "58   we are facing the issue with SSH on our Linux ...   \n",
       "59      System crashed and a vmcore file was generated   \n",
       "60   NFS overload  Handling  during scan: RPC: frag...   \n",
       "61   Regulary getting RPC: fragment too large error...   \n",
       "62   syslog server salogp12(NFS client-RHEL5.11) ha...   \n",
       "63                            NFS version 3 is hanging   \n",
       "64   vendor is reporting they believe they are seei...   \n",
       "65                     kernel: RPC: fragment too large   \n",
       "66            kernel: nfsd: peername failed (err 107)!   \n",
       "67   fragment too large errors in logs due to that ...   \n",
       "68   A NFS that is mounted is causing latency probl...   \n",
       "69     Error message in /var/log/messages (INC0043423)   \n",
       "70                              RPC fragment too large   \n",
       "71   Apr 17 00:18:29 xczzqa0033 kernel: RPC: fragme...   \n",
       "72                            RPC: fragment too large:   \n",
       "73                        NFS: RPC: fragment too large   \n",
       "74                    Server facing performance issue.   \n",
       "75                         multiple issues with server   \n",
       "76             file system  keeps on getting corrupted   \n",
       "77      File system frequently going to read-only mode   \n",
       "78                   Cluster Unexpectedly getting Down   \n",
       "79                             Cluster service crashed   \n",
       "80                    The file system became read-only   \n",
       "81   [FIlesystem entered in read-only] Filesystem e...   \n",
       "82                    Filesystem showing EXT3-fs error   \n",
       "83     Customer has a mount that keeps going readonly.   \n",
       "84                                  There Disk Missing   \n",
       "85                      root cause for disk corruption   \n",
       "86   One of FS goes  in read only mode  frequntly  ...   \n",
       "87   Filesystem goes in Read only mode while perfor...   \n",
       "88                  file system went to read-only mode   \n",
       "89                                 Deleted inode error   \n",
       "90   We are getting error >>>> device dm-37): ext3_...   \n",
       "91   Server is getting hung very frequently, attach...   \n",
       "92                                  Server not booting   \n",
       "93         / is reached to 100% unable to delete files   \n",
       "94   EXT3 Filesystem keep on changing to read only ...   \n",
       "95              File system is going in Read only mode   \n",
       "96   touch: cannot touch `test': Read-only file system   \n",
       "97   Inodes No.'s are missing of some files & getti...   \n",
       "98                       mount point /var read only fs   \n",
       "99                          filesystem became readonly   \n",
       "100                      unable to rename volume group   \n",
       "101                                 RCA from SOSREPORT   \n",
       "102                                          continous   \n",
       "103  We are seeing lot of error message in /var/log...   \n",
       "104    Receiving EXT4-fs warnings in /var/log/messages   \n",
       "105  We get issues regarding some jobs to be run on...   \n",
       "106  kernel: EXT4-fs warning (device dm-1): ext4_dx...   \n",
       "107  there is large amount of open files on the ser...   \n",
       "108  email-node1 kernel: EXT3-fs warning (device dm...   \n",
       "109                    No space left on a device error   \n",
       "110  m222218dbss3001 Server Hung--need Root Cause A...   \n",
       "111  kernel: EXT4-fs warning (device dm-20): ext4_d...   \n",
       "112  EXT4-fs warning (device dm-326): ext4_dx_add_e...   \n",
       "113  not able to create or delete files under a dir...   \n",
       "114    FileNotFoundException - no space left on device   \n",
       "115  we are seeing XT4-fs warning (device dm-1): ex...   \n",
       "116           ext4_dx_add_entry: Directory index full!   \n",
       "117  Constant \"kernel: EXT4-fs warning (device dm-2...   \n",
       "118  receiveing Nov 29 17:28:23 lrdna1gx kernel: EX...   \n",
       "119                    Crashed & came up automatically   \n",
       "120  we have observed one of mount point provide fr...   \n",
       "121                                          NFS issue   \n",
       "122                     Server Got Rebooted - lrau1p17   \n",
       "123  mv: cannot create regular file `/var/www/html/...   \n",
       "124  kernel: EXT4-fs warning (device dm-4): ext4_dx...   \n",
       "125                                    OS panic reboot   \n",
       "126   No space left on device - dayrhedsgp001 - /dsd_0   \n",
       "127        Performance issue - Hardware error in dmesg   \n",
       "128                                        dvice error   \n",
       "129  EXT4-fs warning (device dm-2): ext4_dx_add_ent...   \n",
       "130  gpnuatnap01: OS daemons went down including al...   \n",
       "131                       messages file getting filled   \n",
       "132                cannot move no space left on device   \n",
       "133          dfw-dbblx07c-05 got rebooted and need RCA   \n",
       "134        The oracle database server crashed suddenly   \n",
       "135              Unable to create a file OR directory.   \n",
       "136  Server does not boot. Black screen comes up wi...   \n",
       "137  Dmesg Error_EXT4-fs warning (device dm-57): ex...   \n",
       "138            Performance issue on application server   \n",
       "139  EXT3-fs (dm-126): warning: ext3_dx_add_entry: ...   \n",
       "140  Application not able to start on dcaldd145 server   \n",
       "141                         No tengo mas inodos libres   \n",
       "142  After rebooting server nzxpdb451 database not ...   \n",
       "143        Warning message related to ext4 file system   \n",
       "144                          Systems Were Unresponsive   \n",
       "145  Performance issue (I/O slowness) - lrau1p17 an...   \n",
       "146   Directory Index Full when recovering from backup   \n",
       "147  Samba service is running & port 445 & 139 was ...   \n",
       "148  Erro [11596368.850363] EXT4-fs warning (device...   \n",
       "149  Recent problem suggesting file system space issue   \n",
       "150      rsync is failing with no space left on device   \n",
       "151  kernel: EXT3-fs warning (device dm-4): ext3_dx...   \n",
       "152  EXT4-fs Directory index full! warnings caused ...   \n",
       "153     HIGH Memory utilization reported in the server   \n",
       "154  Messages logs flooded with warning (device dm-...   \n",
       "155  Mar 18 07:45:23 betapmdmz4 kernel: EXT4-fs war...   \n",
       "156  no space left” error while creating subfolders...   \n",
       "157       System File systems went into read-only mode   \n",
       "158  /var/log/messages is getting keep growing due ...   \n",
       "159  MPAPSFTLMV04 kernel: EXT4-fs warning (device d...   \n",
       "160                Server rebooted and looking for RCA   \n",
       "161  EXT4-fs warning (device dm-28): ext4_dx_add_en...   \n",
       "162  ERROR - Standby Synchro on rdc1vldcora102.d1.a...   \n",
       "163           ext4_dx_add_entry: Directory index full!   \n",
       "164                                             seeing   \n",
       "165                      Server running with High Load   \n",
       "166  kernel: EXT4-fs warning (device dm-11): ext4_d...   \n",
       "167  unable to run rm -rf command ext4_dx_add_entry...   \n",
       "168       ext[3/4]_dx_add_entry: Directory index full!   \n",
       "169  kernel: EXT4-fs warning (device dm-49): ext4_d...   \n",
       "170  Are there limitations on the count of files in...   \n",
       "171                         EXT4 Directory index full!   \n",
       "172  Server lrau1p18 was hung . Rebooted the manual...   \n",
       "173  nfs mount points and cifs mount points not ava...   \n",
       "174  After patching server went to Emergency mainte...   \n",
       "175                     CIFS sec=ntlmv2 mounting issue   \n",
       "176  CIFS bug (crash in is_size_safe_to_change - bu...   \n",
       "177  [abrt] kernel: general protection fault: 0000 ...   \n",
       "178  [bz1757872][abrt] kernel: general protection f...   \n",
       "179  Kernel panic occurs very night - possibly rela...   \n",
       "180                                   cifs mount error   \n",
       "181  [BZ 1710421] CIFS mounts stopped working after...   \n",
       "182        Once of the filesystem is utilizing heavily   \n",
       "183                                    server rebooted   \n",
       "184                       Both servers unable to login   \n",
       "185  a0310pvasdb01: Filesystem Errors in Logs and H...   \n",
       "186        Performance issue - Hardware error in dmesg   \n",
       "187                     Strange Errors in message.log.   \n",
       "188      Possible currupt file system need to run fsck   \n",
       "189                        issue with root file system   \n",
       "190                                             ログ調査依頼   \n",
       "191  NFS mounts cannot mount on boot, dropping host...   \n",
       "192  在menssage有报错， kernel: EXT4-fs (dm-8): warning:...   \n",
       "193       Alex01dc01(Virtual Machine Disk Unavailable)   \n",
       "194                     Unable to extend the fileystem   \n",
       "195                                         ext4文件系统报错   \n",
       "196  Server was rebooted on 03rd August 2019 and st...   \n",
       "197                         Multiple filesystem errors   \n",
       "198                       /toms_share became read-only   \n",
       "199            filesystem wen in to readonly, need RCA   \n",
       "200         Linux boxes not booting after storage loss   \n",
       "201                   requalar boot fail every weekend   \n",
       "202  EXT4-fs warning: checktime reached, running e2...   \n",
       "203                ext4文件系统EXT4-fs error (device dm-4)   \n",
       "204  Bad entry in directory (problem in dirs and file)   \n",
       "205  Remote CIFS directory refuses to mount. Permis...   \n",
       "206   Mount point issue | NT_STATUS_ACCOUNT_LOCKED_OUT   \n",
       "207  CIFS Mounts are not mounting automatically aft...   \n",
       "208                             CIFS cannot be mounted   \n",
       "209                                Update Kernel Error   \n",
       "210     Unable to mount CIFS share on RHEL 6.x servers   \n",
       "211         Unable to list the samba share  smbstatus,   \n",
       "212  Intermittent connection issue while sending th...   \n",
       "213                 CIFS umount from particular server   \n",
       "214  Unable to mount the cifs mount point getting e...   \n",
       "215           CIFS account gets locked out post reboot   \n",
       "216                      CIFS filesystem inconsistency   \n",
       "217               Montage CIFS fait planter le systeme   \n",
       "\n",
       "                                           description  \n",
       "0    What problem/issue/behavior are you having tro...  \n",
       "1    Hi Team,\\n\\nWe have RHEL 7.5 VM and attached m...  \n",
       "2    What problem/issue/behavior are you having tro...  \n",
       "3    What problem/issue/behavior are you having tro...  \n",
       "4    What problem/issue/behavior are you having tro...  \n",
       "5    What problem/issue/behavior are you having tro...  \n",
       "6    What problem/issue/behavior are you having tro...  \n",
       "7    What problem/issue/behavior are you having tro...  \n",
       "8    ¿Qué problema/comportamiento le está causando ...  \n",
       "9    What problem/issue/behavior are you having tro...  \n",
       "10   Hi\\nis there a way to set block size (default ...  \n",
       "11   we are setting a box for a backup agent. the h...  \n",
       "12   What problem/issue/behavior are you having tro...  \n",
       "13   What problem/issue/behavior are you having tro...  \n",
       "14   We got the new request from Client , client wa...  \n",
       "15   What problem/issue/behavior are you having tro...  \n",
       "16   We tried to assign block size of 8192 bytes to...  \n",
       "17   What problem/issue/behavior are you having tro...  \n",
       "18   What problem/issue/behavior are you having tro...  \n",
       "19   Hi,\\nwe are not able to mount a XFS FS (blocks...  \n",
       "20   We experience some performance issue, and we w...  \n",
       "21   What problem/issue/behavior are you having tro...  \n",
       "22   What problem/issue/behavior are you having tro...  \n",
       "23   Hi team,\\n\\nwe are planning to increase the bl...  \n",
       "24   What problem/issue/behavior are you having tro...  \n",
       "25   Hi Red Hat Support,\\n\\nSimilar ticket has been...  \n",
       "26   Was bzw. welches Verhalten bereitet Ihnen Prob...  \n",
       "27   XFS recommended block size for 100TB file syst...  \n",
       "28   What problem/issue/behavior are you having tro...  \n",
       "29   What problem/issue/behavior are you having tro...  \n",
       "30   What problem/issue/behavior are you having tro...  \n",
       "31   What problem/issue/behavior are you having tro...  \n",
       "32   What problem/issue/behavior are you having tro...  \n",
       "33   Assistance to determine why a node server in p...  \n",
       "34   What problem/issue/behavior are you having tro...  \n",
       "35   What problem/issue/behavior are you having tro...  \n",
       "36   rpcbind suddenly died after running well for a...  \n",
       "37   What problem/issue/behavior are you having tro...  \n",
       "38   I have added export domain successfully on RHE...  \n",
       "39   What problem/issue/behavior are you having tro...  \n",
       "40   Hi\\nNeed help with resolving RPC: fragment too...  \n",
       "41   What problem/issue/behavior are you having tro...  \n",
       "42   We are getting RPC: fragment too large erros i...  \n",
       "43   We are getting \"Dec 16 08:50:11 tlnckapv0018 k...  \n",
       "44   What problem/issue/behavior are you having tro...  \n",
       "45   What problem/issue/behavior are you having tro...  \n",
       "46   What problem/issue/behavior are you having tro...  \n",
       "47   What problem/issue/behavior are you having tro...  \n",
       "48   We are seeing repeated \"RPC: fragment too larg...  \n",
       "49   What problem/issue/behavior are you having tro...  \n",
       "50   What problem/issue/behavior are you having tro...  \n",
       "51   We are observing latency issue on our from pro...  \n",
       "52   What problem/issue/behavior are you having tro...  \n",
       "53   What problem/issue/behavior are you having tro...  \n",
       "54   What problem/issue/behavior are you having tro...  \n",
       "55   What problem/issue/behavior are you having tro...  \n",
       "56   What problem/issue/behavior are you having tro...  \n",
       "57   Que tipo de problema/comportamento você está e...  \n",
       "58   We are continuing to have sporadic problems us...  \n",
       "59   What problem/issue/behavior are you having tro...  \n",
       "60   What problem/issue/behavior are you having tro...  \n",
       "61   What problem/issue/behavior are you having tro...  \n",
       "62   What problem/issue/behavior are you having tro...  \n",
       "63   What problem/issue/behavior are you having tro...  \n",
       "64   We have a VM with RedHat 6.9 installed.  This ...  \n",
       "65   What problem/issue/behavior are you having tro...  \n",
       "66   What problem/issue/behavior are you having tro...  \n",
       "67   What problem/issue/behavior are you having tro...  \n",
       "68   What problem/issue/behavior are you having tro...  \n",
       "69   The error message below is seen repeatedly bet...  \n",
       "70   We are seeing an RPC fragment too larger error...  \n",
       "71   We are receiving 'kernel: RPC: fragment too la...  \n",
       "72   What problem/issue/behavior are you having tro...  \n",
       "73   What problem/issue/behavior are you having tro...  \n",
       "74   Server facing performance issue, and it is tak...  \n",
       "75   Server is hosted on xen host. We tried booting...  \n",
       "76   What problem/issue/behavior are you having tro...  \n",
       "77   What problem/issue/behavior are you having tro...  \n",
       "78   What problem/issue/behavior are you having tro...  \n",
       "79   What problem/issue/behavior are you having tro...  \n",
       "80   What problem/issue/behavior are you having tro...  \n",
       "81   [English]\\n\\nServer has a volume from a IBM DS...  \n",
       "82   We are getting the following filesystem error ...  \n",
       "83   Reboots server and comes up fine, but the moun...  \n",
       "84   What problem/issue/behavior are you having tro...  \n",
       "85   What problem/issue/behavior are you having tro...  \n",
       "86   What problem/issue/behavior are you having tro...  \n",
       "87   On server RG546 one of the filesystem goes in ...  \n",
       "88                  file system went to read-only mode  \n",
       "89   We are getting  ext3_lookup : deleted inode er...  \n",
       "90   Hi Team ,\\n\\nCould you please let us know why ...  \n",
       "91   Team,\\n\\nsever is getting very frequently and ...  \n",
       "92   We have rebooted the server and it is not comi...  \n",
       "93   What problem/issue/behavior are you having tro...  \n",
       "94   Team,\\n   We are facing an issue on below file...  \n",
       "95   What problem/issue/behavior are you having tro...  \n",
       "96   What problem/issue/behavior are you having tro...  \n",
       "97   We are getting continuously inodes Error Logs ...  \n",
       "98   What problem/issue/behavior are you having tro...  \n",
       "99   What problem/issue/behavior are you having tro...  \n",
       "100  What problem/issue/behavior are you having tro...  \n",
       "101  What problem/issue/behavior are you having tro...  \n",
       "102  What problem/issue/behavior are you having tro...  \n",
       "103  What problem/issue/behavior are you having tro...  \n",
       "104  We are seeing the following in /var/log/messag...  \n",
       "105  What problem/issue/behavior are you having tro...  \n",
       "106  What problem/issue/behavior are you having tro...  \n",
       "107  What problem/issue/behavior are you having tro...  \n",
       "108  What problem/issue/behavior are you having tro...  \n",
       "109  Hi there,\\n\\nWe have an issue where users get ...  \n",
       "110  m222218dbss3001 hung and needed to be power cy...  \n",
       "111  What problem/issue/behavior are you having tro...  \n",
       "112  What problem/issue/behavior are you having tro...  \n",
       "113  What problem/issue/behavior are you having tro...  \n",
       "114  We are getting the error  java.io.FileNotFound...  \n",
       "115  What problem/issue/behavior are you having tro...  \n",
       "116  kernel: EXT4-fs warning (device dm-1): ext4_dx...  \n",
       "117  We are getting a lot of \"kernel: EXT4-fs warni...  \n",
       "118  What problem/issue/behavior are you having tro...  \n",
       "119  What problem/issue/behavior are you having tro...  \n",
       "120  What problem/issue/behavior are you having tro...  \n",
       "121  Dear Concern:\\nFacing NFS issue in server end....  \n",
       "122  What problem/issue/behavior are you having tro...  \n",
       "123  What problem/issue/behavior are you having tro...  \n",
       "124  We are getting continues an errors in logs, ke...  \n",
       "125  What problem/issue/behavior are you having tro...  \n",
       "126  What problem/issue/behavior are you having tro...  \n",
       "127  What problem/issue/behavior are you having tro...  \n",
       "128  I started seeing this error on our server.  Wh...  \n",
       "129  What problem/issue/behavior are you having tro...  \n",
       "130  Need to check what have caused the daemons to ...  \n",
       "131  Messages file is getting filled due to below e...  \n",
       "132  What problem/issue/behavior are you having tro...  \n",
       "133  What problem/issue/behavior are you having tro...  \n",
       "134  What problem/issue/behavior are you having tro...  \n",
       "135  It is ext4 filesystem.\\n\\nContact number: +1 4...  \n",
       "136  What problem/issue/behavior are you having tro...  \n",
       "137  Hi Team , \\n\\nWe have observered multiple erro...  \n",
       "138  We have one application server on which oracle...  \n",
       "139  What problem/issue/behavior are you having tro...  \n",
       "140  What problem/issue/behavior are you having tro...  \n",
       "141  Nos muestra error de que no hay mas inodos lib...  \n",
       "142  Getting error as below \\n\\nMar 18 09:32:00 nzx...  \n",
       "143  We can see warning message on the server \\nEXT...  \n",
       "144  What problem/issue/behavior are you having tro...  \n",
       "145  What problem/issue/behavior are you having tro...  \n",
       "146  What problem/issue/behavior are you having tro...  \n",
       "147  Continuously getting directory index full mess...  \n",
       "148  Que tipo de problema/comportamento você está e...  \n",
       "149  Developers on our staff related an issue in wh...  \n",
       "150  rsync is failing with no space left on device ...  \n",
       "151  We are getting below error in messages logs co...  \n",
       "152  What problem/issue/behavior are you having tro...  \n",
       "153     HIGH Memory utilization reported in the server  \n",
       "154  The /var/log/messages is flooded with below er...  \n",
       "155  Although the files/dirs. have been cleaned up ...  \n",
       "156  What problem/issue/behavior are you having tro...  \n",
       "157  What problem/issue/behavior are you having tro...  \n",
       "158  /var/log/messages is getting keep growing/full...  \n",
       "159  What problem/issue/behavior are you having tro...  \n",
       "160  What problem/issue/behavior are you having tro...  \n",
       "161  Hello Support,\\nWe  are receiving following ma...  \n",
       "162  What problem/issue/behavior are you having tro...  \n",
       "163  [root@crmdbn2 log]# df -TH\\nFilesystem        ...  \n",
       "164  What problem/issue/behavior are you having tro...  \n",
       "165  What problem/issue/behavior are you having tro...  \n",
       "166  Getting warning in the logs regarding index fu...  \n",
       "167  What problem/issue/behavior are you having tro...  \n",
       "168  below error getting while file generating thro...  \n",
       "169  What problem/issue/behavior are you having tro...  \n",
       "170  What problem/issue/behavior are you having tro...  \n",
       "171  Hello,\\n\\nI have a lot of warnings, related to...  \n",
       "172  What problem/issue/behavior are you having tro...  \n",
       "173  ¿Qué problema/comportamiento le está causando ...  \n",
       "174  What problem/issue/behavior are you having tro...  \n",
       "175  while mounting the CIFS share getting permissi...  \n",
       "176  What problem/issue/behavior are you having tro...  \n",
       "177  Description of problem:\\nNothing special was r...  \n",
       "178  Description of problem:\\nduring compiling/link...  \n",
       "179  What problem/issue/behavior are you having tro...  \n",
       "180  Was bzw. welches Verhalten bereitet Ihnen Prob...  \n",
       "181  What problem/issue/behavior are you having tro...  \n",
       "182  Hello team,\\n\\nOnce of the file system /opt/zi...  \n",
       "183  Where are you experiencing the behavior?  What...  \n",
       "184  Hi Team,\\n\\nBelow servers unable to login. Che...  \n",
       "185  We are running a 2 node active-failover cluste...  \n",
       "186  What problem/issue/behavior are you having tro...  \n",
       "187  What problem/issue/behavior are you having tro...  \n",
       "188  What problem/issue/behavior are you having tro...  \n",
       "189  The machine went to maintenance mode and i hav...  \n",
       "190  問題となっている不具合や動作は何ですか? 期待される動作はどのようなものですか?\\n\\n以下...  \n",
       "191  What problem/issue/behavior are you having tro...  \n",
       "192                  在menssage有报错，麻烦帮忙看一下，请帮忙看一下是否影响系统  \n",
       "193  What problem/issue/behavior are you having tro...  \n",
       "194  What problem/issue/behavior are you having tro...  \n",
       "195  \"您遇到了什么问题？您所期望获得的结果是什么？\"\\n\\nEXT4-fs (dm-2): wa...  \n",
       "196  What problem/issue/behavior are you having tro...  \n",
       "197  dmesg output.\\n\\nXT4-fs error (device dm-2): _...  \n",
       "198  What problem/issue/behavior are you having tro...  \n",
       "199  Can you please look in to the below issue, as ...  \n",
       "200  What problem/issue/behavior are you having tro...  \n",
       "201  What problem/issue/behavior are you having tro...  \n",
       "202  This is the warning message received in dmesg ...  \n",
       "203  Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error...  \n",
       "204  Displays the Ext4-fs error message (device sc1...  \n",
       "205  CIFS mount is no longer working.\\nWhen trying ...  \n",
       "206  Hello,\\n\\nsosreport of hslep-dbcia server atta...  \n",
       "207  What problem/issue/behavior are you having tro...  \n",
       "208  What problem/issue/behavior are you having tro...  \n",
       "209  ¿Qué problema/comportamiento le está causando ...  \n",
       "210  What problem/issue/behavior are you having tro...  \n",
       "211  What problem/issue/behavior are you having tro...  \n",
       "212  What problem/issue/behavior are you having tro...  \n",
       "213  What problem/issue/behavior are you having tro...  \n",
       "214  What problem/issue/behavior are you having tro...  \n",
       "215  What problem/issue/behavior are you having tro...  \n",
       "216  What problem/issue/behavior are you having tro...  \n",
       "217  Quelle sorte de problème/comportement inattend...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Rank Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sbr_top_issue/3.7/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# skips useless warnings in the pke methods\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "import pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    #Replace newlines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    #Remove these unnecessary info from case description that is only need by the support associates\n",
    "    #For resolution, the issue that customer has is what we need\n",
    "    # We are going to use this additional text info as seperators and replace with some pattern\n",
    "    seps = ['What problem/issue/behavior are you having trouble with?  What do you expect to see?', 'Where are you experiencing the behavior?  What environment?', 'When does the behavior occur? Frequently?  Repeatedly?   At certain times?', 'What information can you provide around timeframes and the business impact?']\n",
    "    for sep in seps:\n",
    "        text = text.replace(sep, '|#|')\n",
    "        text_list = [each.strip() for each in text.split('|#|') if each.strip()]\n",
    "        #print(text_list)\n",
    "        #print(f\"Actual Issue =>>> {text_list[0]}\")\n",
    "        issue = text_list[0]\n",
    "    return issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute single rank algorithm to extract key phrases\n",
    "def keyphrases(text):\n",
    "    \n",
    "    # define the set of valid Part Of Speech tags \n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    \n",
    "    #create a SingleRank extractor\n",
    "    singleRank_extractor = pke.unsupervised.SingleRank()\n",
    "    \n",
    "    # load the content of the document\n",
    "    singleRank_extractor.load_document(input=text, language='en', normalization=None)\n",
    "    \n",
    "    # candidate selection (select the longest sequences of nouns and adjectives as candidates)\n",
    "    singleRank_extractor.candidate_selection(pos)\n",
    "    \n",
    "    # candidate_weighing\n",
    "    # candidate phrases are weighted using sum of their word's scores computed\n",
    "    # using random walk. In graph, nodes are words of certain part-of-speech(nouns & adjectives)\n",
    "    # that are connected if they occur in a window of 10 words\n",
    "    singleRank_extractor.candidate_weighting(window=10, pos=pos)\n",
    "    \n",
    "    # rank the keyphrase and get the 10-higest scored candidates\n",
    "    keyphrases_with_scores = singleRank_extractor.get_n_best(n=10)\n",
    "    phrases = [keyphrase for keyphrase, score in keyphrases_with_scores]\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_phrases(df):\n",
    "    df['subject_key_phrases'] = df['subject'].apply(lambda x: keyphrases(preprocessing(x)))\n",
    "    df['description_key_phrases'] = df['description'].apply(lambda x: keyphrases(preprocessing(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Keywords for cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "df = add_phrases(cases)\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description_key_phrases'] = df.apply(lambda x: x['subject_key_phrases'] + x['description_key_phrases'],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col5 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col1 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col2 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col3 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col5 {\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dc\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >resource_display_id__c</th>        <th class=\"col_heading level0 col1\" >casenumber</th>        <th class=\"col_heading level0 col2\" >subject</th>        <th class=\"col_heading level0 col3\" >description</th>        <th class=\"col_heading level0 col4\" >subject_key_phrases</th>        <th class=\"col_heading level0 col5\" >description_key_phrases</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col0\" class=\"data row0 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col1\" class=\"data row0 col1\" >02144975</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col2\" class=\"data row0 col2\" >mount XFS failed: Function not implemented with block size set to 16K</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col3\" class=\"data row0 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "ds-san-dev-lux-01$ sudo mkfs.xfs -b size=16k /dev/xfs_vg01/xfs_lv01\n",
       "Enter password for yuhuang (QUALPASS):\n",
       "meta-data=/dev/xfs_vg01/xfs_lv01 isize=512    agcount=32, agsize=10485600 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=16384  blocks=335539200, imaxpct=5\n",
       "         =                       sunit=1      swidth=1024 blks\n",
       "naming   =version 2              bsize=16384  ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=16384  blocks=130432, version=2\n",
       "         =                       sectsz=512   sunit=1 blks, lazy-count=1\n",
       "realtime =none                   extsz=16384  blocks=0, rtextents=0\n",
       "ds-san-dev-lux-01$ sudo mkdir /local/syncdata02\n",
       "ds-san-dev-lux-01$ sudo mount /dev/xfs_vg01/xfs_lv01 /local/syncdata02\n",
       "mount: mount /dev/mapper/xfs_vg01-xfs_lv01 on /local/syncdata02 failed: Function not implemented\n",
       "ds-san-dev-lux-01$\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "try to mount XFS with 16k block size.\n",
       "[root@ds-san-dev-lux-01 ~]# cat /etc/release\n",
       "Red Hat Enterprise Linux Server release 7.5 (Maipo)\n",
       "[root@ds-san-dev-lux-01 ~]#\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "try to mount XFS with 16k block size.\n",
       "[root@ds-san-dev-lux-01 ~]# cat /etc/release\n",
       "Red Hat Enterprise Linux Server release 7.5 (Maipo)\n",
       "[root@ds-san-dev-lux-01 ~]#\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "the problem is repeatable.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col4\" class=\"data row0 col4\" >['block size', 'mount xfs', 'function', 'k']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow0_col5\" class=\"data row0 col5\" >['block size', 'mount xfs', 'function', 'k', 'sudo mkfs.xfs -b', 'sudo mount', 'sunit=1 blks', 'syncdata02 mount', 'rtextents=0 ds', 'ftype=1 log', 'internal log', 'sudo mkdir', 'agsize=10485600 blks', 'bsize=16384']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col0\" class=\"data row1 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col1\" class=\"data row1 col1\" >02168396</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col2\" class=\"data row1 col2\" >Disk partitioning - Failed if I use 16K blocksize</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col3\" class=\"data row1 col3\" >Hi Team,\n",
       "\n",
       "We have RHEL 7.5 VM and attached multiple disks. I wanted to partitioning with 64K blocksize for each file system. I am not able to set 64K block size for file system and it is throuwing 'mount failed: Function not implemented' while mounting 64K blksize LVM\n",
       "\n",
       "Please help me to resolve this and provide the steps.\n",
       "\n",
       "Please reach Santosh +61 44 91 91 895 for this issue and updated his mail id.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col4\" class=\"data row1 col4\" >['k blocksize', 'partitioning']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow1_col5\" class=\"data row1 col5\" >['k blocksize', 'partitioning', 'k block size', 'k blksize lvm', 'file system', 'k blocksize', 'santosh +61', 'multiple disks', 'issue', 'able', 'vm', 'rhel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col0\" class=\"data row2 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col1\" class=\"data row2 col1\" >02532866</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col2\" class=\"data row2 col2\" >We set the block size doesn't reflect to the xfs system.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col3\" class=\"data row2 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We set the block size doesn't reflect to the xfs system. \n",
       "\n",
       "[root@cmtoldbcmgpsg01 ~]# /sbin/blockdev --getra /dev/mapper/vg_wi_ssd-lv_data01\n",
       "16384\n",
       "[root@cmtoldbcmgpsg01 ~]#  xfs_info /dev/mapper/vg_wi_ssd-lv_data01\n",
       "meta-data=/dev/mapper/vg_wi_ssd-lv_data01 isize=512    agcount=32, agsize=48017600 blks\n",
       "         =                       sectsz=4096  attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0 spinodes=0\n",
       "data     =                       bsize=4096   blocks=1536563200, imaxpct=5\n",
       "         =                       sunit=64     swidth=256 blks\n",
       "naming   =version 2              bsize=4096   ascii-ci=0 ftype=1\n",
       "log      =internal               bsize=4096   blocks=521728, version=2\n",
       "         =                       sectsz=4096  sunit=1 blks, lazy-count=1\n",
       "realtime =none                   extsz=4096   blocks=0, rtextents=0\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "System slow.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "N/A\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Need to fix it for performance issue asap.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col4\" class=\"data row2 col4\" >['xfs system', 'block size']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow2_col5\" class=\"data row2 col5\" >['xfs system', 'block size', 'ci=0 ftype=1 log', 'finobt=0 spinodes=0 data', 'bsize=4096', 'count=1 realtime', 'sunit=1 blks', 'mapper', 'xfs system', 'lv_data01', 'sectsz=4096', 'blockdev --getra']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col0\" class=\"data row3 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col1\" class=\"data row3 col1\" >02410915</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col2\" class=\"data row3 col2\" >Want to know correct syntax to format a logical volume with xfs filesystem with specific block size</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col3\" class=\"data row3 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Could you please provide us a detailed document on how to create a logical volume with XFS file system with specific block size.\n",
       "\n",
       "When we use the argument -n in the below command, We doubt it has changed block size to 8192.\n",
       "\n",
       "Any filesystem that is formatted with more than 4KB is not getting mounted.\n",
       "\n",
       "cmaprh3 ~]# mkfs -t xfs -f -s size=4096 -b size=8192 /dev/sdh\n",
       "meta-data=/dev/sdh               isize=512    agcount=4, agsize=655360 blks\n",
       "         =                       sectsz=4096  attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=8192   blocks=2621440, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=8192   ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=8192   blocks=1280, version=2\n",
       "         =                       sectsz=4096  sunit=1 blks, lazy-count=1\n",
       "realtime =none                   extsz=8192   blocks=0, rtextents=0\n",
       "[root@usdatcmaprh3 ~]# mount /dev/sdh /dummy/\n",
       "mount: mount /dev/sdh on /dummy failed: Function not implemented\n",
       "[root@usdatcmaprh3 ~]#\n",
       "\n",
       "\n",
       "[root@usdatcmaprh3 ~]# mkfs.xfs -n size=8192 /dev/systemvg/lv_tmp\n",
       "meta-data=/dev/systemvg/lv_tmp   isize=512    agcount=4, agsize=983040 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=4096   blocks=3932160, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=8192   ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=4096   blocks=2560, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=4096   blocks=0, rtextents=0 \n",
       "\n",
       "[root@in2itgvlnxbuild01 ~]# xfs_growfs /\n",
       "meta-data=/dev/mapper/rootvg-root isize=256    agcount=8, agsize=2455296 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=0        finobt=0\n",
       "data     =                       bsize=4096   blocks=18996224, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=4096   ascii-ci=0 ftype=0\n",
       "log      =internal               bsize=4096   blocks=4795, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=4096   blocks=0, rtextents=0\n",
       "data blocks changed from 18996224 to 20306944\n",
       "\n",
       "\n",
       "Please provide us the document at the earliest.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col4\" class=\"data row3 col4\" >['specific block size', 'xfs filesystem', 'logical volume', 'correct syntax']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow3_col5\" class=\"data row3 col5\" >['specific block size', 'xfs filesystem', 'logical volume', 'correct syntax', 'specific block size', 'sdh meta', 'ci=0 ftype=1 log', 'xfs file system', 'block size', 'rtextents=0 data blocks', 'size=4096 -b size=8192', 'mkfs -t xfs', 'internal log', 'count=1 realtime']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col0\" class=\"data row4 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col1\" class=\"data row4 col1\" >01734684</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col2\" class=\"data row4 col2\" >Failed to mount XFS file system with 16K block size on RHEL 7 System</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col3\" class=\"data row4 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hi RH Support Team,\n",
       "\n",
       "We are facing trouble mounting an xfs file system with block size 16k on RHEL 7 System. To get 3PAR Deduplication, we are going with 16k block size. Can we have your assistance on mounting a file system using 16k block size please?\n",
       "\n",
       "Steps performed: \n",
       " \n",
       "Step 1. Below are 2 5 GB Disks presented to Server /dev/sdc1, /dev/sdd1\n",
       "\n",
       "[root@vf2lvnetz01 ~]# fdisk -l /dev/sdc1 /dev/sdd1\n",
       "\n",
       "Disk /dev/sdc1: 5367 MB, 5367660544 bytes, 10483712 sectors\n",
       "Units = sectors of 1 * 512 = 512 bytes\n",
       "Sector size (logical/physical): 512 bytes / 512 bytes\n",
       "I/O size (minimum/optimal): 512 bytes / 512 bytes\n",
       "\n",
       "\n",
       "Disk /dev/sdd1: 5367 MB, 5367660544 bytes, 10483712 sectors\n",
       "Units = sectors of 1 * 512 = 512 bytes\n",
       "Sector size (logical/physical): 512 bytes / 512 bytes\n",
       "I/O size (minimum/optimal): 512 bytes / 512 bytes\n",
       "\n",
       "Step 2. Created Physical Volumes, Volume Groups and Logical Volumes in similar way for both Disks.\n",
       "\n",
       "[root@vf2lvnetz01 ~]# pvcreate /dev/sdc1 ; pvcreate /dev/sdd1 ; vgcreate -s 64m vg02 /dev/sdc1 ; vgcreate -s 64m vg03 /dev/sdd1 ; lvcreate -l 79 -n u01 vg02 ; lvcreate -l 79 -n u02 vg03\n",
       "  Physical volume \"/dev/sdc1\" successfully created\n",
       "  Physical volume \"/dev/sdd1\" successfully created\n",
       "  Volume group \"vg02\" successfully created\n",
       "  Volume group \"vg03\" successfully created\n",
       "  Logical volume \"u01\" created.\n",
       "  Logical volume \"u02\" created.\n",
       "\n",
       "Step 3: Formatted /dev/vg02/u01 as xfs by specifying 16384 (16k) block size. \n",
       "\n",
       "[root@vf2lvnetz01 ~]# mkfs.xfs -b size=16384 /dev/vg02/u01\n",
       "meta-data=/dev/vg02/u01          isize=256    agcount=4, agsize=80896 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=0        finobt=0\n",
       "data     =                       bsize=16384  blocks=323584, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=16384  ascii-ci=0 ftype=0\n",
       "log      =internal log           bsize=16384  blocks=640, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=16384  blocks=0, rtextents=0\n",
       "\n",
       "Step 4: Formatted /dev/vg03/u02 as xfs with default block size.\n",
       "\n",
       "[root@vf2lvnetz01 ~]# mkfs.xfs /dev/vg03/u02\n",
       "meta-data=/dev/vg03/u02          isize=256    agcount=4, agsize=323584 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=0        finobt=0\n",
       "data     =                       bsize=4096   blocks=1294336, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=4096   ascii-ci=0 ftype=0\n",
       "log      =internal log           bsize=4096   blocks=2560, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=4096   blocks=0, rtextents=0\n",
       "\n",
       "\n",
       "Step 5: when trying to mount both file systems, /u02 with default block size mounted fine, but getting error as Function not implemented for 16k block size file system. \n",
       "\n",
       "[root@vf2lvnetz01 ~]# mount /u01 /u02\n",
       "mount:  /u01 is not a block device\n",
       "\n",
       "[root@vf2lvnetz01 ~]# mount -a\n",
       "mount: mount /dev/mapper/vg02-u01 on /u01 failed: Function not implemented\n",
       "\n",
       "We need your urgent help on this as 3PAR suggests to use 16k block size for deduplication and Our Management decided to go with it.\n",
       "\n",
       "Thanks,\n",
       "Ram\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Red Hat Enterprise Linux 7\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Always on RHEL 7\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "Many Server deployments are queued (not delivered to Business) because of this issue and once we release the systems with 4K block size we may not get any downtime to rebuild the systems. We need your urgent help on this as 3PAR recommends to use 16k block size.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col4\" class=\"data row4 col4\" >['xfs file system', 'k block size', 'system', 'rhel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow4_col5\" class=\"data row4 col5\" >['xfs file system', 'k block size', 'system', 'rhel', '16k block size file system', '16k block size', 'block size 16k', 'bytes sector size', 'default block size', 'block size', 'o size', 'xfs file system', 'mount -a mount', 'block device']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col0\" class=\"data row5 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col1\" class=\"data row5 col1\" >01906558</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col2\" class=\"data row5 col2\" >Unable to increase XFS block size to 64KB because of page size limitation</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col3\" class=\"data row5 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We need to set the block size of a XFS filesystem to 64K but it is dependent on the pagesize. But pagesize is limited by kernel which is hardcoded to 4K.\n",
       "\n",
       "We could like to confirm this to ensure there is no alternative work around to achieve this.\n",
       "\n",
       "What is the maximum supported XFS block size in RHEL 7?\n",
       "https://access.redhat.com/solutions/1614393\n",
       "\n",
       "\n",
       "How to invoke CONFIG_PAGE_SIZE_8KB directive in RHEL7?\n",
       "https://access.redhat.com/solutions/3056341\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "We would like to confirm this asap so we can proceed with other options.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col4\" class=\"data row5 col4\" >['xfs block size', 'page size limitation', 'kb', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow5_col5\" class=\"data row5 col5\" >['xfs block size', 'page size limitation', 'kb', 'unable', 'xfs block size', 'config_page_size_8 kb directive', 'block size', 'xfs filesystem', 'alternative work', 'https://access.redhat.com/solutions/1614393', 'pagesize', 'rhel', 'k', 'dependent']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col0\" class=\"data row6 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col1\" class=\"data row6 col1\" >02073948</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col2\" class=\"data row6 col2\" >mkfs.xfs -f -b size=32k produces \"Function not implemented\" on mount</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col3\" class=\"data row6 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "What do we have to do in order to have an xfs block size of 32k mount? We understand that we need to likely set the kernel page size cache limit higher somehow. (getconf PAGE_SIZE is 4k by default)\n",
       "This is what we would like to do for the purpose of Oracle Stripe size: mkfs.xfs -f -b size=32k /dev/mapper/vg_07-oradata06a and then mount. Thank you.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Dev environment currently\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "consistently across file systems\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "No impact as of yet, we are in the deployment stage</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col4\" class=\"data row6 col4\" >['mkfs.xfs -f -b', 'function', 'size=32k', 'mount']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow6_col5\" class=\"data row6 col5\" >['mkfs.xfs -f -b', 'function', 'size=32k', 'mount', 'kernel page size cache', 'mkfs.xfs -f -b size=32k', 'oracle stripe size', 'xfs block size', 'getconf page_size', 'mapper', 'purpose', 'vg_07-oradata06a', '32k', 'order']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col0\" class=\"data row7 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col1\" class=\"data row7 col1\" >02122142</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col2\" class=\"data row7 col2\" >Unable to mount xfs file system if  the volume is  formated with blocksize of  65536</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col3\" class=\"data row7 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Planned to  increase the block size of the xfs file system  to  increase  SAS application performance . but failed to mount it when formatted with 65536 block size.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Certain Time\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "New setup .</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col4\" class=\"data row7 col4\" >['mount xfs file system', 'volume', 'unable', 'blocksize']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow7_col5\" class=\"data row7 col5\" >['mount xfs file system', 'volume', 'unable', 'blocksize', 'xfs file system', 'sas application performance', 'block size']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col0\" class=\"data row8 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col1\" class=\"data row8 col1\" >02468840</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col2\" class=\"data row8 col2\" >Is possible reduce the bs in xfs?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col3\" class=\"data row8 col3\" >¿Qué problema/comportamiento le está causando dificultades? ¿Qué espera ver?\n",
       "\n",
       "The customer need reduce the block size in some file system in RHEL 6, but he need know if is possible and what are the consequences.\n",
       "\n",
       "The environment is RHEL 6 for Oracle Database, but the problem is when restore a backup with commvault over this system the consumption of the IO in the SAN is very high. \n",
       "\n",
       "The SAN  administrator says that the performance can better if the size block is minor to 512 bytes. \n",
       "\n",
       "So the customer want know if is possible reduce without destroy the volumes and if he change the bs what are the consequences.\n",
       "\n",
       "¿En dónde se está presentando el comportamiento? ¿En qué entorno?\n",
       "\n",
       "In RHEL 6 in physical server with 3Par SAN  of HPE.\n",
       "\n",
       "¿Cuándo ocurre este comportamiento? ¿Con frecuencia? ¿Repetidamente? ¿En momentos determinados?\n",
       "\n",
       "Only when restore a backup with commvault.\n",
       "\n",
       "¿Qué información puede brindar acerca de los plazos y el impacto comercial?\n",
       "\n",
       "A impact in the low general performance of the SAN environment.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col4\" class=\"data row8 col4\" >['xfs', 'bs', 'possible']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow8_col5\" class=\"data row8 col5\" >['xfs', 'bs', 'possible', 'de los plazos y el impacto comercial', 'está presentando el comportamiento', 'comportamiento le está causando dificultades', 'qué información puede', 'qué espera ver', 'cuándo ocurre este comportamiento', 'qué entorno', 'qué problema', 'san environment', 'low general performance']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col0\" class=\"data row9 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col1\" class=\"data row9 col1\" >02333745</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col2\" class=\"data row9 col2\" >Partition Table Issue for UEFI and  16 Bloxk Size</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col3\" class=\"data row9 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "vmware admins ask me to format the disks in 16k block size.\n",
       "<%#\n",
       "kind: ptable\n",
       "name: Kickstart default custom\n",
       "oses:\n",
       "- CentOS 5\n",
       "- CentOS 6\n",
       "- CentOS 7\n",
       "- Fedora 16\n",
       "- Fedora 17\n",
       "- Fedora 18\n",
       "- Fedora 19\n",
       "- Fedora 20\n",
       "- RedHat 5\n",
       "- RedHat 6\n",
       "- RedHat 7\n",
       "%>\n",
       "zerombr\n",
       "\n",
       "clearpart --drives=vda --all\n",
       "part /boot --fstype=xfs --size=512 --ondisk=vda --asprimary\n",
       "part pv.01 --size=1024 --grow --ondisk=vda --asprimary\n",
       "volgroup  vg_<%= @host.shortname %> pv.01 \n",
       "logvol / --fstype=xfs --vgname=vg_<%= @host.shortname %> --name=lv_root -b size=16384 --size=6144\n",
       "logvol /var --fstype=xfs --vgname=vg_<%= @host.shortname %> --name=lv_var -b size=16384 --size=2048\n",
       "logvol /home --fstype=xfs --vgname=vg_<%= @host.shortname %> --name=lv_home -b size=16384 --size=2048\n",
       "logvol /tmp --fstype=xfs --vgname=vg_<%= @host.shortname %> --name=lv_tmp -b size=16384 --size=2048\n",
       "logvol swap --fstype=swap --vgname=vg_<%= @host.shortname %> --name=lv_swap01 -b size=16384 --size=2048</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col4\" class=\"data row9 col4\" >['partition table issue', 'bloxk size', 'uefi']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow9_col5\" class=\"data row9 col5\" >['partition table issue', 'bloxk size', 'uefi', 'lv_tmp -b size=16384 --size=2048 logvol swap', '-b size=16384 --size=2048 logvol', 'lv_root -b size=16384 --size=6144 logvol', '@host.shortname %', '--size=2048 logvol', 'lv_var -b size=16384', '%', 'vda --asprimary part', 'size=16384 --size=2048', 'vda --all part']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col0\" class=\"data row10 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col1\" class=\"data row10 col1\" >02288182</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col2\" class=\"data row10 col2\" >Kickstart block size</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col3\" class=\"data row10 col3\" >Hi\n",
       "is there a way to set block size (default is 4k block size) in the partition table in the kickstart? \n",
       "\n",
       "Currently, we using this\n",
       "####### Extract from kickstart - START ####### \n",
       "cat <<EOF > /tmp/diskpart.cfg\n",
       "zerombr\n",
       "clearpart --all --initlabel\n",
       "part /boot --size 1024 --asprimary\n",
       "$EFI_PART\n",
       "part pv.01 --ondisk sda --grow --size=4096\n",
       "volgroup $VG_NAME pv.01\n",
       "logvol swap --name=swap1 --vgname=$VG_NAME --size=$SWAP_SIZE\n",
       "logvol / --name=root --vgname=$VG_NAME --size=4096\n",
       "$SNAP_PART\n",
       "logvol /opt --name=opt --vgname=$VG_NAME --size=4096\n",
       "logvol /home --name=home --vgname=$VG_NAME --size=2048 --fsoptions=\"nodev\"\n",
       "logvol /tmp --name=tmp --vgname=$VG_NAME --size=1024 --maxsize=2048 --grow --fsoptions=\"nodev,nosuid\"\n",
       "logvol /var --name=var --vgname=$VG_NAME --size=2048 --maxsize=8192 --grow\n",
       "logvol /var/log --name=varlog --vgname=$VG_NAME --size=1024 --maxsize=8192 --grow\n",
       "logvol /var/log/audit --name=audit --vgname=$VG_NAME --size=1024 --maxsize=4096 --grow\n",
       "\n",
       "EOF\n",
       "####### Extract from kickstart - STOP ####### \n",
       "\n",
       "Is there a way to specify blocksize to 16K as a parameter? \n",
       "\n",
       "This need to be working for RHEL5, 6, and 7.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col4\" class=\"data row10 col4\" >['kickstart block size']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow10_col5\" class=\"data row10 col5\" >['kickstart block size', 'vg_name pv.01 logvol swap', '--size=4096 logvol', 'snap_part logvol', '4k block size', 'logvol', 'efi_part part pv.01', 'block size', 'diskpart.cfg zerombr clearpart', 'start #', 'partition table']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col0\" class=\"data row11 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col1\" class=\"data row11 col1\" >01802828</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col2\" class=\"data row11 col2\" >mount a xfs that has bigger block size than 4K</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col3\" class=\"data row11 col3\" >we are setting a box for a backup agent. the host has 2 16TB SAN disks and will use them as disk pool for backup. the backup software indicates that those 2 disks are supposed to be formatted as XFA with 64KB as block size.\n",
       "we did it and couldn't mount the  XFS.\n",
       "it errors out as \"failed: Function not implemented\"\n",
       "\n",
       "please advice.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col4\" class=\"data row11 col4\" >['bigger block size', 'xfs', 'k', 'mount']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow11_col5\" class=\"data row11 col5\" >['bigger block size', 'xfs', 'k', 'mount', 'tb san disks', 'backup agent', 'backup software', 'disk pool', 'backup', 'block size', 'disks', 'function', 'xfa', 'host']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col0\" class=\"data row12 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col1\" class=\"data row12 col1\" >01811136</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col2\" class=\"data row12 col2\" >Need Oracle DB on XFS (not ASM) Best Practise and tuning parameters</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col3\" class=\"data row12 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We are setting up a PoC for running a 6TB Oracle DB on an HP DL580 Gen9 with RHEL 7.3.\n",
       "We have to use file systems, not Oracle ASM.\n",
       "I couldn't find any Best Practise or Guide on Oracle DB operation on RHEL 7 with xfs.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col4\" class=\"data row12 col4\" >['best practise', 'oracle db', 'asm', 'xfs', 'parameters']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow12_col5\" class=\"data row12 col5\" >['best practise', 'oracle db', 'asm', 'xfs', 'parameters', 'tb oracle db', 'oracle db operation', 'oracle asm', 'hp dl580 gen9', 'best practise', 'rhel', 'file systems', 'guide', 'xfs', 'poc']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col0\" class=\"data row13 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col1\" class=\"data row13 col1\" >02275628</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col2\" class=\"data row13 col2\" >Need to create a XFS FS with 16K block size</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col3\" class=\"data row13 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "[root@boteusappv1084 ~]# xfs_info /dev/mapper/vg_data-lv_data\n",
       "meta-data=/dev/mapper/vg_data-lv_data isize=512    agcount=4, agsize=133546752 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0 spinodes=0\n",
       "data     =                       bsize=4096   blocks=534187008, imaxpct=5\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=4096   ascii-ci=0 ftype=1\n",
       "log      =internal               bsize=4096   blocks=260833, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=4096   blocks=0, rtextents=0</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col4\" class=\"data row13 col4\" >['k block size', 'xfs fs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow13_col5\" class=\"data row13 col5\" >['k block size', 'xfs fs', 'ci=0 ftype=1 log', 'finobt=0 spinodes=0 data', 'lv_data meta', 'bsize=4096', 'count=1 realtime', 'blks', 'vg_data', 'sectsz=512', 'mapper', 'lv_data']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col0\" class=\"data row14 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col1\" class=\"data row14 col1\" >02297397</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col2\" class=\"data row14 col2\" >8k format OS and application related FS</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col3\" class=\"data row14 col3\" >We got the new request from Client , client want OS and non OS related FS should 8 K format.\n",
       "\n",
       "Could you please provide the steps to create the server with 8 K format,</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col4\" class=\"data row14 col4\" >['format os', 'fs', 'application']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow14_col5\" class=\"data row14 col5\" >['format os', 'fs', 'application', 'non os related fs', 'k format', 'os', 'client', 'new request', 'steps', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col0\" class=\"data row15 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col1\" class=\"data row15 col1\" >01853201</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col2\" class=\"data row15 col2\" >How can I change the block size to 16Kb in RHEL 7.3</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col3\" class=\"data row15 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "This is a general question and no sosreport is required for this issue.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "NA\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "NA\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "NA</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col4\" class=\"data row15 col4\" >['block size', 'rhel', '16kb']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow15_col5\" class=\"data row15 col5\" >['block size', 'rhel', '16kb', 'general question', 'issue', 'sosreport']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col0\" class=\"data row16 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col1\" class=\"data row16 col1\" >02099567</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col2\" class=\"data row16 col2\" >Requirement to increase block size of logical volume</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col3\" class=\"data row16 col3\" >We tried to assign block size of 8192 bytes to a logical volume during FS creation, but unable to mount them.\n",
       "Could you please suggest us to  mount FS of 8192 bytes block size?\n",
       "\n",
       "\n",
       "\n",
       "[root@argsapprdnls1 ~]# mkfs.xfs -b size=8192 /dev/mapper/vg02-sdisk01vol\n",
       "mkfs.xfs: /dev/mapper/vg02-sdisk01vol appears to contain an existing filesystem (xfs).\n",
       "mkfs.xfs: Use the -f option to force overwrite.\n",
       "[root@argsapprdnls1 ~]# mkfs.xfs -f -b size=8192 /dev/mapper/vg02-sdisk01vol\n",
       "meta-data=/dev/mapper/vg02-sdisk01vol isize=512    agcount=4, agsize=4194304 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=8192   blocks=16777216, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=8192   ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=8192   blocks=8192, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=8192   blocks=0, rtextents=0\n",
       "\n",
       "\n",
       "[root@argsapprdnls1 ~]# mount /dev/mapper/vg02-sdisk01vol /mnt\n",
       "mount: mount /dev/mapper/vg02-sdisk01vol on /mnt failed: Function not implemented</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col4\" class=\"data row16 col4\" >['logical volume', 'block size', 'requirement']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow16_col5\" class=\"data row16 col5\" >['logical volume', 'block size', 'requirement', 'mkfs.xfs -f -b size=8192', 'mkfs.xfs -b size=8192', 'ci=0 ftype=1 log', 'bytes block size', 'vg02-sdisk01vol isize=512', 'mount fs', 'mapper', 'internal log', 'vg02-sdisk01vol', 'bsize=8192']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col0\" class=\"data row17 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col1\" class=\"data row17 col1\" >01720996</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col2\" class=\"data row17 col2\" >Oracle database on file system with 8k blocks in x86_64 hardware</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col3\" class=\"data row17 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We are trying to find the most optimal file system setting for Oracle databases on x86_64 hardware. Oracle consultants suggest that we use 8k block sizes for our file systems.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Mounting a file system with block sizes that exceed the page size (4k) is not supported on stock RHEL7 kernels for x86_64 hardware. Still, as a test we configured an XFS file system with 8k blocks on a standard RHEL7 server on x86_64. We would like to know what would be the most optimal setting in file system, kernel, etc. to be able to best support Oracle databases on RHEL7. Should we align the file system to the internal format of the database? Is it at all possible to use 8k blocks on x86_64 hardware? Any other advice regarding this issue is also greatly appreciated.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col4\" class=\"data row17 col4\" >['file system', 'x86_64 hardware', 'oracle database', 'blocks']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow17_col5\" class=\"data row17 col5\" >['file system', 'x86_64 hardware', 'oracle database', 'blocks', 'optimal file system', 'oracle databases', 'oracle consultants', 'x86_64 hardware', 'file systems', 'block sizes']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col0\" class=\"data row18 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col1\" class=\"data row18 col1\" >02192105</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col2\" class=\"data row18 col2\" >cannot mount xfs if 64k block size specified - error function not implemented</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col3\" class=\"data row18 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Below is the testing I went through when mounting xfs with 64k blocak size failed.\n",
       "Reconfigured with default 4k and it works, redo with 64k and fails:\n",
       "[root@cvmatpg ~]# mount /dev/cv_cvmatpg_disk000/mnt_disk000 /mnt/disk000\n",
       "mount: mount /dev/mapper/cv_cvmatpg_disk000-mnt_disk000 on /mnt/disk000 failed: Function not implemented\n",
       "[root@cvmatpg ~]# xfs_repair /dev/cv_cvmatpg_disk000/mnt_disk000\n",
       "Phase 1 - find and verify superblock...\n",
       "Phase 2 - using internal log\n",
       "        - zero log...\n",
       "        - scan filesystem freespace and inode maps...\n",
       "        - found root inode chunk\n",
       "Phase 3 - for each AG...\n",
       "        - scan and clear agi unlinked lists...\n",
       "        - process known inodes and perform inode discovery...\n",
       "        - agno = 0\n",
       "        - agno = 1\n",
       "        - agno = 2\n",
       "        - agno = 3\n",
       "        - process newly discovered inodes...\n",
       "Phase 4 - check for duplicate blocks...\n",
       "        - setting up duplicate extent list...\n",
       "        - check for inodes claiming duplicate blocks...\n",
       "        - agno = 1\n",
       "        - agno = 0\n",
       "        - agno = 2\n",
       "        - agno = 3\n",
       "Phase 5 - rebuild AG headers and trees...\n",
       "        - reset superblock...\n",
       "Phase 6 - check inode connectivity...\n",
       "        - resetting contents of realtime bitmap and summary inodes\n",
       "        - traversing filesystem ...\n",
       "        - traversal finished ...\n",
       "        - moving disconnected inodes to lost+found ...\n",
       "Phase 7 - verify and correct link counts...\n",
       "done\n",
       "[root@cvmatpg ~]# mount /dev/cv_cvmatpg_disk000/mnt_disk000 /mnt/disk000\n",
       "mount: mount /dev/mapper/cv_cvmatpg_disk000-mnt_disk000 on /mnt/disk000 failed: Function not implemented\n",
       "[root@cvmatpg ~]# fsck.xfs /dev/cv_cvmatpg_disk000/mnt_disk000\n",
       "If you wish to check the consistency of an XFS filesystem or\n",
       "repair a damaged filesystem, see xfs_repair(8).\n",
       "[root@cvmatpg ~]# mkfs.xfs /dev/cv_cvmatpg_disk000/mnt_disk000\n",
       "mkfs.xfs: /dev/cv_cvmatpg_disk000/mnt_disk000 appears to contain an existing filesystem (xfs).\n",
       "mkfs.xfs: Use the -f option to force overwrite.\n",
       "[root@cvmatpg ~]# mkfs.xfs -f /dev/cv_cvmatpg_disk000/mnt_disk000\n",
       "meta-data=/dev/cv_cvmatpg_disk000/mnt_disk000 isize=512    agcount=4, agsize=71565056 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=4096   blocks=286260224, imaxpct=5\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=4096   ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=4096   blocks=139775, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=4096   blocks=0, rtextents=0\n",
       "[root@cvmatpg ~]# mount /dev/cv_cvmatpg_disk000/mnt_disk000 /mnt/disk000\n",
       "[root@cvmatpg ~]# umount /dev/cv_cvmatpg_disk000/mnt_disk000 /mnt/disk000\n",
       "umount: /mnt/disk000: not mounted\n",
       "[root@cvmatpg ~]# mkfs.xfs -f -b size=64k /dev/cv_cvmatpg_disk000/mnt_disk000\n",
       "meta-data=/dev/cv_cvmatpg_disk000/mnt_disk000 isize=512    agcount=4, agsize=4472816 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=65536  blocks=17891264, imaxpct=5\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=65536  ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=65536  blocks=8735, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=65536  blocks=0, rtextents=0\n",
       "[root@cvmatpg ~]# umount /dev/cv_cvmatpg_disk000/mnt_disk000 /mnt/disk000\n",
       "umount: /dev/cv_cvmatpg_disk000/mnt_disk000: not mounted\n",
       "umount: /mnt/disk000: not mounted\n",
       "[root@cvmatpg ~]# mount /dev/cv_cvmatpg_disk000/mnt_disk000 /mnt/disk000\n",
       "mount: mount /dev/mapper/cv_cvmatpg_disk000-mnt_disk000 on /mnt/disk000 failed: Function not implemented\n",
       "[root@cvmatpg ~]#</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col4\" class=\"data row18 col4\" >['error function', 'block size', 'xfs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow18_col5\" class=\"data row18 col5\" >['error function', 'block size', 'xfs', 'root inode chunk phase', 'ci=0 ftype=1 log', 'check inode connectivity', 'internal log', 'mkfs.xfs -f -b size=64k', 'disk000 mount', '# mount', 'cv_cvmatpg_disk000', 'scan filesystem freespace', 'xfs filesystem']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col0\" class=\"data row19 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col1\" class=\"data row19 col1\" >02461025</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col2\" class=\"data row19 col2\" >XFS - RHEL6 blocksize</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col3\" class=\"data row19 col3\" >Hi,\n",
       "we are not able to mount a XFS FS (blocksize 64K)\n",
       "\n",
       "The block size (64K) is an application requirement.\n",
       "\n",
       "------------\n",
       "[root@esesbedb3 ~]# mount /dev/mapper/netbackup-lv_msdp /MSDP\n",
       "\n",
       "mount: Function not implemented\n",
       "\n",
       " \n",
       "\n",
       "meta-data=/dev/mapper/netbackup-lv_msdp isize=256    agcount=32, agsize=16777215 blks\n",
       "\n",
       "         =                       sectsz=512   attr=2, projid32bit=0\n",
       "\n",
       "data     =                       bsize=65536  blocks=536870784, imaxpct=5\n",
       "\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "\n",
       "naming   =version 2              bsize=65536  ascii-ci=0\n",
       "\n",
       "log      =internal log           bsize=65536  blocks=32608, version=2\n",
       "\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "\n",
       "realtime =none                   extsz=65536  blocks=0, rtextents=0\n",
       "------------\n",
       "\n",
       "Could you please support us to implement this type of configuration?\n",
       "\n",
       "Thanks,\n",
       "Paolo.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col4\" class=\"data row19 col4\" >['rhel6 blocksize', 'xfs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow19_col5\" class=\"data row19 col5\" >['rhel6 blocksize', 'xfs', 'sunit=0 blks', '# mount', 'lv_msdp /msdp', 'internal log', 'block size', 'application requirement', 'sunit=0', 'xfs fs', 'mapper', 'sectsz=512']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col0\" class=\"data row20 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col1\" class=\"data row20 col1\" >01790915</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col2\" class=\"data row20 col2\" >Can we set block size larger than 4096 for ext4 filesystem?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col3\" class=\"data row20 col3\" >We experience some performance issue, and we would like to rebuild ext4 filesystem with block size larger than 4096.\n",
       "\n",
       "Can we do so?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col4\" class=\"data row20 col4\" >['block size larger', 'ext4 filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow20_col5\" class=\"data row20 col5\" >['block size larger', 'ext4 filesystem', 'block size larger', 'ext4 filesystem', 'performance issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col0\" class=\"data row21 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col1\" class=\"data row21 col1\" >01752984</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col2\" class=\"data row21 col2\" >XFS Tuning</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col3\" class=\"data row21 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "On a linux bare-metal system (cisco ucs blade b200m4) we have 4 iscsi luns which which are added to one volumegroup. We create a logical volume with stripe 4 and stripesize 64. On top we create the xfs filesystem with -d su=64k -d sw=4.  For max performance should we also set XFS blocksize to 64k? The hosted application is SAS 9.4 with a preferred blocksize of 64k.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col4\" class=\"data row21 col4\" >['xfs tuning']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow21_col5\" class=\"data row21 col5\" >['xfs tuning', '-d su=64k -d sw=4', 'xfs blocksize', 'blade b200m4', 'xfs filesystem', 'max performance', 'metal system', 'linux bare', 'preferred blocksize', 'iscsi luns', 'logical volume']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col0\" class=\"data row22 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col1\" class=\"data row22 col1\" >01909456</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col2\" class=\"data row22 col2\" >mount xfs failed in Function not implemented error.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col3\" class=\"data row22 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hi Support\n",
       "As below part it mount fail within change the block size to 16K.\n",
       "[root@rhel74 ~]# mkfs.xfs -b size=16384 /dev/sdb1 -f\n",
       "meta-data=/dev/sdb1              isize=512    agcount=4, agsize=16368 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=16384  blocks=65472, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=16384  ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=16384  blocks=512, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=16384  blocks=0, rtextents=0\n",
       "[root@rhel74 ~]# mount /dev/sdb1 /mnt/\n",
       "mount: mount /dev/sdb1 on /mnt failed: Function not implemented\n",
       "\n",
       "Thanks / Andre</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col4\" class=\"data row22 col4\" >['mount xfs', 'error', 'function']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow22_col5\" class=\"data row22 col5\" >['mount xfs', 'error', 'function', '# mount', 'mkfs.xfs -b size=16384', 'sdb1', 'ftype=1 log', 'internal log', 'bsize=16384', 'mount', 'count=1 realtime', 'sparse=0 data', 'block size']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col0\" class=\"data row23 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col1\" class=\"data row23 col1\" >02378022</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col2\" class=\"data row23 col2\" >Page size increase: on rhel cls589</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col3\" class=\"data row23 col3\" >Hi team,\n",
       "\n",
       "we are planning to increase the block size of Database FS from default block size of 4 Kb to 8kB.\n",
       "\n",
       "How ever PAGESIZE limitation its not allowing us to do.\n",
       "\n",
       "Need to know:\n",
       "1. Is it advisable to increase the PAGESIZE?\n",
       "2. Is it advisable to increase the block size while creating the FS?\n",
       "3. Is it advisable to have multiple Block size FS running on same server ?\n",
       "\n",
       "What is the impact if we need to update this ?\n",
       "Regards,\n",
       "Nikil.George</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col4\" class=\"data row23 col4\" >['page size increase', 'rhel cls589']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow23_col5\" class=\"data row23 col5\" >['page size increase', 'rhel cls589', 'multiple block size fs', 'default block size', 'block size', 'database fs', 'fs', 'same server', 'advisable', 'pagesize limitation', 'regards', 'kb']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col0\" class=\"data row24 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col1\" class=\"data row24 col1\" >02475590</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col2\" class=\"data row24 col2\" >Can not mount xfs filesystem after specifying block size</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col3\" class=\"data row24 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "After creating xfs file system with this command  \"mkfs -t xfs -f -s size=4096 -b size=65536  /dev/sdb1\" ,  mounting /dev/sdb1 failed with this message \"mount: mount /dev/sdb1 on /abc failed: Function not implemented\"\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Mount point can not be created, so installation can not continue</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col4\" class=\"data row24 col4\" >['block size', 'xfs filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow24_col5\" class=\"data row24 col5\" >['block size', 'xfs filesystem', 'mkfs -t xfs -f', 'xfs file system', 'size=4096 -b size=65536', 'sdb1', 'command', 'mount', 'message', 'function']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col0\" class=\"data row25 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col1\" class=\"data row25 col1\" >02107908</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col2\" class=\"data row25 col2\" >Change block size of filesystem to 16K</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col3\" class=\"data row25 col3\" >Hi Red Hat Support,\n",
       "\n",
       "Similar ticket has been raised, please refer ticket https://access.redhat.com/support/cases/#/case/02037718 for details. We wish to change the block size of the file system to 16K (including /boot & other core file systems) without destroying data.\n",
       "\n",
       "Attempting to use RHEL 6 & 7 installation iso to boot in rescue mode, umount file systems then use \"blockdev --setbsz 16384 /dev/sda1\" could not change the block size. I was also unable to umount /mnt/sysimage.\n",
       "\n",
       "Please assist and provide the commands that need to change the block size of the file system.\n",
       "\n",
       "Thanks & Regards,\n",
       "Loi Nguyen</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col4\" class=\"data row25 col4\" >['change block size', 'k', 'filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow25_col5\" class=\"data row25 col5\" >['change block size', 'k', 'filesystem', 'other core file systems', 'umount file systems', 'file system', 'block size', 'similar ticket', 'red hat support', 'ticket https://access.redhat.com/support/cases/#/case/02037718', 'rescue mode', 'installation iso', 'blockdev --setbsz']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col0\" class=\"data row26 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col1\" class=\"data row26 col1\" >01859381</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col2\" class=\"data row26 col2\" >xfs: mount with 32k blockzise failed: Function not implemented</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col3\" class=\"data row26 col3\" >Was bzw. welches Verhalten bereitet Ihnen Probleme? Was ist dabei unerwartet?\n",
       "\n",
       "we have created one lvs with 32KB as blocksize. We use XFS. We cann't mount that FS.\n",
       "\n",
       "[root@dev-dks1-db2-v02 ~]# mkfs.xfs -b size=32768 /dev/mapper/fau_dks-fau_dks -f\n",
       "meta-data=/dev/mapper/fau_dks-fau_dks isize=512    agcount=4, agsize=491488 blks\n",
       "         =                       sectsz=512   attr=2, projid32bit=1\n",
       "         =                       crc=1        finobt=0, sparse=0\n",
       "data     =                       bsize=32768  blocks=1965952, imaxpct=25\n",
       "         =                       sunit=0      swidth=0 blks\n",
       "naming   =version 2              bsize=32768  ascii-ci=0 ftype=1\n",
       "log      =internal log           bsize=32768  blocks=959, version=2\n",
       "         =                       sectsz=512   sunit=0 blks, lazy-count=1\n",
       "realtime =none                   extsz=32768  blocks=0, rtextents=0\n",
       "[root@dev-dks1-db2-v02 ~]# mount /dev/mapper/fau_dks-fau_dks\n",
       "mount: mount /dev/mapper/fau_dks-fau_dks on /opt/schufa/db2/fau_dks failed: Function not implemented\n",
       "\n",
       "How can we mount that FS. For DB we must use a blocksize more as 4KB.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col4\" class=\"data row26 col4\" >['32k blockzise', 'function', 'mount', 'xfs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow26_col5\" class=\"data row26 col5\" >['32k blockzise', 'function', 'mount', 'xfs', 'fau_dks mount', 'fau_dks isize=512', 'verhalten bereitet ihnen probleme', 'fau_dks', 'ci=0 ftype=1 log', 'ist dabei unerwartet', 'internal log', 'bsize=32768', 'mapper', '# mount']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col0\" class=\"data row27 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col1\" class=\"data row27 col1\" >02088195</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col2\" class=\"data row27 col2\" >XFS recommended block size for 100TB file system size</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col3\" class=\"data row27 col3\" >XFS recommended block size for 100TB file system size</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col4\" class=\"data row27 col4\" >['tb file system size', 'block size', 'xfs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow27_col5\" class=\"data row27 col5\" >['tb file system size', 'block size', 'xfs', 'tb file system size', 'block size', 'xfs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col0\" class=\"data row28 col0\" >1614393</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col1\" class=\"data row28 col1\" >02005935</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col2\" class=\"data row28 col2\" >is redhat 7 file system support 64k block size?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col3\" class=\"data row28 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "na\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "na\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "na\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "na</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col4\" class=\"data row28 col4\" >['file system support', 'block size', 'redhat']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow28_col5\" class=\"data row28 col5\" >['file system support', 'block size', 'redhat']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col0\" class=\"data row29 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col1\" class=\"data row29 col1\" >02007816</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col2\" class=\"data row29 col2\" >pldandbd3 kernel: RPC: fragment too large: 1313166163</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col3\" class=\"data row29 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Please check why we are getting this error</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col4\" class=\"data row29 col4\" >['pldandbd3 kernel', 'large', 'fragment', 'rpc']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow29_col5\" class=\"data row29 col5\" >['pldandbd3 kernel', 'large', 'fragment', 'rpc', 'error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col0\" class=\"data row30 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col1\" class=\"data row30 col1\" >01761997</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col2\" class=\"data row30 col2\" >General performance issues on VM eseordmgtapp3/4</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col3\" class=\"data row30 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Applications are starting very slowly on this TIBCO system (eseordmgtapp3).\n",
       "\n",
       "TIBCO support observed  the following in the dmesg:\n",
       "\n",
       "6179290.816016] RPC: fragment too large: 812319233\n",
       "[6179291.117713] RPC: fragment too large: 812057089\n",
       "[6179291.419378] RPC: fragment too large: 812057089\n",
       "[6179291.721066] RPC: fragment too large: 812450305\n",
       "[6179292.022687] RPC: fragment too large: 812450305\n",
       "[6179292.324306] RPC: fragment too large: 812188161\n",
       "[6179292.626008] RPC: fragment too large: 812188161\n",
       "[6179292.927772] RPC: fragment too large: 812515841\n",
       "\n",
       "It seems a NFS-related issue. This system mounts a NetApp NFS share.\n",
       "\n",
       "Can you help us to fix the problem?\n",
       "\n",
       "Sosreport in attachment.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "It's a production TIBCO vmware virtual machine.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "When starting TIBCO applications.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col4\" class=\"data row30 col4\" >['general performance issues', 'vm']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow30_col5\" class=\"data row30 col5\" >['general performance issues', 'vm', 'tibco system', 'netapp nfs share', 'tibco support', 'rpc', 'large', 'fragment', 'system', 'nfs', 'attachment', 'sosreport']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col0\" class=\"data row31 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col1\" class=\"data row31 col1\" >01963131</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col2\" class=\"data row31 col2\" >kernel: FILEACCESS_ERROR   : Failed to find LRU record</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col3\" class=\"data row31 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "FDSUV09734:\n",
       "Oct 28 10:37:40 fdsuv09734 kernel: FILEACCESS_ERROR   : Failed to find LRU record\n",
       "Oct 28 10:37:51 fdsuv09734 kernel: FILEACCESS_ERROR   : Failed to find LRU record\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "This is production environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Not sure, but reported from \"dmesg\"\n",
       "\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "RPC: fragment too large: 1195725856\n",
       "RPC: fragment too large: 1212501072\n",
       "RPC: fragment too large: 50331667\n",
       "ip_tables: (C) 2000-2006 Netfilter Core Team\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "fileaccess_mod module is older than RHEL 6.2 ... applying fixups\n",
       "FileAccess module was inserted successfully. Version is - 10.2.2.1105\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record\n",
       "FILEACCESS_ERROR   : Failed to find LRU record</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col4\" class=\"data row31 col4\" >['lru record', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow31_col5\" class=\"data row31 col5\" >['lru record', 'kernel', 'fdsuv09734 kernel', 'lru record', 'fdsuv09734', 'oct']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col0\" class=\"data row32 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col1\" class=\"data row32 col1\" >02382482</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col2\" class=\"data row32 col2\" >Unable to connect with normal user intermittently , with root user also getting more time to ssh</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col3\" class=\"data row32 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Since last 2 days on multiple server noticed this issue.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Unable to connect with normal user, with root user also getting more time to ssh.\n",
       "When checked sssd service found \"sssd dead but subsys locked\"\n",
       "\n",
       "@rmlbbpocp1021 ~]# uptime; /etc/init.d/sssd status\n",
       " 02:46:48 up 38 days, 23:41,  2 users,  load average: 0.19, 1.76, 7.96\n",
       "sssd dead but subsys locked\n",
       "\n",
       "We have stop and started the service again .. but still facing issue, taking more time to ssh the server with root user and also facing unable to ssh to server with normal user intermittently.\n",
       "Now in logs file we found error - \n",
       "\n",
       "May 15 03:57:22 rmlbbpocp1021 kernel: sssd_nss[13380]: segfault at a4 ip 00000031ea82500c sp 00007ffcb5d7a4a0 error 4 in libdbus-1.so.3.4.0[31ea800000+40000]\n",
       "May 15 04:40:25 rmlbbpocp1021 kernel: sssd_nss[12023]: segfault at a4 ip 00000031ea82500c sp 00007ffdd2508de0 error 4 in libdbus-1.so.3.4.0[31ea800000+40000]\n",
       "\n",
       "=========================================================\n",
       "\n",
       "\n",
       "@rmlbbpocp1021 ~]# grep segfault /var/log/messages|grep sssd\n",
       "May 15 03:57:22 rmlbbpocp1021 kernel: sssd_nss[13380]: segfault at a4 ip 00000031ea82500c sp 00007ffcb5d7a4a0 error 4 in libdbus-1.so.3.4.0[31ea800000+40000]\n",
       "May 15 04:40:25 rmlbbpocp1021 kernel: sssd_nss[12023]: segfault at a4 ip 00000031ea82500c sp 00007ffdd2508de0 error 4 in libdbus-1.so.3.4.0[31ea800000+40000]\n",
       "\n",
       "@rmlbbpocp1021 ~]#  /etc/init.d/sssd status\n",
       "sssd (pid  13213) is running...\n",
       "\n",
       "t@rmlbbpocp1021 ~]# ps -ef | grep -i sssd\n",
       "root     11553 13213  0 04:31 ?        00:00:01 /usr/libexec/sssd/sssd_pam --uid 0 --gid 0 --debug-to-files\n",
       "root     13213     1  0 02:57 ?        00:00:00 /usr/sbin/sssd -f -D\n",
       "root     13225 13213  0 02:57 ?        00:00:00 /usr/libexec/sssd/sssd_autofs --uid 0 --gid 0 --debug-to-files\n",
       "root     14770 13213  0 04:42 ?        00:00:00 /usr/libexec/sssd/sssd_nss --uid 0 --gid 0 --debug-to-files\n",
       "root     17347 13213  1 04:51 ?        00:00:01 /usr/libexec/sssd/sssd_be --domain default --uid 0 --gid 0 --debug-to-files\n",
       "root     18375 17363  0 04:52 pts/0    00:00:00 grep -i sssd\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "on this server first time</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col4\" class=\"data row32 col4\" >['root user', 'normal user', 'more time', 'ssh', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow32_col5\" class=\"data row32 col5\" >['root user', 'normal user', 'more time', 'ssh', 'unable', 'multiple server', 'issue', 'days', 'last']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col0\" class=\"data row33 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col1\" class=\"data row33 col1\" >02104404</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col2\" class=\"data row33 col2\" >Fencing Issue in a Production Server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col3\" class=\"data row33 col3\" >Assistance to determine why a node server in production was fenced from a 6-Node Redhat GFS2 cluster environment. Fencing occurred at 7:30AM on 5/14/2018.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col4\" class=\"data row33 col4\" >['production server', 'issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow33_col5\" class=\"data row33 col5\" >['production server', 'issue', 'redhat gfs2 cluster environment', 'node server', 'production', 'assistance']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col0\" class=\"data row34 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col1\" class=\"data row34 col1\" >02433188</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col2\" class=\"data row34 col2\" >On NFS server get RPC: fragment too large messages while client is getting not responding messages</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col3\" class=\"data row34 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We have a NFS(  Veritas NAS) on RHEL 7.4 and a clients on RHEL 7.6.\n",
       "\n",
       "While applying load on our client we are getting messages\n",
       "\n",
       "\n",
       "Jul 23 13:08:02-2019 VA_01_02 kernel: [75293.243214] RPC: fragment too large: 1309409280\n",
       "Jul 23 13:08:02-2019 VA_01_02 kernel: [75293.243289] rpc-srv/tcp: nfsd: got error -104 when sending 116 bytes - shutting down socket\n",
       "Jul 23 13:08:05-2019 VA_01_02 kernel: [75296.454089] RPC: fragment too large: 404029440\n",
       "Jul 23 13:08:07-2019 VA_01_02 kernel: [75298.443830] RPC: fragment too large: 278331392\n",
       "\n",
       "Previous case was open 02417005\n",
       "\n",
       "Where we have applied some tcp tunings on both client and NAS neither has made any difference.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "RHEL 7.4 NAS and RHEL 7.6 client\n",
       "\n",
       "sosreport from client 5990\n",
       "sosreport form NAS node 1 and 2\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Once load is applied on the client\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "This is a blocker for our application which is moving from Solaris to RHEL.\n",
       "We dont see this issue  on a  Solaris client.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col4\" class=\"data row34 col4\" >['large messages', 'nfs server', 'messages', 'fragment', 'rpc', 'client']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow34_col5\" class=\"data row34 col5\" >['large messages', 'nfs server', 'messages', 'fragment', 'rpc', 'client', 'va_01_02 kernel', 'socket jul', 'veritas nas', 'rpc', 'jul', 'tcp tunings', 'large', 'fragment', 'nas', 'rhel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col0\" class=\"data row35 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col1\" class=\"data row35 col1\" >02291885</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col2\" class=\"data row35 col2\" >Soft Lockups</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col3\" class=\"data row35 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Need to find out how this issue is being generated to better troubleshoot the issues.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "The server is in a VMWare environment. Troubleshooting has been done where servers are pinned and resources are static at 100% required.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Usually at night when batch jobs run.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "The issues range from low to high issues. In the log below it eventually corrupted a SAS data file (which was later recovered, by re running a batch process).</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col4\" class=\"data row35 col4\" >['soft lockups']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow35_col5\" class=\"data row35 col5\" >['soft lockups', 'issues', 'issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col0\" class=\"data row36 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col1\" class=\"data row36 col1\" >01936433</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col2\" class=\"data row36 col2\" >rpcbind suddenly dead after running well for a few days</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col3\" class=\"data row36 col3\" >rpcbind suddenly died after running well for a few days.  \"# service rpcbind status\"  shows message \"rpcbind dead but pid file exists\".  No rpcbind error found in /var/adm/messages. To start back,  I run \"#service rpcbind start\" and \"service nfs restart\" and it runs without problem for a few days.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col4\" class=\"data row36 col4\" >['few days', 'dead']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow36_col5\" class=\"data row36 col5\" >['few days', 'dead', 'service rpcbind status', 'service rpcbind start', 'service nfs restart', 'rpcbind error', 'few days', 'pid file', 'dead', 'message', 'problem', 'messages']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col0\" class=\"data row37 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col1\" class=\"data row37 col1\" >02418936</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col2\" class=\"data row37 col2\" >NFS utility hungs when large file being copied</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col3\" class=\"data row37 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We were trying to copy data from one server to another and this device was acting as NFS client but when we tried to copy the small file, it was copied but it got hung when we initiate the large file transfer. \n",
       "\n",
       "Do we need to make any change in server side or client side? \n",
       "\n",
       "Do we need to use a different mount option?  Please advice \n",
       "\n",
       "I\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "While copying, it just got into hung state and mount is no longer accessible and same time we are not able to df. \n",
       "\n",
       "Jun 29 06:02:09 l122855dbss2003 kernel: RPC: fragment too large: 369295618\n",
       "Jun 29 06:02:09 l122855dbss2003 kernel: RPC: fragment too large: 369295360\n",
       "Jun 29 06:02:09 l122855dbss2003 kernel: RPC: fragment too large: 369295618\n",
       "Jun 29 06:02:09 l122855dbss2003 kernel: RPC: fragment too large: 2425088\n",
       "Jun 29 06:02:10 l122855dbss2003 kernel: RPC: fragment too large: 369295618\n",
       "Jun 29 06:02:10 l122855dbss2003 kernel: RPC: fragment too large: 369295618\n",
       "Jun 29 06:02:13 l122855dbss2003 kernel: RPC: fragment too large: 369295618\n",
       "Jun 29 06:02:13 l122855dbss2003 kernel: RPC: fragment too large: 369295362\n",
       "Jun 29 06:02:14 l122855dbss2003 kernel: RPC: fragment too large: 4194560\n",
       "Jun 29 06:02:16 l122855dbss2003 kernel: RPC: fragment too large: 1195725856\n",
       "Jun 29 06:02:38 l122855dbss2003 kernel: RPC: fragment too large: 1224736768\n",
       "Jun 29 06:02:40 l122855dbss2003 kernel: RPC: fragment too large: 1212501072\n",
       "Jun 29 06:04:29 l122855dbss2003 kernel: RPC: fragment too large: 1195725856\n",
       "Jun 29 06:04:29 l122855dbss2003 kernel: RPC: fragment too large: 369295618\n",
       "Jun 29 06:05:02 l122855dbss2003 kernel: RPC: fragment too large: 1195725856\n",
       "Jun 29 06:05:02 l122855dbss2003 kernel: RPC: fragment too large: 1195725856\n",
       "Jun 29 06:07:33 l122855dbss2003 kernel: RPC: fragment too large: 50331667\n",
       "Jun 29 06:09:06 l122855dbss2003 kernel: nfs: server 10.94.135.100 not responding, still trying\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "customer wants to migrate their data to the other side which is not happening due to this issue.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col4\" class=\"data row37 col4\" >['nfs utility hungs', 'large file']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow37_col5\" class=\"data row37 col5\" >['nfs utility hungs', 'large file', 'client side', 'server side', 'different mount option', 'nfs client', 'large file transfer', 'small file', 'server', 'change', 'device', 'advice']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col0\" class=\"data row38 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col1\" class=\"data row38 col1\" >02166929</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col2\" class=\"data row38 col2\" >please suggest how can i add VMs to export domain so that i can import these VMs .</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col3\" class=\"data row38 col3\" >I have added export domain successfully on RHEVM4  and then started copying Virtual machines on my NFS server that were running in RHEV 3.3 environment previously.When all data get copied then i checked on RHEVM portal ,it was showing export domain down and i am not able to detach or activate this domain.and in logs it is saying \n",
       "\"Storage Domain Export_Domain (Data Center MCA) was deactivated by system because it's not visible by any of the hosts.\"\n",
       "please suggest how can i add VMs to export domain so that i can import these VMs .\n",
       "\n",
       "Regards\n",
       "Kapil Kataria</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col4\" class=\"data row38 col4\" >['domain', 'vms']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow38_col5\" class=\"data row38 col5\" >['domain', 'vms', 'storage domain export_domain', 'data center mca', 'export domain', 'domain', 'regards kapil kataria', 'nfs server', 'virtual machines', 'data', 'rhevm portal', 'vms']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col0\" class=\"data row39 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col1\" class=\"data row39 col1\" >02460104</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col2\" class=\"data row39 col2\" >server got hung</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col3\" class=\"data row39 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "server got hung. we need to know the root cause for this issue.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "dev\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "never\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "it got hung around 10 am cest.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col4\" class=\"data row39 col4\" >['server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow39_col5\" class=\"data row39 col5\" >['server', 'root cause', 'hung', 'issue', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col0\" class=\"data row40 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col1\" class=\"data row40 col1\" >02420696</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col2\" class=\"data row40 col2\" >RPC: fragment too large</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col3\" class=\"data row40 col3\" >Hi\n",
       "Need help with resolving RPC: fragment too large.  SOS Report and other documentations will be attached soon.\n",
       "Regards,\n",
       "tom\n",
       "1-845-240-2698</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col4\" class=\"data row40 col4\" >['large', 'fragment', 'rpc']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow40_col5\" class=\"data row40 col5\" >['large', 'fragment', 'rpc', 'other documentations', 'sos report', 'large', 'fragment', 'rpc', 'regards', 'tom', 'help']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col0\" class=\"data row41 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col1\" class=\"data row41 col1\" >01870112</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col2\" class=\"data row41 col2\" >server was hung, did power drain to recover it. need to do RCA for server hung</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col3\" class=\"data row41 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "server was hung, did power drain to recover it. need to do RCA for server hung</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col4\" class=\"data row41 col4\" >['server hung', 'hung', 'server', 'rca', 'power']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow41_col5\" class=\"data row41 col5\" >['server hung', 'hung', 'server', 'rca', 'power', 'server hung', 'hung', 'server', 'rca', 'power']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col0\" class=\"data row42 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col1\" class=\"data row42 col1\" >02306207</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col2\" class=\"data row42 col2\" >Getting RPC: fragment too large erros in /var/log/messages</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col3\" class=\"data row42 col3\" >We are getting RPC: fragment too large erros in /var/log/messages. We have NFS shares mounted on these servers and it comes from EMC Unity disk array. Please confirm why this error was generated on servers and will there be any performance issue in NFS shares? \n",
       "\n",
       "RPC fragment too large:\n",
       "\n",
       "Jan 30 15:19:01 cesprdbilla003 audispd: node=cesprdbilla003 type=CRYPTO_KEY_USER msg=audit(1548821941.601:4860582): user pid=4671 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=session fp=? direction=both spid=4677 suid=74 rport=51632 laddr=10.108.120.208 lport=22  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'\n",
       "\n",
       "Jan 30 15:19:01 cesprdbilla003 audispd: node=cesprdbilla003 type=CRYPTO_KEY_USER msg=audit(1548821941.601:4860583): user pid=4671 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=16:a7:29:61:03:f3:88:f3:61:b7:ee:3b:44:4d:0f:37 direction=? spid=4671 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'\n",
       "\n",
       "Jan 30 15:19:01 cesprdbilla003 audispd: node=cesprdbilla003 type=CRYPTO_KEY_USER msg=audit(1548821941.601:4860584): user pid=4671 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=87:0b:6b:a9:20:fd:c2:f0:eb:f3:38:2f:5c:ca:84:6a direction=? spid=4671 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'\n",
       "\n",
       "Jan 30 15:19:01 cesprdbilla003 audispd: node=cesprdbilla003 type=USER_LOGIN msg=audit(1548821941.601:4860585): user pid=4671 uid=0 auid=4294967295 ses=4294967295 msg='op=login acct=\"(unknown)\" exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=ssh res=failed'\n",
       "\n",
       "Jan 30 15:19:02 cesprdbilla003 kernel: RPC: fragment too large: 1195725856\n",
       "\n",
       "Jan 30 15:19:02 cesprdbilla003 audispd: node=cesprdbilla003 type=CRYPTO_KEY_USER msg=audit(1548821942.559:4860586): user pid=4820 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=16:a7:29:61:03:f3:88:f3:61:b7:ee:3b:44:4d:0f:37 direction=? spid=4820 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'\n",
       "\n",
       "Jan 30 15:19:02 cesprdbilla003 audispd: node=cesprdbilla003 type=CRYPTO_KEY_USER msg=audit(1548821942.559:4860587): user pid=4820 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=87:0b:6b:a9:20:fd:c2:f0:eb:f3:38:2f:5c:ca:84:6a direction=? spid=4820 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'\n",
       "\n",
       " \n",
       "\n",
       " \n",
       "\n",
       "Jan 30 15:19:30 cesprdbilla004 audispd: node=cesprdbilla004 type=CRYPTO_KEY_USER msg=audit(1548821970.844:4261070): user pid=16163 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=5e:5e:a3:c1:ca:39:f0:11:31:f3:52:b7:5c:83:08:1b direction=? spid=16163 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'\n",
       "\n",
       "Jan 30 15:19:30 cesprdbilla004 audispd: node=cesprdbilla004 type=USER_LOGIN msg=audit(1548821970.844:4261071): user pid=16163 uid=0 auid=4294967295 ses=4294967295 msg='op=login acct=\"(unknown)\" exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=ssh res=failed'\n",
       "\n",
       "Jan 30 15:19:31 cesprdbilla004 kernel: RPC: fragment too large: 1195725856\n",
       "\n",
       "Jan 30 15:19:31 cesprdbilla004 audispd: node=cesprdbilla004 type=CRYPTO_KEY_USER msg=audit(1548821971.815:4261072): user pid=16305 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=6f:06:1f:0e:29:2b:eb:c3:0e:04:34:57:82:d4:71:b8 direction=? spid=16305 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'\n",
       "\n",
       "Jan 30 15:19:31 cesprdbilla004 audispd: node=cesprdbilla004 type=CRYPTO_KEY_USER msg=audit(1548821971.815:4261073): user pid=16305 uid=0 auid=4294967295 ses=4294967295 msg='op=destroy kind=server fp=5e:5e:a3:c1:ca:39:f0:11:31:f3:52:b7:5c:83:08:1b direction=? spid=16305 suid=0  exe=\"/usr/sbin/sshd\" hostname=? addr=10.108.120.140 terminal=? res=success'</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col4\" class=\"data row42 col4\" >['large erros', 'log', 'fragment', 'messages', 'rpc']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow42_col5\" class=\"data row42 col5\" >['large erros', 'log', 'fragment', 'messages', 'rpc', \"user pid=4671 uid=0 auid=4294967295 ses=4294967295 msg='op\", \"uid=0 auid=4294967295 ses=4294967295 msg='op\", \"auid=4294967295 ses=4294967295 msg='op\", \"ses=4294967295 msg='op\", 'uid=0 auid=4294967295', 'cesprdbilla003 type', 'crypto_key_user msg', 'cesprdbilla003 audispd', 'cesprdbilla004 type', 'user pid=4671']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col0\" class=\"data row43 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col1\" class=\"data row43 col1\" >02280416</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col2\" class=\"data row43 col2\" >We are getting \"Dec 16 08:50:11 tlnckapv0018 kernel: RPC: fragment too large: 1735489335\" Please advise a fix for this</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col3\" class=\"data row43 col3\" >We are getting \"Dec 16 08:50:11 tlnckapv0018 kernel: RPC: fragment too large: 1735489335\" Please advise a fix for this , There were no changes made to the NFS client recently\n",
       "\n",
       "#NFS Mount points on tlnckapv0018 & tlnckapv0011\n",
       "\n",
       "gbptlsrvrpr:/sapmnt/RPR/profile       /sapmnt/RPR/profile     nfs  vers=3,defaults  0  0\n",
       "gbptlsrvrpr:/sapmnt/RPR/global        /sapmnt/RPR/global      nfs  vers=3,defaults  0  0\n",
       "gbptlsrvrpr:/scratch1                 /scratch1               nfs  vers=3,defaults  0  0\n",
       "gbptlsrvrpr:/usr/sap/put              /usr/sap/put            nfs  vers=3,defaults  0  0\n",
       "gbptlsrvrpr:/usr/sap/trans            /usr/sap/trans          nfs  vers=3,defaults  0  0\n",
       "tlsrvmpr:/data_work/shuttle           /data_work/shuttle      nfs  vers=3,defaults  0  0\n",
       "tlsrvgpr:/sapcds                      /sapcds                 nfs  vers=3,defaults  0  0</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col4\" class=\"data row43 col4\" >['tlnckapv0018 kernel', 'fragment', 'large', 'rpc', 'dec', 'fix']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow43_col5\" class=\"data row43 col5\" >['tlnckapv0018 kernel', 'fragment', 'large', 'rpc', 'dec', 'fix', 'nfs mount', 'nfs client', 'nfs', 'vers=3,defaults', 'tlnckapv0018 kernel', 'rpr', 'tlnckapv0018', 'sap', 'gbptlsrvrpr:/sapmnt', 'profile']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col0\" class=\"data row44 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col1\" class=\"data row44 col1\" >02510806</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col2\" class=\"data row44 col2\" >NFS share was inaccessible from cluster.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col3\" class=\"data row44 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "NFS share folder was inaccessible on linux client VM-USFFENTPRD05 /06 / 07 / 08 which is we fixed after the reboot.\n",
       "\n",
       "Please see below given logs and provide the root cause.\n",
       "NFS Cluster Logs :-\n",
       "\n",
       "[root@VFFPRDUTFSRL05 log]# cat messages | grep time\n",
       "Nov  4 21:11:14 VFFPRDUTFSRL05 lrmd[6596]: warning: NA2NFS_amLVM_monitor_10000 process (PID 19670) timed out\n",
       "Nov  4 21:11:14 VFFPRDUTFSRL05 lrmd[6596]: warning: NA2NFS_amLVM_monitor_10000:19670 - timed out after 30000ms\n",
       "Nov  4 21:12:06 VFFPRDUTFSRL05 lrmd[6596]: warning: NA2NFS_amLVM_start_0 process (PID 21751) timed out\n",
       "Nov  4 21:12:06 VFFPRDUTFSRL05 lrmd[6596]: warning: NA2NFS_amLVM_start_0:21751 - timed out after 30000ms\n",
       "\n",
       "Client Logs :- \n",
       "[root@VM-USFFENTPRD08 log]# cat messages | grep nfs\n",
       "Nov  4 21:13:05 VM-USFFENTPRD08 kernel: nfs: server na2nfsvr.iam.sungard.prod not responding, timed out\n",
       "Nov  5 01:41:29 VM-USFFENTPRD08 kernel: nfs: server na2nfsvr.iam.sungard.prod not responding, still trying\n",
       "Nov  5 03:37:47 VM-USFFENTPRD08 kernel: nfs: server na2nfsvr.iam.sungard.prod not responding, still trying\n",
       "Nov  5 04:00:32 VM-USFFENTPRD08 kernel: Installing knfsd (copyright (C) 1996 okir@monad.swb.de).\n",
       "Nov  5 04:00:55 VM-USFFENTPRD08 kernel: FS-Cache: Netfs 'nfs' registered for caching</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col4\" class=\"data row44 col4\" >['nfs share', 'cluster', 'inaccessible']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow44_col5\" class=\"data row44 col5\" >['nfs share', 'cluster', 'inaccessible', 'cat messages | grep nfs', 'cat messages | grep time', 'nfs cluster logs', 'nfs share folder', 'linux client vm', 'usffentprd08 kernel', 'nov', 'nfs', 'vm', 'client logs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col0\" class=\"data row45 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col1\" class=\"data row45 col1\" >02453510</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col2\" class=\"data row45 col2\" >Server getting RPC: fragment too large xxx</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col3\" class=\"data row45 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Please find the attached snap shot where getting RPC: fragment error\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "not getting login prompt in VM console but showing this error\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "frequently\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Application hungs and we asked for reboot server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col4\" class=\"data row45 col4\" >['large xxx', 'fragment', 'rpc', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow45_col5\" class=\"data row45 col5\" >['large xxx', 'fragment', 'rpc', 'server', 'fragment error', 'snap shot', 'rpc']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col0\" class=\"data row46 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col1\" class=\"data row46 col1\" >02445568</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col2\" class=\"data row46 col2\" >Need RCA for server inaccessible on 3rd Aug at 6:</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col3\" class=\"data row46 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hello Team \n",
       "\n",
       "On 3rd Aug we had noticed that server was not accessible either by normal user or by root user.\n",
       "We have take reboot of server ( at Aug  3 06:55:15 rmlbbpapp0004 kernel: IPv6: Loaded, but administratively disabled, reboot required to enable)  and then after login via root started the sssd service resolved the issue.\n",
       "We had also noticed server got rebooted at Aug  3 02:57:52 also.\n",
       "\n",
       "Need to know what could be the root cause.\n",
       "\n",
       "We have not found any logs from 00:22 to 02:57 for 3 Aug also.\n",
       "\n",
       "I have attached nmon logs and all logs from var log message for 3Aug  \n",
       "along with sos report\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col4\" class=\"data row46 col4\" >['3rd aug', 'server inaccessible', 'rca']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow46_col5\" class=\"data row46 col5\" >['3rd aug', 'server inaccessible', 'rca', 'var log message', 'root user', 'nmon logs', '3rd aug', 'root cause', 'normal user', 'server', 'aug', 'root', 'logs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col0\" class=\"data row47 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col1\" class=\"data row47 col1\" >02446720</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col2\" class=\"data row47 col2\" >The server vmwplsapp07-prd went hung and had to reboot to make it operational</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col3\" class=\"data row47 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "The server vmwplsapp07-prd went to hung state unexpectedly and had to reboot the server to make it operational. The server is in VMware environment and I took a snapshot before reboot of system.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "The server is in Production environment and running Pulse.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Once\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "The snapshot of the hung system is available</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col4\" class=\"data row47 col4\" >['hung', 'operational', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow47_col5\" class=\"data row47 col5\" >['hung', 'operational', 'server', 'vmware environment', 'server', 'hung state', 'snapshot', 'reboot', 'operational', 'system']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col0\" class=\"data row48 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col1\" class=\"data row48 col1\" >02014278</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col2\" class=\"data row48 col2\" >Repeated \"RPC: fragment too large:\" every 7 days</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col3\" class=\"data row48 col3\" >We are seeing repeated \"RPC: fragment too large:\" every 7 days (Saturday AM). Example log entry is show below -\n",
       "\n",
       " @timestamp\tJanuary 6th 2018, 08:06:31.000\n",
       "t @version\t1\n",
       "t _id\tAWDKgeg9kSS6m8QDvoqy\n",
       "t _index\tlogstash-2018.01.06\n",
       "# _score\t1\n",
       "t _type\tsyslog\n",
       "t beat.hostname\tcpx-app-e01\n",
       "t beat.name\tcpx-app-e01\n",
       "t beat.version\t5.2.1\n",
       "t host\tcpx-app-e01\n",
       "t input_type\tlog\n",
       "t message\tJan  6 08:06:31 cpx-app-e01 kernel: RPC: fragment too large: 369295618\n",
       "# offset\t388,912\n",
       " received_at\tJanuary 6th 2018, 08:06:34.465\n",
       "t received_from\tcpx-app-e01\n",
       "t source\t/var/log/messages\n",
       "t syslog_hostname\tcpx-app-e01\n",
       "t syslog_message\tRPC: fragment too large: 369295618\n",
       "t syslog_program\tkernel\n",
       "t syslog_timestamp\tJan  6 08:06:31\n",
       "t tags\tbeats_input_codec_plain_applied\n",
       "t type\tsyslog</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col4\" class=\"data row48 col4\" >['days', 'large', 'fragment', 'rpc']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow48_col5\" class=\"data row48 col5\" >['days', 'large', 'fragment', 'rpc', 'e01 t beat.name', 'e01 t source', 'awdkgeg9kss6m8qdvoqy t _ index', 'e01 t input_type', 'e01 t syslog_message', 'e01 t beat.version', 't _ type', 'log t message', 'kernel t syslog_timestamp', 'syslog t beat.hostname']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col0\" class=\"data row49 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col1\" class=\"data row49 col1\" >01674498</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col2\" class=\"data row49 col2\" >RPC: fragment too large</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col3\" class=\"data row49 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Getting the following message found in the kernel log.  \n",
       "\n",
       "bash> dmesg\n",
       "RPC: fragment too large: 1195725856\n",
       "RPC: fragment too large: 1212501072\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "RHEL 6.8 on Dell R420 server.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "Event is random.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col4\" class=\"data row49 col4\" >['large', 'fragment', 'rpc']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow49_col5\" class=\"data row49 col5\" >['large', 'fragment', 'rpc', 'dmesg rpc', 'kernel log', 'rpc', 'fragment', 'following message', 'large', 'bash']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col0\" class=\"data row50 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col1\" class=\"data row50 col1\" >02390587</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col2\" class=\"data row50 col2\" >Bad packet length . Disconnecting: Packet corrupt ERROR</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col3\" class=\"data row50 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We run EDA tools in our environment. In the above mentioned servers Tools are crashing with \"Bad packet length . Disconnecting: Packet corrupt\" error. outgoing ssh connection from this host to other host is terminated after some time with this error\n",
       "\n",
       "Tried below to fix the issues :\n",
       "https://access.redhat.com/solutions/142643\n",
       "Restarted ssh service \n",
       "Enabled ssh protocl 1\n",
       "\n",
       "But no luck\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "In server, in linux environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "We run EDA tools in our environment. In the above mentioned servers Tools are crashing with \"Bad packet length . Disconnecting: Packet corrupt\" error. outgoing ssh connection from this host to other host is terminated after some time with this error\n",
       "\n",
       "Tried below to fix the issues :\n",
       "https://access.redhat.com/solutions/142643\n",
       "Restarted ssh service \n",
       "Enabled ssh protocl 1\n",
       "\n",
       "But no luck</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col4\" class=\"data row50 col4\" >['packet corrupt error', 'bad packet length']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow50_col5\" class=\"data row50 col5\" >['packet corrupt error', 'bad packet length', 'https://access.redhat.com/solutions/142643 restarted ssh service', 'enabled ssh protocl', 'outgoing ssh connection', 'bad packet length', 'other host', 'servers tools', 'eda tools', 'error', 'host', 'above']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col0\" class=\"data row51 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col1\" class=\"data row51 col1\" >02388158</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col2\" class=\"data row51 col2\" >We are observing latency issue on our from production server to NFS mount point 10.1.14.253:/FS_TIBCO_EMS</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col3\" class=\"data row51 col3\" >We are observing latency issue on our from production server to NFS mount point 10.1.14.253:/FS_TIBCO_EMS</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col4\" class=\"data row51 col4\" >['nfs mount point 10.1.14.253:/fs_tibco_ems', 'production server', 'latency issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow51_col5\" class=\"data row51 col5\" >['nfs mount point 10.1.14.253:/fs_tibco_ems', 'production server', 'latency issue', 'nfs mount point 10.1.14.253:/fs_tibco_ems', 'production server', 'latency issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col0\" class=\"data row52 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col1\" class=\"data row52 col1\" >01998237</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col2\" class=\"data row52 col2\" >RPC: fragment too large errors</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col3\" class=\"data row52 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We have NFS shares mounted with default options on some servers. We see in the /var/log/messages errors like below:\n",
       "Dec 15 16:29:57 sla13294 kernel: RPC: fragment too large: 3014912\n",
       "Dec 15 16:30:00 sla13294 kernel: RPC: fragment too large: 369295363\n",
       "Dec 15 16:30:03 sla13294 kernel: RPC: fragment too large: 369295619\n",
       "Dec 15 16:30:06 sla13294 kernel: RPC: fragment too large: 369295875\n",
       "Dec 15 16:30:09 sla13294 kernel: RPC: fragment too large: 369296131\n",
       "Server owner reports that during that time application, that uses this NFS share, crashes.\n",
       "NFS team stands that there is no errors on their side.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Except of operating system servers seem to have nothing in common: some are virtual, some physical, lay in different network segments, connect to different NFS servers.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "It happens always in the similiar time every Friday:\n",
       "messages.4.bz2:Nov 24 15:06:21 sla13294 kernel: RPC: fragment too large: 3014912\n",
       "messages.4.bz2:Nov 24 15:06:24 sla13294 kernel: RPC: fragment too large: 369295363\n",
       "messages.4.bz2:Nov 24 15:06:27 sla13294 kernel: RPC: fragment too large: 369295619\n",
       "messages.4.bz2:Nov 24 15:06:30 sla13294 kernel: RPC: fragment too large: 369295875\n",
       "messages.4.bz2:Nov 24 15:06:33 sla13294 kernel: RPC: fragment too large: 369296131\n",
       " \n",
       "10:06:55\n",
       "◄\n",
       "messages.3.bz2:Dec 1 15:31:17 sla13294 kernel: RPC: fragment too large: 3014912\n",
       "messages.3.bz2:Dec 1 15:31:20 sla13294 kernel: RPC: fragment too large: 369295363\n",
       "messages.3.bz2:Dec 1 15:31:23 sla13294 kernel: RPC: fragment too large: 369295619\n",
       "messages.3.bz2:Dec 1 15:31:26 sla13294 kernel: RPC: fragment too large: 369295875\n",
       "messages.3.bz2:Dec 1 15:31:29 sla13294 kernel: RPC: fragment too large: 369296131\n",
       " \n",
       "10:07:13\n",
       "◄\n",
       "messages.2.bz2:Dec 8 16:02:04 sla13294 kernel: RPC: fragment too large: 3014912\n",
       "messages.2.bz2:Dec 8 16:02:07 sla13294 kernel: RPC: fragment too large: 369295363\n",
       "messages.2.bz2:Dec 8 16:02:10 sla13294 kernel: RPC: fragment too large: 369295619\n",
       "messages.2.bz2:Dec 8 16:02:13 sla13294 kernel: RPC: fragment too large: 369295875\n",
       "messages.2.bz2:Dec 8 16:02:16 sla13294 kernel: RPC: fragment too large: 369296131\n",
       " \n",
       "10:07:30\n",
       "◄\n",
       "messages.1.bz2:Dec 15 16:29:57 sla13294 kernel: RPC: fragment too large: 3014912\n",
       "messages.1.bz2:Dec 15 16:30:00 sla13294 kernel: RPC: fragment too large: 369295363\n",
       "messages.1.bz2:Dec 15 16:30:03 sla13294 kernel: RPC: fragment too large: 369295619\n",
       "messages.1.bz2:Dec 15 16:30:06 sla13294 kernel: RPC: fragment too large: 369295875\n",
       "messages.1.bz2:Dec 15 16:30:09 sla13294 kernel: RPC: fragment too large: 369296131\n",
       "\n",
       "Also on others servers:\n",
       "Nov 24 13:48:08 sl018085 kernel: RPC: fragment too large: 3014912\n",
       "Nov 24 13:48:11 sl018085 kernel: RPC: fragment too large: 369295363\n",
       "Nov 24 13:48:14 sl018085 kernel: RPC: fragment too large: 369295619\n",
       "Nov 24 13:48:17 sl018085 kernel: RPC: fragment too large: 369295875\n",
       "Nov 24 13:48:20 sl018085 kernel: RPC: fragment too large: 369296131\n",
       "Nov 24 13:53:26 sla17854 kernel: RPC: fragment too large: 3014912\n",
       "Nov 24 13:53:29 sla17854 kernel: RPC: fragment too large: 369295363\n",
       "Nov 24 13:53:32 sla17854 kernel: RPC: fragment too large: 369295619\n",
       "Nov 24 13:53:35 sla17854 kernel: RPC: fragment too large: 369295875\n",
       "Nov 24 13:53:38 sla17854 kernel: RPC: fragment too large: 369296131\n",
       "Dec 1 14:07:29 sla17854 kernel: RPC: fragment too large: 3014912\n",
       "Dec 1 14:07:30 sla17854 kernel: RPC: fragment too large: 369295619\n",
       "Dec 1 14:07:31 sla17854 kernel: RPC: fragment too large: 369295875\n",
       "Dec 1 14:07:32 sla17854 kernel: RPC: fragment too large: 369296131\n",
       "Dec 1 14:07:33 sla17854 kernel: RPC: fragment too large: 369295363\n",
       "Dec 1 14:20:42 sl018085 kernel: RPC: fragment too large: 3014912\n",
       "Dec 1 14:20:45 sl018085 kernel: RPC: fragment too large: 369295363\n",
       "Dec 1 14:20:48 sl018085 kernel: RPC: fragment too large: 369295619\n",
       "Dec 1 14:20:51 sl018085 kernel: RPC: fragment too large: 369295875\n",
       "Dec 1 14:20:54 sl018085 kernel: RPC: fragment too large: 369296131\n",
       "Dec 1 14:30:08 sl019070 kernel: RPC: fragment too large: 3014912\n",
       "Dec 1 14:30:11 sl019070 kernel: RPC: fragment too large: 369295363\n",
       "Dec 1 14:30:14 sl019070 kernel: RPC: fragment too large: 369295619\n",
       "Dec 1 14:30:17 sl019070 kernel: RPC: fragment too large: 369295875\n",
       "Dec 1 14:30:20 sl019070 kernel: RPC: fragment too large: 369296131\n",
       "Dec 8 14:32:33 sla17854 kernel: RPC: fragment too large: 3014912\n",
       "Dec 8 14:32:36 sla17854 kernel: RPC: fragment too large: 369295363\n",
       "Dec 8 14:32:39 sla17854 kernel: RPC: fragment too large: 369295619\n",
       "Dec 8 14:32:42 sla17854 kernel: RPC: fragment too large: 369295875\n",
       "Dec 8 14:32:45 sla17854 kernel: RPC: fragment too large: 369296131\n",
       "Dec 8 14:39:07 sl018085 kernel: RPC: fragment too large: 3014912\n",
       "Dec 8 14:39:10 sl018085 kernel: RPC: fragment too large: 369295363\n",
       "Dec 8 14:39:13 sl018085 kernel: RPC: fragment too large: 369295619\n",
       "Dec 8 14:39:16 sl018085 kernel: RPC: fra</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col4\" class=\"data row52 col4\" >['large errors', 'fragment', 'rpc']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow52_col5\" class=\"data row52 col5\" >['large errors', 'fragment', 'rpc', 'sla13294 kernel', 'nfs team', 'nfs share', 'nfs shares', 'dec', 'large', 'fragment', 'messages errors', 'rpc', 'time application']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col0\" class=\"data row53 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col1\" class=\"data row53 col1\" >02045308</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col2\" class=\"data row53 col2\" >Server unresponsive</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col3\" class=\"data row53 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Server became unresponsive during business hours. Simple commands would return after several minutes. SSH connections failed with access denied. Server and console stopped responding, had to reboot.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "First time</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col4\" class=\"data row53 col4\" >['unresponsive']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow53_col5\" class=\"data row53 col5\" >['unresponsive', 'simple commands', 'several minutes', 'ssh connections', 'business hours', 'server', 'access', 'unresponsive', 'console']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col0\" class=\"data row54 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col1\" class=\"data row54 col1\" >02296545</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col2\" class=\"data row54 col2\" >Server got rebooted automatically. need to know why</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col3\" class=\"data row54 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Today, both of the mentioned system got rebooted automatically.\n",
       "We need to understand the possible reason for the same.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col4\" class=\"data row54 col4\" >['server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow54_col5\" class=\"data row54 col5\" >['server', 'possible reason', 'same', 'system', 'today']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col0\" class=\"data row55 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col1\" class=\"data row55 col1\" >02363970</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col2\" class=\"data row55 col2\" >directory listing under nfs mount of gluster volume shows incorrect entries</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col3\" class=\"data row55 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "directory listing under nfs mount of gluster volume shows incorrect entries  on the node n-gpx1-c201\n",
       "\n",
       "[root@n-gpx1-c201 ~]# ls /rhgs/client/vol1/live/iptv/ingest/ftphomes/rtlhd/archive/ | wc\n",
       "    509     509   25959\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "PRODUCTION</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col4\" class=\"data row55 col4\" >['gluster volume', 'nfs mount', 'incorrect entries', 'directory']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow55_col5\" class=\"data row55 col5\" >['gluster volume', 'nfs mount', 'incorrect entries', 'directory', 'archive/ | wc', 'incorrect entries', 'gluster volume', 'node n', '# ls', 'nfs mount', 'gpx1-c201', 'ingest', 'iptv', 'ftphomes']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col0\" class=\"data row56 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col1\" class=\"data row56 col1\" >02185835</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col2\" class=\"data row56 col2\" >user's NAS home directories are not getting mounted</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col3\" class=\"data row56 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "when we checked what caused this issue. we found that nfslock and rpcbind daemons are down. We started these daemons manually and the issue fixed.  We need to know what is realy taking these 2 daemons down.  How can we prevent to happening this again.?\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "DR environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "It happens frequently.  Everytime, problem reported by users, we have to start nfslock and rpcbind  daemons to fix the issue.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "As per user, it happens frequently at midnight to 2 am EST.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col4\" class=\"data row56 col4\" >['nas home directories', 'user']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow56_col5\" class=\"data row56 col5\" >['nas home directories', 'user', 'daemons', 'issue', 'nfslock', 'realy']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col0\" class=\"data row57 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col1\" class=\"data row57 col1\" >02424456</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col2\" class=\"data row57 col2\" >Processo rpciod com consumo elevado de CPU</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col3\" class=\"data row57 col3\" >Que tipo de problema/comportamento você está enfrentando? O que você espera encontrar?\n",
       "\n",
       "- Um servidor cliente NFS rodando um Apache está apresentando diversos processos rpciod com picos de utilização de 100% de processamento.\n",
       "\n",
       "- O filesystem do conteúdo estático do Apache é montado via NFS:\n",
       "\n",
       "[p573005@dt7261lx454 ~]$ df -h /opt/apache/jboss-ews-2.1/httpd/htdocs/sinbc/statics-portal\n",
       "Filesystem            Size  Used Avail Use% Mounted on\n",
       "dt7261lx453:/opt/apache/jboss-ews-2.1/httpd/htdocs/sinbc/statics-portal\n",
       "                      9.8G  2.0G  7.3G  22% /opt/apache/jboss-ews-2.1/httpd/htdocs/sinbc/statics-portal\n",
       "\n",
       "\n",
       "- Junto a isso, o load average do servidor está muito elevado, impactando no atendimento das requisições do Apache:\n",
       "\n",
       "[p573005@dt7261lx454 ~]$ uptime\n",
       " 09:45:12 up 496 days,  6:03,  5 users,  load average: 57459.07, 57459.09, 57433.75\n",
       "\n",
       "- O servidor possui 32 processadores e 64 GB de memória e não foi identificada falta ou contenção em processamento.\n",
       "\n",
       "Onde você está enfrentando esse tipo de comportamento? Em qual ambiente? \n",
       "\n",
       "Ambiente de produção.\n",
       "\n",
       "Quando este comportamento ocorre? Com qual frequência? Várias vezes? Algumas vezes? \n",
       "\n",
       "Informado que o comportamento está ocorrendo há pelo menos 3 dias.\n",
       "\n",
       "Quais informações você pode fornecer sobre períodos e o impacto empresarial?\n",
       "\n",
       "Até o momento não foi informado.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col4\" class=\"data row57 col4\" >['processo rpciod com consumo elevado', 'cpu']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow57_col5\" class=\"data row57 col5\" >['processo rpciod com consumo elevado', 'cpu', 'quais informações você pode fornecer sobre períodos e', 'onde você está enfrentando esse tipo', 'comportamento você está enfrentando', 'apache está apresentando diversos processos', 'servidor está muito elevado', 'comportamento está ocorrendo', 'que você espera encontrar', 'servidor cliente nfs rodando', 'quando este comportamento ocorre', 'foi identificada falta ou contenção']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col0\" class=\"data row58 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col1\" class=\"data row58 col1\" >02257895</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col2\" class=\"data row58 col2\" >we are facing the issue with SSH on our Linux servers</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col3\" class=\"data row58 col3\" >We are continuing to have sporadic problems using SSH with our Linux servers. For example at this time, I can SSH into mnstlfs02 but I cannot SSH into mnstlfs01. When logging into mnstlfs01 it behaves as if I am entering an incorrect password even though I am entering the same password. Later today it might change as to which server I can SSH into and which one I cannot. This issue is being seen by multiple people across multiple servers. \n",
       "\n",
       "Also we noticed in the console of the server it saying RPC:fragment too large. kindly let us know why we are facing the issue.\n",
       "sometimes when we tried to login in the server we are getting error like permission denied,</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col4\" class=\"data row58 col4\" >['linux servers', 'ssh', 'issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow58_col5\" class=\"data row58 col5\" >['linux servers', 'ssh', 'issue', 'multiple servers', 'linux servers', 'incorrect password', 'same password', 'multiple people', 'server', 'sporadic problems', 'example', 'console', 'issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col0\" class=\"data row59 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col1\" class=\"data row59 col1\" >02234282</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col2\" class=\"data row59 col2\" >System crashed and a vmcore file was generated</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col3\" class=\"data row59 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "** NOTE: Please attach the Premium: OEM Level 3 Support Partner Entitlement to this case.\n",
       "** THIS IS AN L3 CASE FOR IBM - PLEASE SEE THE ACTUAL END CUSTOMER INFORMATION IN SECTION II. BELOW **\n",
       " \n",
       "I. SUMMARY\n",
       "System crashed and a vmcore file was generated\n",
       " \n",
       "II. CUSTOMER DETAILS\n",
       "* Customer:  Maersk Line A/S\n",
       "* RHN user:  Pending on customer\n",
       "* L3 case:   TS001473844\n",
       "* Location of diagnostic data: <If not attached to this case>\n",
       " \n",
       "III. HARDWARE CONFIGURATION\n",
       "Please provide as many details as possible including the following:\n",
       "* System Model name: \n",
       "* CPU info: \n",
       "* Memory Info: \n",
       "* Hardware Information: <Only hardware suspected of being related to the issue>\n",
       " \n",
       "IV. PROBLEM DESCRIPTION\n",
       "* Issue detail: \n",
       "1) System crashed and the customer would like the vmcore file analyzed.\n",
       "2) Customer split the vmcore file for easier upload\n",
       " \n",
       "V. COLLABORATION NEEDED\n",
       "* Description of assistance being requested of Red Hat:\n",
       "1) Customer would like the vmcore file analyzed\n",
       "2) Waiting for the customer to confirm the RedHat ID\n",
       " \n",
       "VI. ADDITIONAL INFORMATION\n",
       "* Additional details not specifically in the previous sections that may be relevant to the case.\n",
       "Not sure if https://access.redhat.com/solutions/3646141 relates to the problem</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col4\" class=\"data row59 col4\" >['vmcore file', 'system']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow59_col5\" class=\"data row59 col5\" >['vmcore file', 'system', 'actual end customer information', 'customer details', 'l3 case', 'customer', 'vmcore file', 'additional details', 'hardware information', 'system model name', 'additional information', 'case']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row60\" class=\"row_heading level0 row60\" >60</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col0\" class=\"data row60 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col1\" class=\"data row60 col1\" >02150176</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col2\" class=\"data row60 col2\" >NFS overload  Handling  during scan: RPC: fragment too large: 1279873876</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col3\" class=\"data row60 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Our customer reported issues that NFS is unable to handle large packets during scan and may be some other use cases : Following ticket was raised and closed as non reproducible\n",
       "https://access.redhat.com/support/cases/#/case/01988040\n",
       "\n",
       "Now customers are asking us to provide mechanism for NFS overload protection .\n",
       "We would like discuss with RH \n",
       "1) Is the issue similar to  : https://access.redhat.com/solutions/2065483 can we use solution listed here for the correction \n",
       "2) Is there any reproducer or suggestion from redhat to reproduce this issue in our test environment \n",
       "3) What are the suggestion to recover from this state \n",
       "4) Can we write healthcheck scripts to determine if NFS server is not responding based on simple read/write/flock commands</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col4\" class=\"data row60 col4\" >['nfs overload', 'rpc', 'scan', 'fragment', 'large']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow60_col5\" class=\"data row60 col5\" >['nfs overload', 'rpc', 'scan', 'fragment', 'large', 'nfs overload protection', 'nfs server', 'other use cases', 'nfs', 'non reproducible https://access.redhat.com/support/cases/#/case/01988040', 'large packets', 'issue similar', 'simple read', 'test environment', 'issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row61\" class=\"row_heading level0 row61\" >61</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col0\" class=\"data row61 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col1\" class=\"data row61 col1\" >01910584</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col2\" class=\"data row61 col2\" >Regulary getting RPC: fragment too large errors causing issue to Hadoop application.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col3\" class=\"data row61 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Regulary getting RPC: fragment too large errors causing issue to Hadoop application. Please look into this ASAP.\n",
       "\n",
       "Aug 11 00:05:44 lrdne22epapd1i kernel: RPC: fragment too large: 1665073152\n",
       "Aug 11 00:05:44 lrdne22epapd1i kernel: RPC: fragment too large: 302055450\n",
       "Aug 11 00:05:53 lrdne22epapd1i kernel: RPC: fragment too large: 268501205\n",
       "Aug 11 00:05:53 lrdne22epapd1i kernel: RPC: fragment too large: 1195725856\n",
       "Aug 11 00:05:53 lrdne22epapd1i kernel: RPC: fragment too large: 369295618\n",
       "Aug 11 00:05:53 lrdne22epapd1i kernel: RPC: fragment too large: 50331659\n",
       "Aug 11 00:05:53 lrdne22epapd1i kernel: RPC: fragment too large: 50331667\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 1195725856\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 1195725856\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 1195725856\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 1195725856\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 1212498244\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 16777216\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 1414744096\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 1010792557\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 83888896\n",
       "Aug 11 00:06:05 lrdne22epapd1i kernel: RPC: fragment too large: 35913728\n",
       "Aug 11 00:06:17 lrdne22epapd1i kernel: RPC: fragment too large: 1509949440\n",
       "Aug 11 00:06:17 lrdne22epapd1i kernel: RPC: fragment too large: 1245992784\n",
       "Aug 11 00:06:17 lrdne22epapd1i kernel: RPC: fragment too large: 1229866575\n",
       "Aug 11 00:06:29 lrdne22epapd1i kernel: RPC: fragment too large: 305397761</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col4\" class=\"data row61 col4\" >['large errors', 'hadoop application', 'issue', 'fragment', 'rpc', 'regulary']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow61_col5\" class=\"data row61 col5\" >['large errors', 'hadoop application', 'issue', 'fragment', 'rpc', 'regulary', 'lrdne22epapd1i kernel', 'large errors', 'large', 'fragment', 'aug', 'rpc', 'hadoop application', 'issue', 'asap', 'regulary']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row62\" class=\"row_heading level0 row62\" >62</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col0\" class=\"data row62 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col1\" class=\"data row62 col1\" >02280914</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col2\" class=\"data row62 col2\" >syslog server salogp12(NFS client-RHEL5.11) has issues mounting various NFS mounts (not always..issue intermittent)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col3\" class=\"data row62 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "syslog server salogp12(NFS client-RHEL5.11) has issues mounting various NFS mounts...this issue is intermittent\n",
       "Please find the attached sosreport\n",
       "\n",
       "sample dmesg from salogp12(NFS client)\n",
       "nfsacl: server sawasp05 not responding, timed out\n",
       "nfs: server sawasp05 not responding, timed out\n",
       "nfs: server sawasp05 not responding, timed out\n",
       "nfs: server sawasp05 not responding, timed out\n",
       "nfsacl: server sawasp05 not responding, timed out\n",
       "nfsacl: server sawasp05 not responding, timed out\n",
       "nfsacl: server sawasp05 not responding, timed out\n",
       "nfs: server sawasp05 not responding, timed out\n",
       "nfsacl: server sawasp05 not responding, timed out\n",
       "nfs: server sawasp05 not responding, timed out\n",
       "nfs: server sawasp07 not responding, timed out\n",
       "nfs: server saepap03 not responding, timed out\n",
       "nfs: server sawasp07 not responding, timed out\n",
       "nfs: server saepap03 not responding, timed out\n",
       "nfs: server sawasp07 not responding, timed out\n",
       "nfs: server saepap03 not responding, timed out\n",
       "nfs: server saepap03 not responding, timed out\n",
       "nfs: server saepap03 not responding, timed out\n",
       "[root@salogp12 ~]#\n",
       "\n",
       "\n",
       "sample dmesg from NFS servers:\n",
       "sawasp05(RHEL5.11)\n",
       "NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory\n",
       "NFSD: starting 90-second grace period\n",
       "JBD: barrier-based sync failed on dm-7-8 - disabling barriers\n",
       "JBD: barrier-based sync failed on dm-8-8 - disabling barriers\n",
       "VMCIUtil: Updating context id from 0xffffffff to 0x30531af1 on event 0.\n",
       "VMCIUtil: Updating context id from 0x30531af1 to 0x30531af1 on event 0.\n",
       "RPC: bad TCP reclen 0x7ff4fffd (large)\n",
       "\n",
       "\n",
       "saepap03(RHEL6.10)\n",
       "NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory\n",
       "NFSD: starting 90-second grace period\n",
       "RPC: fragment too large: 2146762749</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col4\" class=\"data row62 col4\" >['syslog server salogp12(nfs client', 'various nfs mounts', 'issue intermittent', 'issues', 'rhel5.11']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow62_col5\" class=\"data row62 col5\" >['syslog server salogp12(nfs client', 'various nfs mounts', 'issue intermittent', 'issues', 'rhel5.11', 'nfsv4 state recovery directory nfsd', 'syslog server salogp12(nfs client', 'various nfs mounts', 'server sawasp05', 'nfs servers', 'server saepap03', 'server sawasp07', 'nfs', 'grace period jbd', 'grace period rpc']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row63\" class=\"row_heading level0 row63\" >63</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col0\" class=\"data row63 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col1\" class=\"data row63 col1\" >02257668</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col2\" class=\"data row63 col2\" >NFS version 3 is hanging</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col3\" class=\"data row63 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Running the following command used to mount the NFS server, but now it does not. The fstab entry had to modified to remove nfsvers=3 to be able to mount. Otherwise it hung.  The following command would not mount\n",
       "\n",
       "mount -t nfs epnfl-q001s0:/nfsdata/hmsoft /hmsoft -o nfsvers=3,defaults,soft\n",
       "\n",
       "however this worked\n",
       "mount -t nfs epnfl-q001s0:/nfsdata/hmsoft /hmsoft -o defaults,soft\n",
       "\n",
       "This used to work and I have other servers that are mounted this way. However unmounted and remounting fails. Please advise. \n",
       "\n",
       "qlkannga001 is the client\n",
       "epnfl-q001s0 is the server\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "QA, Testing\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatably and frequent.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "It is important</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col4\" class=\"data row63 col4\" >['nfs version']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow63_col5\" class=\"data row63 col5\" >['nfs version', 'mount -t nfs epnfl', 'client epnfl', 'nfs server', 'epnfl', 'fstab entry', 'nfs', 'other servers', '-t', 'hmsoft', 'q001s0:/nfsdata']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row64\" class=\"row_heading level0 row64\" >64</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col0\" class=\"data row64 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col1\" class=\"data row64 col1\" >02102349</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col2\" class=\"data row64 col2\" >vendor is reporting they believe they are seeing disk I/O latency on the server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col3\" class=\"data row64 col3\" >We have a VM with RedHat 6.9 installed.  This server was recently YUM'd. The server also has SQL installed on it.  Their SQL queries have gone from taking 20 minutes to a couple hours.  They believe it is related to I/O latency.  Server has been rebooted and it did improve slightly, but it is definitely not back to where it was.  I need someone from RedHat to take a look to see if there are any changes or updates that we need to do to the VM/Server configuration.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col4\" class=\"data row64 col4\" >['i', 'disk', 'server', 'latency', 'vendor']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow64_col5\" class=\"data row64 col5\" >['i', 'disk', 'server', 'latency', 'vendor', 'server configuration', 'sql queries', 'server', 'couple hours', 'sql', 'redhat', 'minutes', 'vm', \"yum'd\", 'look']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row65\" class=\"row_heading level0 row65\" >65</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col0\" class=\"data row65 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col1\" class=\"data row65 col1\" >02163089</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col2\" class=\"data row65 col2\" >kernel: RPC: fragment too large</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col3\" class=\"data row65 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We have seen huge numbers of \"kernel: RPC: fragment too large\" messages in our varies numbers (19 so far) of Redhat VMs from 6.9 to 7.5.  All 19 Redhat VMs have NFS shares mounted up.\n",
       "\n",
       "We are suspecting these error messages might be able to help explain the corrupted NFS files that we've been experiencing and which are logged in case #02151157.\n",
       "\n",
       "Please see the attach spreadsheet where I captured from Splunk. You can find huge numbers of RPC:fragment too large messages. The fragment size ranges from a couple of hundred MB to nearly 2 GB, which quite confused me as I know the largest rsize/wsize block size is only 1MB, Why is it supposed to be such large fragment size in these alerts?\n",
       "\n",
       "Let me know if you need more files/logs.\n",
       "\n",
       "Thanks and regards,\n",
       "\n",
       "Richard</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col4\" class=\"data row65 col4\" >['large', 'fragment', 'rpc', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow65_col5\" class=\"data row65 col5\" >['large', 'fragment', 'rpc', 'kernel', 'such large fragment size', 'fragment size', 'large messages', 'wsize block size', 'huge numbers', 'varies numbers', 'fragment', 'error messages', 'large', 'more files']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row66\" class=\"row_heading level0 row66\" >66</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col0\" class=\"data row66 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col1\" class=\"data row66 col1\" >02502364</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col2\" class=\"data row66 col2\" >kernel: nfsd: peername failed (err 107)!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col3\" class=\"data row66 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Both application Server and NFS server are on different VLAN's. We have ports opened for NFS communication from APP to NFS box . This setup was working for a while and all of sudden started throwing issues. When we moved NFS to same VLAN as APP , it started working.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col4\" class=\"data row66 col4\" >['err', 'peername', 'nfsd', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow66_col5\" class=\"data row66 col5\" >['err', 'peername', 'nfsd', 'kernel', 'nfs server', 'nfs box', 'nfs communication', 'nfs', 'different vlan', 'same vlan', 'application server', 'app', 'ports', 'setup']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row67\" class=\"row_heading level0 row67\" >67</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col0\" class=\"data row67 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col1\" class=\"data row67 col1\" >02002308</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col2\" class=\"data row67 col2\" >fragment too large errors in logs due to that many  servers are hanging.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col3\" class=\"data row67 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We are getting below errors in demsg due to that many servers are hanging and not able to work .\n",
       "\n",
       "TCP: Peer 0000:0000:0000:0000:0000:ffff:0ab6:848e:51953/80 unexpectedly shrunk window 2906370894:2906373790 (repaired)\n",
       "TCP: Peer 10.159.236.243:2049/968 unexpectedly shrunk window 1286497629:1286497700 (repaired)\n",
       "TCP: Peer 10.159.236.243:2049/968 unexpectedly shrunk window 1286497629:1286497700 (repaired)\n",
       "TCP: Peer 10.159.236.243:2049/968 unexpectedly shrunk window 1286497629:1286497700 (repaired)\n",
       "RPC: fragment too large: 369295616\n",
       "RPC: fragment too large: 369295360\n",
       "RPC: fragment too large: 369295616\n",
       "RPC: fragment too large: 369295872\n",
       "RPC: fragment too large: 369296128\n",
       "RPC: fragment too large: 1131571572\n",
       "RPC: fragment too large: 1195725856</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col4\" class=\"data row67 col4\" >['logs due', 'large errors', 'many', 'servers', 'fragment']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow67_col5\" class=\"data row67 col5\" >['logs due', 'large errors', 'many', 'servers', 'fragment', 'many servers', 'fragment', 'tcp', 'window', 'peer', 'rpc', 'large', 'demsg', 'errors', 'able']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row68\" class=\"row_heading level0 row68\" >68</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col0\" class=\"data row68 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col1\" class=\"data row68 col1\" >02364016</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col2\" class=\"data row68 col2\" >A NFS that is mounted is causing latency problems.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col3\" class=\"data row68 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We've received error messages saying the O/S is having difficulties writing to this NFS:\n",
       "\n",
       "d2-ad-infs04.ad.sec.gov:/ifs/data/nfs/OFMW_AppFiles01  /u01/app/oracle/product              nfs  rw,bg,rsize=32768,wsize=32768,hard,intr,tcp,noacl,timeo=600 0 0\n",
       "\n",
       "\n",
       "Apr 17 00:34:04 d1-sec-prodoem2 kernel: RPC: fragment too large: 50331667\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 1195725856\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 1195725856\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 369295618\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 369295618\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "We're also seeing these error messages in /var/log/messages\n",
       "Apr 17 00:33:41 d1-sec-prodoem2 kernel: RPC: fragment too large: 1195725856\n",
       "Apr 17 00:34:04 d1-sec-prodoem2 kernel: RPC: fragment too large: 50331667\n",
       "Apr 17 00:34:04 d1-sec-prodoem2 kernel: RPC: fragment too large: 50331667\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 1195725856\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 1195725856\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 369295618\n",
       "Apr 17 00:34:24 d1-sec-prodoem2 kernel: RPC: fragment too large: 369295618</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col4\" class=\"data row68 col4\" >['latency problems', 'nfs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow68_col5\" class=\"data row68 col5\" >['latency problems', 'nfs', 'prodoem2 kernel', 'nfs', 'error messages', 'apr', 'd1-sec', 'large', 'rpc', 'fragment', 'rw', 'bg']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row69\" class=\"row_heading level0 row69\" >69</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col0\" class=\"data row69 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col1\" class=\"data row69 col1\" >02171286</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col2\" class=\"data row69 col2\" >Error message in /var/log/messages (INC0043423)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col3\" class=\"data row69 col3\" >The error message below is seen repeatedly between 2200-0100 AEST. The issue occurs across several VMS and do not necessarily occur every night. We found the following KBA- https://access.redhat.com/solutions/2065483 which stipulate an issue with the nfs server. Could you advise additional steps on troubleshooting the fault. \n",
       "\n",
       " messages-20180826:Aug 24 00:08:37 mprasappr40 kernel: RPC: fragment too large: 369295618</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col4\" class=\"data row69 col4\" >['error message', 'inc0043423', 'messages', 'log']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow69_col5\" class=\"data row69 col5\" >['error message', 'inc0043423', 'messages', 'log', 'following kba- https://access.redhat.com/solutions/2065483', 'nfs server', 'mprasappr40 kernel', 'additional steps', 'several vms', 'issue', 'error message', 'aug', 'aest', 'night']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row70\" class=\"row_heading level0 row70\" >70</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col0\" class=\"data row70 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col1\" class=\"data row70 col1\" >02088085</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col2\" class=\"data row70 col2\" >RPC fragment too large</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col3\" class=\"data row70 col3\" >We are seeing an RPC fragment too larger error which is then seeming to cause an application that runs on this server to crash. Unfortunately we can't see where this fragment is coming from or what it is.\n",
       "\n",
       "Apr 26 14:39:50 cpi-ads-q04 kernel: RPC: fragment too large: 218762506\n",
       "Apr 26 14:39:50 cpi-ads-q04 kernel: RPC: fragment too large: 1195725856\n",
       "Apr 26 14:39:50 cpi-ads-q04 kernel: RPC: fragment too large: 1330664521\n",
       "Apr 26 14:39:50 cpi-ads-q04 kernel: RPC: fragment too large: 1330664521</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col4\" class=\"data row70 col4\" >['rpc fragment', 'large']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow70_col5\" class=\"data row70 col5\" >['rpc fragment', 'large', 'rpc fragment', 'q04 kernel', 'fragment', 'rpc', 'ads', 'cpi', 'large', 'apr', 'larger error', 'application']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row71\" class=\"row_heading level0 row71\" >71</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col0\" class=\"data row71 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col1\" class=\"data row71 col1\" >02083025</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col2\" class=\"data row71 col2\" >Apr 17 00:18:29 xczzqa0033 kernel: RPC: fragment too large: 369296129</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col3\" class=\"data row71 col3\" >We are receiving 'kernel: RPC: fragment too large' on 30 + Linux guests.\n",
       "This is happening all versions of RHEL 6.8 - 7.4\n",
       "Could you please help me determine the root cause and how to prevent it from happening.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col4\" class=\"data row71 col4\" >['xczzqa0033 kernel', 'fragment', 'rpc', 'large', 'apr']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow71_col5\" class=\"data row71 col5\" >['xczzqa0033 kernel', 'fragment', 'rpc', 'large', 'apr', 'linux guests', 'root cause', 'large', 'fragment', 'rpc', 'rhel', 'versions', 'kernel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row72\" class=\"row_heading level0 row72\" >72</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col0\" class=\"data row72 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col1\" class=\"data row72 col1\" >02393181</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col2\" class=\"data row72 col2\" >RPC: fragment too large:</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col3\" class=\"data row72 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We are having the following messages on kernel log:\n",
       "May 26 05:31:00 ppdmoeda07901 kernel: [22389038.544058] RPC: fragment too large: 1195725856\n",
       "May 26 05:31:19 ppdmoeda07901 kernel: [22389057.751310] RPC: fragment too large: 1224736768\n",
       "May 26 05:31:26 ppdmoeda07901 kernel: [22389064.796544] RPC: fragment too large: 1212501072\n",
       "May 26 05:32:54 ppdmoeda07901 kernel: [22389152.300726] RPC: fragment too large: 50331667\n",
       "May 26 05:36:37 ppdmoeda07901 kernel: [22389375.384710] RPC: fragment too large: 1195725856\n",
       "May 26 05:36:38 ppdmoeda07901 kernel: [22389375.873674] RPC: fragment too large: 369295618\n",
       "May 26 05:38:33 ppdmoeda07901 kernel: [22389491.134109] RPC: fragment too large: 1195725856\n",
       "May 26 05:38:34 ppdmoeda07901 kernel: [22389491.555625] RPC: fragment too large: 1195725856\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.502979] RPC: fragment too large: 369295618\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.531069] RPC: fragment too large: 369295360\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.564584] RPC: fragment too large: 369295618\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.592251] RPC: fragment too large: 2425088\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.624337] RPC: fragment too large: 369295618\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.656599] RPC: fragment too large: 369295618\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.689262] RPC: fragment too large: 369295618\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.718163] RPC: fragment too large: 369295362\n",
       "May 26 08:12:53 ppdmoeda07901 kernel: [22398743.746250] RPC: fragment too large: 4194560\n",
       "May 26 08:12:55 ppdmoeda07901 kernel: [22398745.490136] RPC: fragment too large: 1195725856\n",
       "May 26 08:13:15 ppdmoeda07901 kernel: [22398765.925879] RPC: fragment too large: 1212501072\n",
       "May 26 08:13:19 ppdmoeda07901 kernel: [22398769.494296] RPC: fragment too large: 50331667\n",
       "\n",
       "This seems result of high activity on the NFS.\n",
       "We unmount and re-mount the NFS but the issue still persists.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "This happens on production.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "This behavior happens always on the weekends when there is more activity on the NFS.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "It has no business impact.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col4\" class=\"data row72 col4\" >['large', 'fragment', 'rpc']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow72_col5\" class=\"data row72 col5\" >['large', 'fragment', 'rpc', 'ppdmoeda07901 kernel', 'kernel log', 'fragment', 'large', 'may', 'rpc', 'high activity', 'nfs', 'result', 'issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row73\" class=\"row_heading level0 row73\" >73</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col0\" class=\"data row73 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col1\" class=\"data row73 col1\" >02078244</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col2\" class=\"data row73 col2\" >NFS: RPC: fragment too large</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col3\" class=\"data row73 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "NFS issue, Io wait jumps and NFS hangs\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "On the NFS Client/production envirnment.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "unknown\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "this impacting production and customer</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col4\" class=\"data row73 col4\" >['large', 'fragment', 'rpc', 'nfs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow73_col5\" class=\"data row73 col5\" >['large', 'fragment', 'rpc', 'nfs', 'nfs issue', 'nfs', 'jumps', 'io']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row74\" class=\"row_heading level0 row74\" >74</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col0\" class=\"data row74 col0\" >2065483</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col1\" class=\"data row74 col1\" >01947984</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col2\" class=\"data row74 col2\" >Server facing performance issue.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col3\" class=\"data row74 col3\" >Server facing performance issue, and it is taking time to login to server. Kindly assist on it.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col4\" class=\"data row74 col4\" >['performance issue', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow74_col5\" class=\"data row74 col5\" >['performance issue', 'server', 'performance issue', 'time', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row75\" class=\"row_heading level0 row75\" >75</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col0\" class=\"data row75 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col1\" class=\"data row75 col1\" >02178396</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col2\" class=\"data row75 col2\" >multiple issues with server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col3\" class=\"data row75 col3\" >Server is hosted on xen host. We tried booting the server, once we server is booted we are not able to login to the server. While booting it shows multiple errors. We can also see some FS issues with some lv's.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col4\" class=\"data row75 col4\" >['multiple issues', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow75_col5\" class=\"data row75 col5\" >['multiple issues', 'server', 'fs issues', 'multiple errors', 'server', 'xen host', 'lv', 'able']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row76\" class=\"row_heading level0 row76\" >76</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col0\" class=\"data row76 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col1\" class=\"data row76 col1\" >02044499</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col2\" class=\"data row76 col2\" >file system  keeps on getting corrupted</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col3\" class=\"data row76 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "/tmp file system in the system keep on getting corrupted we need to find the root cause cause why it is happening and we need to provide solution for this issue\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "frequently it is been observed\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "frequently\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "due to this issue application stop working</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col4\" class=\"data row76 col4\" >['file system']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow76_col5\" class=\"data row76 col5\" >['file system', 'root cause cause', 'file system', 'issue', 'solution', 'system']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row77\" class=\"row_heading level0 row77\" >77</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col0\" class=\"data row77 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col1\" class=\"data row77 col1\" >01742082</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col2\" class=\"data row77 col2\" >File system frequently going to read-only mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col3\" class=\"data row77 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "we have a filesystem on LV with PV as SAN LUN .This filesystem is frequetly going to readonly mode .\n",
       "the issue gets cleared after we remount the file system .\n",
       "But it reoccur after a certain period .\n",
       "\n",
       "Filesystem            Size  Used Avail Use% Mounted on\n",
       "/dev/mapper/sanvg-sanapps\n",
       "                      493G   89G  379G  20% /apps3\n",
       "\n",
       "  PV                                           VG     Fmt  Attr PSize   PFree\n",
       "  /dev/mpath/360060e80100a84200530204a00000064 sanvg  lvm2 a--  500.00G      0\n",
       "  /dev/mpath/360060e80100a84200530204a00000065 sanvg  lvm2 a--  500.00G 499.99G\n",
       "  /dev/sda2                                    rootvg lvm2 a--  135.59G   6.59G\n",
       "\n",
       "  LV        VG     Attr   LSize   Origin Snap%  Move Log Copy%  Convert\n",
       "  appsVol00 rootvg -wi-a-  25.00G\n",
       "  homeVol00 rootvg -wi-ao  10.00G\n",
       "  rootVol00 rootvg -wi-ao  30.00G\n",
       "  swapVol00 rootvg -wi-ao  24.00G\n",
       "  tmpVol00  rootvg -wi-ao  10.00G\n",
       "  varVol00  rootvg -wi-ao  30.00G\n",
       "  sanapps   sanvg  -wi-ao 500.00G\n",
       "\n",
       "360060e80100a84200530204a00000065 dm-7 HITACHI,DF600F\n",
       "[size=500G][features=0][hwhandler=0][rw]\n",
       "\\_ round-robin 0 [prio=1][active]\n",
       " \\_ 5:0:0:1 sde 8:64  [active][ready]\n",
       "\\_ round-robin 0 [prio=0][enabled]\n",
       " \\_ 3:0:0:1 sdc 8:32  [active][ready]\n",
       "360060e80100a84200530204a00000064 dm-6 HITACHI,DF600F\n",
       "[size=500G][features=0][hwhandler=0][rw]\n",
       "\\_ round-robin 0 [prio=1][active]\n",
       " \\_ 3:0:0:0 sdb 8:16  [active][ready]\n",
       "\\_ round-robin 0 [prio=0][enabled]\n",
       " \\_ 5:0:0:0 sdd 8:48  [active][ready]\n",
       "\n",
       "pesitent biding not configured\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "in Production Server\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Frequently and repeatedly\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "--------\n",
       "EXT3-fs error (device dm-8): ext3_lookup: unlinked inode 61947905 in dir #56295428\n",
       "Aborting journal on device dm-8.\n",
       "ext3_abort called.\n",
       "EXT3-fs error (device dm-8): ext3_journal_start_sb: Detected aborted journal\n",
       "Remounting filesystem read-only\n",
       "attempt to access beyond end of device\n",
       "dm-8: rw=0, want=7411860248, limit=1048576000\n",
       "\n",
       "--------------------------------</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col4\" class=\"data row77 col4\" >['file system', 'mode']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow77_col5\" class=\"data row77 col5\" >['file system', 'mode', 'g', 'appsvol00 rootvg', 'move log copy%', 'ao', 'rootvg', 'san lun .this', 'attr psize', 'round', 'robin', 'mpath/360060e80100a84200530204a00000065 sanvg']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row78\" class=\"row_heading level0 row78\" >78</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col0\" class=\"data row78 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col1\" class=\"data row78 col1\" >02079037</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col2\" class=\"data row78 col2\" >Cluster Unexpectedly getting Down</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col3\" class=\"data row78 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Cluster Unexpectedly getting Down with below error\n",
       "EXT3-fs: mounted filesystem with ordered data mode.\n",
       "Apr 16 19:13:39 drnclinux kernel: kjournald starting.  Commit interval 5 seconds\n",
       "Apr 16 19:13:39 drnclinux kernel: EXT3-fs warning: maximal mount count reached, running e2fsck is recommended\n",
       "Apr 16 19:13:39 drnclinux kernel: EXT3 FS on dm-26, internal journal\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Prod\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "SOS report</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col4\" class=\"data row78 col4\" >['cluster']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow78_col5\" class=\"data row78 col5\" >['cluster', 'drnclinux kernel', 'maximal mount count', 'ext3-fs warning', 'error ext3-fs', 'apr', 'data mode', 'commit interval', 'ext3 fs', 'internal journal', 'e2fsck']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row79\" class=\"row_heading level0 row79\" >79</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col0\" class=\"data row79 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col1\" class=\"data row79 col1\" >01715731</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col2\" class=\"data row79 col2\" >Cluster service crashed</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col3\" class=\"data row79 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "I can see the following in the messages file\n",
       "Oct  5 10:46:44 lbblxs01 smbd[13166]:   Denied connection from  (172.16.3.42)\n",
       "Oct  5 11:09:11 lbblxs01 kernel: EXT3-fs error (device dm-74): ext3_free_inode: bit already cleared for inode 545934\n",
       "Oct  5 11:09:11 lbblxs01 kernel: Aborting journal on device dm-74.\n",
       "Oct  5 11:09:11 lbblxs01 kernel: EXT3-fs error (device dm-74) in ext3_delete_inode: IO failure\n",
       "Oct  5 11:09:11 lbblxs01 kernel: ext3_abort called.\n",
       "Oct  5 11:09:11 lbblxs01 kernel: EXT3-fs error (device dm-74): ext3_journal_start_sb: Detected aborted journal\n",
       "Oct  5 11:09:11 lbblxs01 kernel: Remounting filesystem read-only\n",
       "Oct  5 11:09:42 lbblxs01 clurgmgrd: [22102]: <err> fs:UNITestOraHome: isAlive failed write test on [/UNITEST/ora]. Return code: 1\n",
       "Oct  5 11:09:42 lbblxs01 clurgmgrd: [22102]: <err> fs:UNITestOraHome: Mount point is not accessible!\n",
       "Oct  5 11:09:42 lbblxs01 clurgmgrd[22102]: <notice> status on fs \"UNITestOraHome\" returned 1 (generic error)\n",
       "Oct  5 11:09:42 lbblxs01 clurgmgrd[22102]: <notice> Stopping service service:UNITEST\n",
       "Oct  5 11:09:53 lbblxs01 clurgmgrd: [22102]: <err> script:UNITestOraCtl: stop of /usr/local/bin/oractl.UNITEST failed (returned 1)\n",
       "Oct  5 11:09:53 lbblxs01 clurgmgrd[22102]: <notice> stop on script \"UNITestOraCtl\" returned 1 (generic error)\n",
       "Oct  5 11:09:53 lbblxs01 avahi-daemon[21668]: Withdrawing address record for 172.16.6.22 on br0.\n",
       "Oct  5 11:10:03 lbblxs01 multipathd: dm-76: umount map (uevent)\n",
       "Oct  5 11:10:04 lbblxs01 multipathd: dm-75: umount map (uevent)\n",
       "Oct  5 11:10:04 lbblxs01 multipathd: dm-74: umount map (uevent)\n",
       "Oct  5 11:10:04 lbblxs01 kernel: ext3_abort called.\n",
       "Oct  5 11:10:04 lbblxs01 kernel: EXT3-fs error (device dm-74): ext3_put_super: Couldn't clean up the journal\n",
       "Oct  5 11:10:04 lbblxs01 clurgmgrd[22102]: <crit> #12: RG service:UNITEST failed to stop; intervention required\n",
       "Oct  5 11:10:04 lbblxs01 clurgmgrd[22102]: <notice> Service service:UNITEST is failed\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27994]: [2016/10/05 11:13:48, 0] lib/access.c:check_access(327)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27994]:   Denied connection from  (172.16.4.80)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27996]: [2016/10/05 11:13:48, 0] lib/access.c:check_access(327)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27996]:   Denied connection from  (172.16.4.80)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27998]: [2016/10/05 11:13:48, 0] lib/access.c:check_access(327)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27998]:   Denied connection from  (172.16.4.80)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27999]: [2016/10/05 11:13:48, 0] lib/access.c:check_access(327)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[27999]:   Denied connection from  (172.16.4.80)\n",
       "Oct  5 11:13:48 lbblxs01 smbd[28000]: [2016/10/05 11:13:48, 0] lib/access.c:check_access(327)\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "lbblxs01\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "The cluster was running the service fine,  all i have done is shutdown the database manually.  ?  i have not tun any cluster commands at all, now reports\n",
       "\n",
       "Member Status: Quorate\n",
       "\n",
       " Member Name                                                   ID   Status\n",
       " ------ ----                                                   ---- ------\n",
       " lbblxs01-hb                                                       1 Online, Local, rgmanager\n",
       " lbblxs02-hb                                                       2 Online, rgmanager\n",
       " lbblxs03-hb                                                       3 Online, rgmanager\n",
       " /dev/dm-65                                                        0 Online, Quorum Disk\n",
       "\n",
       " Service Name                                         Owner (Last)                                         State\n",
       " ------- ----                                         ----- ------                                         -----\n",
       " service:FISLIVE                                      lbblxs02-hb                                          started\n",
       " service:FISTEST                                      lbblxs02-hb                                          started\n",
       " service:FISUAT                                       lbblxs02-hb                                          started\n",
       " service:OLMLIVE                                      lbblxs02-hb                                          started    [Z]\n",
       " service:OLMTEST                                      lbblxs02-hb                                          started    [Z]\n",
       " service:OLMTRAIN                                     lbblxs02-hb                                          started    [Z]\n",
       " service:UNILIVE                                      lbblxs01-hb                                          started\n",
       " service:UNITEST                                      (lbblxs01-hb)                                        failed\n",
       " service:UNIUAT                                       lbblxs01-hb                                          started\n",
       "[root@lbblxs01 ~]# view /var/log/messages\n",
       "\n",
       "\n",
       "These are the steps i run be</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col4\" class=\"data row79 col4\" >['cluster service']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow79_col5\" class=\"data row79 col5\" >['cluster service', 'lbblxs01 kernel', 'lbblxs01 clurgmgrd[22102', 'lbblxs01 clurgmgrd', 'messages file oct', 'lbblxs01 multipathd', 'journal oct', 'lbblxs01 smbd[27994', 'lbblxs01 avahi', 'lbblxs01 smbd[27999', 'lbblxs01 smbd[27998']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row80\" class=\"row_heading level0 row80\" >80</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col0\" class=\"data row80 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col1\" class=\"data row80 col1\" >02040890</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col2\" class=\"data row80 col2\" >The file system became read-only</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col3\" class=\"data row80 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "The file system /usr/sap became read-only on Feb 8 (Feb 7 GMT). We rebooted the server to fix the read-only file system issue.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "PROD server\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "It happened on Feb 8th\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "We had to reboot the server to fix the Read-only file system issue.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col4\" class=\"data row80 col4\" >['file system']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow80_col5\" class=\"data row80 col5\" >['file system', 'file system issue', 'file system', 'feb', 'server', 'read', 'sap', 'gmt']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row81\" class=\"row_heading level0 row81\" >81</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col0\" class=\"data row81 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col1\" class=\"data row81 col1\" >01698639</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col2\" class=\"data row81 col2\" >[FIlesystem entered in read-only] Filesystem entrou em read-only</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col3\" class=\"data row81 col3\" >[English]\n",
       "\n",
       "Server has a volume from a IBM DS storage in ex3 (lvm). This filesystem entered in read-only 3 times since yesterday:\n",
       "\n",
       "06/Sep\n",
       "04:12am\n",
       "10:36am\n",
       "\n",
       "05/Sep\n",
       "12:20\n",
       "\n",
       "Need root cause and procedures to avoid it.\n",
       "\n",
       "There are no recent changes.\n",
       "\n",
       "Multipath is not in use.\n",
       "\n",
       "[Portuguese]\n",
       "\n",
       "Servidor tem um volume do storage IBM DS em ext3 (lvm). Esse filesystem entrou em read-only 3 vezes desde ontem:\n",
       "\n",
       "06/Set\n",
       "04:12am\n",
       "10:36am\n",
       "\n",
       "05/Set\n",
       "12:20\n",
       "\n",
       "É necessário a causa raíz e os procedimentos para evitar isso.\n",
       "\n",
       "Nenhuma alteração foi realizada.\n",
       "\n",
       "Multipath não está em uso.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col4\" class=\"data row81 col4\" >['filesystem entrou', 'filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow81_col5\" class=\"data row81 col5\" >['filesystem entrou', 'filesystem', 'causa raíz e os procedimentos para evitar isso', 'nenhuma alteração foi realizada', 'storage ibm ds', 'ibm ds storage', 'esse filesystem entrou', 'set 04:12am', 'sep 04:12am', 'vezes desde ontem', 'está em uso', 'filesystem']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row82\" class=\"row_heading level0 row82\" >82</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col0\" class=\"data row82 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col1\" class=\"data row82 col1\" >01804381</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col2\" class=\"data row82 col2\" >Filesystem showing EXT3-fs error</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col3\" class=\"data row82 col3\" >We are getting the following filesystem error in one of our host. Please assist to resolve.\n",
       "\n",
       "[root@mdsildb2 log]# cat messages | grep ext\n",
       "Mar  5 22:05:18 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7880730 in dir #7323649\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 1687582 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 1687577 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 5881858 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 3915777 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 3915786 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 1687579 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 3915785 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 1687580 in dir #311298\n",
       "Mar  5 22:05:19 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 1687578 in dir #311298\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340082 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340080 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340085 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340079 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340084 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340083 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340078 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340086 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340081 in dir #7323649\n",
       "Mar  6 07:04:20 mdsildb2 kernel: EXT3-fs error (device dm-0): ext3_lookup: unlinked inode 7340077 in dir #7323649</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col4\" class=\"data row82 col4\" >['ext3-fs error', 'filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow82_col5\" class=\"data row82 col5\" >['ext3-fs error', 'filesystem', 'ext3-fs error', 'mdsildb2 kernel', 'device dm-0', 'unlinked inode', 'dir #', 'cat messages | grep ext', 'filesystem error', 'mar', 'ext3_lookup', 'dir']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row83\" class=\"row_heading level0 row83\" >83</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col0\" class=\"data row83 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col1\" class=\"data row83 col1\" >01720553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col2\" class=\"data row83 col2\" >Customer has a mount that keeps going readonly.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col3\" class=\"data row83 col3\" >Reboots server and comes up fine, but the mount point starts readonly. Tries to unmount it and remount it, and can't get it to remount. Wants to know why it goes readonly. Keeps oracle from working properly.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col4\" class=\"data row83 col4\" >['mount', 'customer']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow83_col5\" class=\"data row83 col5\" >['mount', 'customer', 'mount point', 'tries', 'reboots', 'oracle']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row84\" class=\"row_heading level0 row84\" >84</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col0\" class=\"data row84 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col1\" class=\"data row84 col1\" >02374105</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col2\" class=\"data row84 col2\" >There Disk Missing</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col3\" class=\"data row84 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We failed to do disk expansion, there are disk corrupted\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Linux \n",
       "/opt/app\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "At certain time\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Current\n",
       "\n",
       "[root@esesslx0579 ~]# pvs\n",
       "  Couldn't find device with uuid ZyTGlL-zayU-QOd2-BZNg-y1HV-gYVa-1eqTQK.\n",
       "  PV             VG     Fmt  Attr PSize   PFree\n",
       "  /dev/sda2      vg00   lvm2 a--   68.50g   4.75g\n",
       "  /dev/sdc       vg_opt lvm2 a--  400.00g      0\n",
       "  /dev/sdd       vg_opt lvm2 a--  499.00g      0\n",
       "  /dev/sde       vg_opt lvm2 a--  500.00g      0\n",
       "  /dev/sdf       vg_opt lvm2 a--  100.00g      0\n",
       "  /dev/sdg       vg_opt lvm2 a--  600.00g      0\n",
       "  /dev/sdh       vg_opt lvm2 a--  500.00g      0\n",
       "  /dev/sdi       vg_opt lvm2 a--  200.00g      0\n",
       "  /dev/sdj       vg_opt lvm2 a--  210.00g      0\n",
       "  /dev/sdk       vg_opt lvm2 a--  500.00g      0\n",
       "  /dev/sdl       vg_opt lvm2 a--  500.00g      0\n",
       "  /dev/sdm       vg_opt lvm2 a--  500.00g      0\n",
       "  /dev/sdn       vg_opt lvm2 a--  500.00g      0\n",
       "  /dev/sdo       vg_opt lvm2 a--  500.00g   8.00m\n",
       "  /dev/sdp       vg_opt lvm2 a--  500.00g 500.00g\n",
       "  unknown device vg_opt lvm2 a-m   99.00g      0\n",
       "[root@esesslx0579 ~]#\n",
       "\n",
       "\n",
       "[root@esesslx0579 log]# pvs\n",
       "  PV         VG     Fmt  Attr PSize   PFree  \n",
       "  /dev/sda2  vg00   lvm2 a--   68.50g   4.75g\n",
       "  /dev/sdb   vg_opt lvm2 a--   99.00g      0 \n",
       "  /dev/sdc   vg_opt lvm2 a--  400.00g      0 \n",
       "  /dev/sdd   vg_opt lvm2 a--  499.00g      0 \n",
       "  /dev/sde   vg_opt lvm2 a--  500.00g      0 \n",
       "  /dev/sdf   vg_opt lvm2 a--  100.00g      0 \n",
       "  /dev/sdg   vg_opt lvm2 a--  600.00g      0 \n",
       "  /dev/sdh   vg_opt lvm2 a--  500.00g      0 \n",
       "  /dev/sdi   vg_opt lvm2 a--  200.00g      0 \n",
       "  /dev/sdj   vg_opt lvm2 a--  210.00g      0 \n",
       "  /dev/sdk   vg_opt lvm2 a--  500.00g      0 \n",
       "  /dev/sdl   vg_opt lvm2 a--  500.00g      0 \n",
       "  /dev/sdm   vg_opt lvm2 a--  500.00g      0 \n",
       "  /dev/sdn   vg_opt lvm2 a--  500.00g 512.00m</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col4\" class=\"data row84 col4\" >['disk missing']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow84_col5\" class=\"data row84 col5\" >['disk missing', 'disk expansion', 'disk']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row85\" class=\"row_heading level0 row85\" >85</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col0\" class=\"data row85 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col1\" class=\"data row85 col1\" >02376102</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col2\" class=\"data row85 col2\" >root cause for disk corruption</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col3\" class=\"data row85 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "- request for root case for disk corruption  in esesslx0579.ss.sw.ericsson.se\n",
       "- refer to   02374105\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "4th - 7th May 2019</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col4\" class=\"data row85 col4\" >['disk corruption', 'root cause']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow85_col5\" class=\"data row85 col5\" >['disk corruption', 'root cause', 'disk corruption', 'root case', 'esesslx0579.ss.sw.ericsson.se', 'refer', 'request']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row86\" class=\"row_heading level0 row86\" >86</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col0\" class=\"data row86 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col1\" class=\"data row86 col1\" >02353987</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col2\" class=\"data row86 col2\" >One of FS goes  in read only mode  frequntly  . We need to find it why its happening .</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col3\" class=\"data row86 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "DB File system went in read only mode . After umount/mount FS issue getting resolved but after some duration again same issue reoccur .  \n",
       "ukortdhr:root:/root # cat /proc/mounts | grep -i ro,\n",
       "/dev/mapper/vg1-appgrid_vol /app/grid ext3 ro,data=ordered 0 0\n",
       "ukortdhr:root:/root #\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "We need to find why its happening and work around in order to fix the issue .\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col4\" class=\"data row86 col4\" >['mode', 'read', 'fs', 'happening']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow86_col5\" class=\"data row86 col5\" >['mode', 'read', 'fs', 'happening', 'mounts | grep -i ro', 'mount fs issue', 'same issue reoccur', 'grid ext3 ro', 'db file system', '# cat', 'ukortdhr', 'mode', 'read', 'mapper']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row87\" class=\"row_heading level0 row87\" >87</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col0\" class=\"data row87 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col1\" class=\"data row87 col1\" >02103846</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col2\" class=\"data row87 col2\" >Filesystem goes in Read only mode while performing DR activity</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col3\" class=\"data row87 col3\" >On server RG546 one of the filesystem goes in Read only mode while performing DR activity.\n",
       "Due to that delay happened to brought up the services which causes impact to customer\n",
       "We need immediate action on this.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col4\" class=\"data row87 col4\" >['dr activity', 'mode', 'read', 'filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow87_col5\" class=\"data row87 col5\" >['dr activity', 'mode', 'read', 'filesystem', 'dr activity', 'immediate action', 'server rg546', 'read', 'filesystem', 'mode', 'impact', 'services', 'delay', 'customer']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row88\" class=\"row_heading level0 row88\" >88</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col0\" class=\"data row88 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col1\" class=\"data row88 col1\" >01791831</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col2\" class=\"data row88 col2\" >file system went to read-only mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col3\" class=\"data row88 col3\" >file system went to read-only mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col4\" class=\"data row88 col4\" >['file system', 'mode']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow88_col5\" class=\"data row88 col5\" >['file system', 'mode', 'file system', 'mode']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row89\" class=\"row_heading level0 row89\" >89</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col0\" class=\"data row89 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col1\" class=\"data row89 col1\" >01781952</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col2\" class=\"data row89 col2\" >Deleted inode error</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col3\" class=\"data row89 col3\" >We are getting  ext3_lookup : deleted inode error in one of our servers. It gets into hung state every week and after a reboot, it again starts giving the same error till it goes into hung state.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col4\" class=\"data row89 col4\" >['inode error']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow89_col5\" class=\"data row89 col5\" >['inode error', 'hung state', 'same error', 'inode error', 'servers', 'reboot', 'week', 'ext3_lookup']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row90\" class=\"row_heading level0 row90\" >90</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col0\" class=\"data row90 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col1\" class=\"data row90 col1\" >02399658</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col2\" class=\"data row90 col2\" >We are getting error >>>> device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col3\" class=\"data row90 col3\" >Hi Team ,\n",
       "\n",
       "Could you please let us know why we are getting this messages in /var/log/message .\n",
       "\n",
       "Jun  7 04:02:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:02:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:04:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:04:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:06:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:06:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:08:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:08:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:10:56 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:10:56 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:12:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:12:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:14:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:14:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:16:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:16:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:18:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:18:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:20:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:20:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:22:52 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:22:52 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:24:52 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:24:52 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:26:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:26:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:28:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:28:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:30:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:30:55 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:32:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:32:54 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:34:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:34:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:36:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:36:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:38:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:38:53 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913\n",
       "Jun  7 04:41:52 li03b03 kernel: EXT3-fs error (device dm-37): ext3_lookup: unlinked inode 507931 in dir #507913</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col4\" class=\"data row90 col4\" >['unlinked inode', 'device dm-37', 'dir #', 'ext3_lookup', 'error']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow90_col5\" class=\"data row90 col5\" >['unlinked inode', 'device dm-37', 'dir #', 'ext3_lookup', 'error', 'li03b03 kernel', 'ext3-fs error', 'device dm-37', 'unlinked inode', 'dir #', 'jun', 'ext3_lookup', 'message', 'log', 'messages']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row91\" class=\"row_heading level0 row91\" >91</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col0\" class=\"data row91 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col1\" class=\"data row91 col1\" >02367635</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col2\" class=\"data row91 col2\" >Server is getting hung very frequently, attached console screenshot</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col3\" class=\"data row91 col3\" >Team,\n",
       "\n",
       "sever is getting very frequently and when we reboot from console sever is coming up and in some time it is again going into hung state, RHEL - 5.11\n",
       "error we could see as : \"Starting udev: Kernel panic - not syncing: Out of memory and no killable process\"</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col4\" class=\"data row91 col4\" >['console screenshot', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow91_col5\" class=\"data row91 col5\" >['console screenshot', 'server', 'hung state', 'console sever', 'killable process', 'kernel panic', 'memory', 'time', 'rhel', 'sever', 'error', 'team']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row92\" class=\"row_heading level0 row92\" >92</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col0\" class=\"data row92 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col1\" class=\"data row92 col1\" >02069132</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col2\" class=\"data row92 col2\" >Server not booting</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col3\" class=\"data row92 col3\" >We have rebooted the server and it is not coming back. Can  you please help us to bring up the server.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col4\" class=\"data row92 col4\" >[]</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow92_col5\" class=\"data row92 col5\" >['server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row93\" class=\"row_heading level0 row93\" >93</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col0\" class=\"data row93 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col1\" class=\"data row93 col1\" >02328123</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col2\" class=\"data row93 col2\" >/ is reached to 100% unable to delete files</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col3\" class=\"data row93 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "error:\n",
       "rm: cannot remove `jboss-5.1.0.GA/lgpl.html': Read-only file system\n",
       "rm: cannot remove `jboss-5.1.0.GA/JBossORG-EULA.txt': Read-only file system\n",
       "rm: cannot remove `jboss-5.1.0.GA/jar-versions.xml': Read-only file system\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "R&D environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "certain times\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "unable generate SOS report as well</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col4\" class=\"data row93 col4\" >['% unable', 'files']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow93_col5\" class=\"data row93 col5\" >['% unable', 'files', 'file system rm', 'file system', 'jboss-5.1.0.ga', 'rm', 'jar', 'jbossorg', 'eula.txt', 'lgpl.html', 'versions.xml', 'error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row94\" class=\"row_heading level0 row94\" >94</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col0\" class=\"data row94 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col1\" class=\"data row94 col1\" >02099854</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col2\" class=\"data row94 col2\" >EXT3 Filesystem keep on changing to read only mode from read write mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col3\" class=\"data row94 col3\" >Team,\n",
       "   We are facing an issue on below file system which is changed to RO frequently and this cause production issue.\n",
       "[root@hpnpldlpdb03 ~]# df -Th /ora/TPODLP2P/backup01\n",
       "Filesystem    Type    Size  Used Avail Use% Mounted on\n",
       "/dev/mapper/oraTPODLP2P_vg-backuplv\n",
       "              ext3     16T  9.4T  5.4T  64% /ora/TPODLP2P/backup01\n",
       "\n",
       "We noticed below error in /var/log/messages\n",
       "May 15 20:31:05 hpnpldlpdb03 kernel: Remounting filesystem read-only\n",
       "May 15 20:31:38 hpnpldlpdb03 kernel: printk: 16 messages suppressed.\n",
       "May 15 20:31:38 hpnpldlpdb03 kernel: EXT3-fs error (device dm-9): ext3_lookup: unlinked inode 115769357 in dir #115752961\n",
       "May 15 20:31:38 hpnpldlpdb03 kernel: EXT3-fs error (device dm-9): ext3_lookup: unlinked inode 115769372 in dir #115752961\n",
       "May 15 20:31:38 hpnpldlpdb03 kernel: EXT3-fs error (device dm-9): ext3_lookup: unlinked inode 115769376 in dir #115752961\n",
       "May 15 20:31:38 hpnpldlpdb03 kernel: EXT3-fs error (device dm-9): ext3_lookup: unlinked inode 115769350 in dir #115752961\n",
       "\n",
       "Based on the resolution available under https://access.redhat.com/solutions/24651, we unmounted the Filesystem and ran fsck and again mounted it. But still the issue is reoccurring.\n",
       "\n",
       "[root@hpnpldlpdb03 ~]# uname -a\n",
       "Linux hpnpldlpdb03 2.6.18-426.el5 #1 SMP Wed Feb 7 11:05:23 EST 2018 x86_64 x86_64 x86_64 GNU/Linux\n",
       "You have new mail in /var/spool/mail/root\n",
       "[root@hpnpldlpdb03 ~]# dmidecode -t1\n",
       "# dmidecode 2.12\n",
       "SMBIOS 2.8 present.\n",
       "\n",
       "Handle 0x0100, DMI type 1, 27 bytes\n",
       "System Information\n",
       "        Manufacturer: HP\n",
       "        Product Name: ProLiant BL460c Gen8\n",
       "        Version: Not Specified\n",
       "        Serial Number: VCY0P7PRHM\n",
       "        UUID: 627C5919-4BF9-463E-AA80-BA3D1696F5D6\n",
       "        Wake-up Type: Power Switch\n",
       "        SKU Number: 641016-B21\n",
       "        Family: ProLiant</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col4\" class=\"data row94 col4\" >['write mode', 'ext3 filesystem', 'mode']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow94_col5\" class=\"data row94 col5\" >['write mode', 'ext3 filesystem', 'mode', 'x86_64 x86_64 x86_64', 'uname -a linux hpnpldlpdb03', 'hpnpldlpdb03 kernel', 'proliant bl460c gen8', 'bytes system information', 'dmi type', 'ext3-fs error', 'cause production issue', 'dir #', 'serial number']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row95\" class=\"row_heading level0 row95\" >95</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col0\" class=\"data row95 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col1\" class=\"data row95 col1\" >01808816</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col2\" class=\"data row95 col2\" >File system is going in Read only mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col3\" class=\"data row95 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "File system is going in Read only mode twice for the past 4 days, We have rebooted the server to bring back to normal but we wanted to know what has caused the file system to go in read only mode. sosreport has been attached. Kindly take a look and provide the Root cause\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "RHEL 5.11\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "It occurred twice in past 4 days\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "yes there is business impact as we cant reboot the server frequently as it has Database running on it</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col4\" class=\"data row95 col4\" >['file system', 'mode', 'read']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow95_col5\" class=\"data row95 col5\" >['file system', 'mode', 'read', 'file system', 'read', 'root cause', 'sosreport', 'server', 'look', 'mode', 'days', 'past', 'normal']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row96\" class=\"row_heading level0 row96\" >96</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col0\" class=\"data row96 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col1\" class=\"data row96 col1\" >01801492</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col2\" class=\"data row96 col2\" >touch: cannot touch `test': Read-only file system</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col3\" class=\"data row96 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "we are unable to touch any files in production servers live production drive  /u10\n",
       "mount: block device /dev/VolGroup02/LogVolU10 is write-protected, mounting read-only\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "mount: block device /dev/VolGroup02/LogVolU10 is write-protected, mounting read-only\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "[root@au2rhorap05 u10]# touch test\n",
       "touch: cannot touch `test': Read-only file system\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "[root@au2rhorap05 u10]# touch test\n",
       "touch: cannot touch `test': Read-only file system</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col4\" class=\"data row96 col4\" >['file system', 'test', 'touch']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow96_col5\" class=\"data row96 col5\" >['file system', 'test', 'touch', 'production servers', 'production drive', 'block device', 'mount', 'volgroup02', 'files', 'logvolu10', 'write', 'unable']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row97\" class=\"row_heading level0 row97\" >97</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col0\" class=\"data row97 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col1\" class=\"data row97 col1\" >02090500</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col2\" class=\"data row97 col2\" >Inodes No.'s are missing of some files & getting continuously Error Logs for this.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col3\" class=\"data row97 col3\" >We are getting continuously inodes Error Logs for some inode numbers.  We have checked & find this directory & these files, which inodes are not available. This Node is in RHEL Cluster & LV (missing inodes files) is also in RHEL Cluster & shared on both Nodes.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col4\" class=\"data row97 col4\" >['error logs', 'files', 'inodes']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow97_col5\" class=\"data row97 col5\" >['error logs', 'files', 'inodes', 'rhel cluster', 'inode numbers', 'error logs', 'files', 'available', 'node', 'lv', 'inodes', 'directory', 'nodes']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row98\" class=\"row_heading level0 row98\" >98</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col0\" class=\"data row98 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col1\" class=\"data row98 col1\" >01766714</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col2\" class=\"data row98 col2\" >mount point /var read only fs</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col3\" class=\"data row98 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "- missing /etc/shadow > resolved with di grub /usr/sbin/pwconv, and /etc/shadow already generate. reset root password success.\n",
       "- fsck -y /dev/cciss/c0d0p3 > server can be access\n",
       "- replace disk > /var still read only fs\n",
       "how to resolved /var read only fs\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "we already follow this link https://access.redhat.com/solutions/24651\n",
       "\n",
       "environtment : ibm blade series</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col4\" class=\"data row98 col4\" >['mount point']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow98_col5\" class=\"data row98 col5\" >['mount point', 'reset root password success', 'fsck -y', 'di grub', 'replace disk', 'shadow', 'cciss', 'c0d0p3', 'server', 'sbin', 'pwconv']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row99\" class=\"row_heading level0 row99\" >99</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col0\" class=\"data row99 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col1\" class=\"data row99 col1\" >02304397</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col2\" class=\"data row99 col2\" >filesystem became readonly</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col3\" class=\"data row99 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "filesystem /dev/VolGroup00/LogVol02 became readonly last night\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "rebooted the server to force an fsck which repaired the filesystem, server is now operational again, but I'd like someone to investigate the attached sosreport to see if the cause can be identified.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "this is the 1st time we've had this issue with this server (8yrs old in May), i know its running RHEL5.11 ELS 2.6.18-434.el5 but its scheduled to be upgraded to RHEL6.10 in March.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 25700628 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: Aborting journal on device dm-3.\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 25691951 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 25584385 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 6573296 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 30580171 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 25568369 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 33817597 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 25497009 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_lookup: unlinked inode 6560851 in dir #30540645\n",
       "Jan 28 23:24:56 rhrdb045 kernel: ext3_abort called.\n",
       "Jan 28 23:24:56 rhrdb045 kernel: EXT3-fs error (device dm-3): ext3_journal_start_sb: Detected aborted journal\n",
       "Jan 28 23:24:56 rhrdb045 kernel: Remounting filesystem read-only\n",
       "Jan 28 23:38:33 rhrdb045 puppet-agent[26325]: Applied catalog in 0.05 seconds\n",
       "Jan 28 23:50:24 rhrdb045 kernel: __journal_remove_journal_head: freeing b_committed_data\n",
       "Jan 28 23:55:23 rhrdb045 kernel: __journal_remove_journal_head: freeing b_committed_data\n",
       "Jan 28 23:57:30 rhrdb045 kernel: __journal_remove_journal_head: freeing b_committed_data\n",
       "Jan 29 00:08:43 rhrdb045 puppet-agent[27951]: Applied catalog in 0.05 seconds</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col4\" class=\"data row99 col4\" >['filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow99_col5\" class=\"data row99 col5\" >['filesystem', 'last night', 'logvol02', 'volgroup00', 'filesystem']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row100\" class=\"row_heading level0 row100\" >100</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col0\" class=\"data row100 col0\" >24651</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col1\" class=\"data row100 col1\" >02299955</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col2\" class=\"data row100 col2\" >unable to rename volume group</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col3\" class=\"data row100 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "-bash-4.1# vgrename vg1sWS_70_fp15_fix1 vg1sWS_70_fp45\n",
       "  /dev/dasdb1: open failed: Read-only file system\n",
       "  Failed to write physical volume \"/dev/dasdb1\"\n",
       "  Failed to update old PV extension headers in VG vg1sWS_70_fp15_fix1.\n",
       "  Volume group \"vg1sWS_70_fp15_fix1\" not found\n",
       "  Cannot process volume group vg1sWS_70_fp15_fix1\n",
       "-bash-4.1# vmcp q 0301\n",
       "DASD 0301 9336 WAS002 R/W   16777200 BLK ON DASD  F002 SUBCHANNEL = 000D\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "dev/qa</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col4\" class=\"data row100 col4\" >['volume group', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow100_col5\" class=\"data row100 col5\" >['volume group', 'unable', 'volume group vg1sws_70_fp15_fix1', 'old pv extension headers', 'vg vg1sws_70_fp15_fix1', 'volume group', 'vg1sws_70_fp15_fix1 vg1sws_70_fp45', 'physical volume', 'vg1sws_70_fp15_fix1', 'was002 r', 'vmcp q', 'dasd']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row101\" class=\"row_heading level0 row101\" >101</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col0\" class=\"data row101 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col1\" class=\"data row101 col1\" >01860065</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col2\" class=\"data row101 col2\" >RCA from SOSREPORT</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col3\" class=\"data row101 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "The host panicked and rebooted.   We need root cause analysis of the SOSREPORT.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "The host panicked and rebooted.   We need root cause analysis of the SOSREPORT.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "The host panicked and rebooted.   We need root cause analysis of the SOSREPORT.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "The host panicked and rebooted.   We need root cause analysis of the SOSREPORT.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col4\" class=\"data row101 col4\" >['sosreport', 'rca']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow101_col5\" class=\"data row101 col5\" >['sosreport', 'rca', 'root', 'sosreport', 'analysis', 'host']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row102\" class=\"row_heading level0 row102\" >102</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col0\" class=\"data row102 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col1\" class=\"data row102 col1\" >02402109</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col2\" class=\"data row102 col2\" >continous</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col3\" class=\"data row102 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We are continuously getting errors : \n",
       "Jun 11 09:33:31 gftemdapai1lnp kernel: EXT4-fs warning (device dm-40): ext4_dx_add_entry: Directory index full!\n",
       "Jun 11 09:33:31 gftemdapai1lnp kernel: EXT4-fs warning (device dm-40): ext4_dx_add_entry: Directory index full!\n",
       "\n",
       "dmsetup info /dev/dm-40\n",
       "Name:              gftemdapai1lnp_vg-appslv\n",
       "\n",
       "This is related to Ab-Initio application filesystem :\n",
       "\n",
       "/dev/mapper/gftemdapai1lnp_vg-appslv\n",
       "                      3.2T  2.5T  529G  83% /abinitio/apps\n",
       "block size is 4096 bytes and its ext4 filesystem \n",
       "\n",
       "After counting amount of files and directories on this filesystem we have found : \n",
       "25894370 files \n",
       "9791 directories \n",
       "\n",
       "Filesystem              Inodes    IUsed     IFree IUse% Mounted on\n",
       "/dev/mapper/gftemdapai1lnp_vg-appslv\n",
       "                     213180416 25864121 187316295   13% /abinitio/apps\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Its critical production environment that had serious performance issue - we suspect it was because of these errors. After reboot system is back up and running\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Now we can observe these directory index full alerts everyday.\n",
       "At the time of the performance issue multipathing deamon was using around 400/500 % of CPU which was causing huge performance issue\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "This has happened in November 24 2018 also</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col4\" class=\"data row102 col4\" >['continous']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow102_col5\" class=\"data row102 col5\" >['continous', 'initio application filesystem', 'directory index full', 'ext4 filesystem', 'apps block size', 'filesystem', 'device dm-40', 'ext4-fs warning', 'gftemdapai1lnp kernel', 'gftemdapai1lnp_vg', 'ifree iuse%']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row103\" class=\"row_heading level0 row103\" >103</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col0\" class=\"data row103 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col1\" class=\"data row103 col1\" >02035374</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col2\" class=\"data row103 col2\" >We are seeing lot of error message in /var/log/kernel.log as ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col3\" class=\"data row103 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "On our server application creates lot of files in /usr/local filesystem. We suspect above error may be due to this. We have had inode exhaustion once before on same filesystem.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "We are facing this in our production environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "This is the first time we have seen this error.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "We need to understand the impact of error and resolve this issue quickly.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col4\" class=\"data row103 col4\" >['directory index full', 'error message', 'kernel.log', 'log', 'ext4_dx_add_entry', 'lot']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow103_col5\" class=\"data row103 col5\" >['directory index full', 'error message', 'kernel.log', 'log', 'ext4_dx_add_entry', 'lot', 'local filesystem', 'same filesystem', 'server application', 'files', 'lot', 'due', 'exhaustion', 'error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row104\" class=\"row_heading level0 row104\" >104</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col0\" class=\"data row104 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col1\" class=\"data row104 col1\" >01872978</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col2\" class=\"data row104 col2\" >Receiving EXT4-fs warnings in /var/log/messages</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col3\" class=\"data row104 col3\" >We are seeing the following in /var/log/messages...\n",
       "\n",
       "Jun 19 14:36:12 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:12 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:12 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:12 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:12 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:44 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:44 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:44 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:44 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:44 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:53 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:53 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:53 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:53 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:53 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:55 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:55 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:55 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:55 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:36:55 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:05 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:05 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:05 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:05 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:05 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:23 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:23 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:23 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:23 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:23 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:24 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:24 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:24 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:24 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:24 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:28 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:28 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:28 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:28 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:28 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:31 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:31 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:31 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:31 pvsvr0186 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry:2018: Directory index full!\n",
       "Jun 19 14:37:31 pvsvr0186 kernel: EXT4-fs warning (device dm-9)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col4\" class=\"data row104 col4\" >['ext4-fs warnings', 'messages', 'log']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow104_col5\" class=\"data row104 col5\" >['ext4-fs warnings', 'messages', 'log', 'directory index full', 'pvsvr0186 kernel', 'ext4-fs warning', 'jun', 'device', 'ext4_dx_add_entry:2018', 'messages', 'log', 'following']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row105\" class=\"row_heading level0 row105\" >105</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col0\" class=\"data row105 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col1\" class=\"data row105 col1\" >01688050</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col2\" class=\"data row105 col2\" >We get issues regarding some jobs to be run on server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col3\" class=\"data row105 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We get issue to make some jobs to be run on servers.\n",
       "We have 3 servers (AGGPRD0A, AGGPRD0B and AGGPRD0C), on AGGPRD0B it works, but issue is on the 2 others servers.\n",
       "There is a difference when we change the seting of dir_index option.\n",
       "On AGGPRD0A option is  enable and job is falling with error: EXT4-fs warning (device dm-18): ext4_dx_add_entry: Directory index full \n",
       "On AGGPRD0C, option in disable and job is running without ending when the duration is only 5 min on other similar server.\n",
       "\n",
       "How can we solve it?\n",
       "What is the impact if we enable option on AGGPRD0C? A production is running on it.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Each time</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col4\" class=\"data row105 col4\" >['server', 'jobs', 'issues']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow105_col5\" class=\"data row105 col5\" >['server', 'jobs', 'issues', 'aggprd0a option', 'directory index full', 'other similar server', 'option', 'others servers', 'ext4-fs warning', 'device dm-18', 'aggprd0c', 'job', 'servers']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row106\" class=\"row_heading level0 row106\" >106</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col0\" class=\"data row106 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col1\" class=\"data row106 col1\" >01945036</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col2\" class=\"data row106 col2\" >kernel: EXT4-fs warning (device dm-1): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col3\" class=\"data row106 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "kernel: EXT4-fs warning (device dm-1): ext4_dx_add_entry: Directory index full!\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "production environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Frequently\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Its a large business impact. More than 5k people are affected.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col4\" class=\"data row106 col4\" >['directory index full', 'device dm-1', 'ext4-fs warning', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow106_col5\" class=\"data row106 col5\" >['directory index full', 'device dm-1', 'ext4-fs warning', 'kernel', 'directory index full', 'device dm-1', 'ext4-fs warning', 'kernel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row107\" class=\"row_heading level0 row107\" >107</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col0\" class=\"data row107 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col1\" class=\"data row107 col1\" >01703146</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col2\" class=\"data row107 col2\" >there is large amount of open files on the server.we are also getting some miscellaneous errors in /var/log/messages related to directory index. Need the support on this issue as these are affecting system performance.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col3\" class=\"data row107 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "high count of open files which is affecting system performance . Also getting errors related to directory index. Below are the logs and necessary data-\n",
       "\n",
       "[root@maupmlx010v log]# ps -ef | wc -l\n",
       "3019\n",
       "[root@maupmlx010v log]#\n",
       "\n",
       "[root@maupmlx010v log]# lsof | wc -l\n",
       "69577\n",
       "\n",
       "------------------------------------------------------\n",
       "\n",
       "[root@maupmlx010v log]# ps -ef | grep -i \"/opt2/Telstra/CSReports/BHCA/mm_au.exp\" | more\n",
       "root       436     1  0 May19 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root       896     1  0 May24 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root       910     1  0 Aug30 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root       956     1  0 Aug13 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      1048     1  0 Sep03 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      1158     1  0 Jul11 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      1341     1  0 Aug01 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      1368     1  0 May13 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      1386     1  0 Jul26 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      1651     1  0 Jul07 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      1872     1  0 Jul16 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2075     1  0 Jun05 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2178     1  0 May17 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2195     1  0 Aug27 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2298     1  0 May29 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2409     1  0 Jul05 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2500     1  0 May28 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2530     1  0 Aug11 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2616     1  0 Aug03 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2662     1  0 Jul16 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2764     1  0 May26 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2855     1  0 Sep02 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2863     1  0 Jul17 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2869     1  0 Aug03 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      2902     1  0 Jul21 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3110     1  0 Aug12 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3147     1  0 Jul12 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3290     1  0 Aug07 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3361     1  0 Jul04 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3531     1  0 Jun06 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3564     1  0 Jul23 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3725     1  0 Jul06 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3749     1  0 Jun13 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      3902     1  0 Aug19 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      4175     1  0 Aug05 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      4177     1  0 Jul15 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      4201     1  0 Aug15 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      4223     1  0 May22 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      4329     1  0 Jun09 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_au.exp /dev/null\n",
       "root      4374     1  0 Aug16 ?        00:00:00 /usr/bin/expect /opt2/Telstra/CSReports/BHCA/mm_</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col4\" class=\"data row107 col4\" >['miscellaneous errors', 'directory index', 'open files', 'large amount', 'system performance', 'support', 'server.we', 'messages', 'issue', 'log']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow107_col5\" class=\"data row107 col5\" >['miscellaneous errors', 'directory index', 'open files', 'large amount', 'system performance', 'support', 'server.we', 'messages', 'issue', 'log', 'null root', 'more root', 'csreports', 'bhca', 'bin', 'telstra', 'mm_au.exp', 'root@maupmlx010v log', '| wc -l', 'system performance']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row108\" class=\"row_heading level0 row108\" >108</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col0\" class=\"data row108 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col1\" class=\"data row108 col1\" >01769617</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col2\" class=\"data row108 col2\" >email-node1 kernel: EXT3-fs warning (device dm-5): ext3_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col3\" class=\"data row108 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "The system has become read-only. Unable to write to anything:\n",
       "\n",
       "[root@email-node1 etc]# mount \n",
       "/dev/mapper/vg0-root on / type ext3 (rw)\n",
       "proc on /proc type proc (rw)\n",
       "sysfs on /sys type sysfs (rw)\n",
       "devpts on /dev/pts type devpts (rw,gid=5,mode=620)\n",
       "/dev/mapper/vg0-tmp on /tmp type ext3 (rw)\n",
       "/dev/mapper/vg0-prod on /prod type ext3 (rw)\n",
       "/dev/mapper/vg0-var on /var type ext3 (rw)\n",
       "/dev/mapper/vg0-usr on /usr type ext3 (rw)\n",
       "/dev/mapper/vg0-u01 on /u01 type ext3 (rw)\n",
       "/dev/cciss/c0d0p1 on /boot type ext3 (rw)\n",
       "tmpfs on /dev/shm type tmpfs (rw,size=80g)\n",
       "none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n",
       "sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)\n",
       "oracleasmfs on /dev/oracleasm type oracleasmfs (rw)\n",
       "\n",
       "mount: warning: /etc/mtab is not writable (e.g. read-only filesystem).\n",
       "       It's possible that information reported by mount(8) is not\n",
       "       up to date. For actual information about system mount points\n",
       "       check the /proc/mounts file.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production. Email Node / Primary\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "It is happening now. Want to reboot the server, but the some blogs have stated that data could be lost.\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "URGENT.\n",
       "\n",
       "I wanted to run an SOS report, but the system generated the following error when I try to run it:\n",
       "\n",
       "\n",
       "[root@email-node1 etc]# /usr/sbin/sosreport \n",
       "\n",
       "sosreport (version 1.7)\n",
       "\n",
       "This utility will collect some detailed  information about the\n",
       "hardware and  setup of your  Red Hat Enterprise Linux  system.\n",
       "The information is collected and an archive is  packaged under\n",
       "/tmp, which you can send to a support representative.\n",
       "Red Hat will use this information for diagnostic purposes ONLY\n",
       "and it will be considered confidential information.\n",
       "\n",
       "This process may take a while to complete.\n",
       "No changes will be made to your system.\n",
       "\n",
       "Press ENTER to continue, or CTRL-C to quit.\n",
       "\n",
       "I/O warning : failed to load external entity \"/etc/cluster/cluster.conf\"\n",
       "One or more plugins have detected a problem in your configuration.\n",
       "Please review the following messages:\n",
       "\n",
       "gfs2:\n",
       "    * /etc/cluster/cluster.conf contains malformed XML\n",
       "\n",
       "Are you sure you would like to continue (y/n) ? n</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col4\" class=\"data row108 col4\" >['device dm-5', 'directory index full', 'ext3-fs warning', 'node1 kernel', 'ext3_dx_add_entry', 'email']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow108_col5\" class=\"data row108 col5\" >['device dm-5', 'directory index full', 'ext3-fs warning', 'node1 kernel', 'ext3_dx_add_entry', 'email', 'type ext3', 'binfmt_misc type binfmt_misc', 'shm type tmpfs', 'oracleasm type oracleasmfs', 'pts type devpts', 'type proc', 'type sysfs', 'type', 'rw', 'system mount']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row109\" class=\"row_heading level0 row109\" >109</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col0\" class=\"data row109 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col1\" class=\"data row109 col1\" >02294449</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col2\" class=\"data row109 col2\" >No space left on a device error</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col3\" class=\"data row109 col3\" >Hi there,\n",
       "\n",
       "We have an issue where users get a \"no space left on a device\" error when the application is trying to create files on the filesystem eventhough there's enough space available. Please see the error they get as shown below:\n",
       "\n",
       "00:30:08,884 INFO  [stdout] (AsyncAppender-Async) 00:30:08.884 [DefaultQuartzScheduler-camel-1_Worker-3] Email_E102_ProcessStatementsError za.co.absa.estatements.utils.EmailDispatchUtilities - java.nio.file.FileSystemException: /apps/statements/xml/split/ *** Sensitive Data Detected And Removed: Visa ***_FROM_SOURCE_CADOCSF_A2_JAN12.xml -> /apps/statements/email_dispatch/outbound/ *** Sensitive Data Detected And Removed: Visa ***_FROM_SOURCE_CADOCSF_A2_JAN12.xml: No space left on device\n",
       "\n",
       "Also please see error message from \"dmesg\" command\n",
       "=====================================\n",
       "\n",
       "[8542874.447702] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542883.450841] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542897.046007] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542906.333562] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542914.088604] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542922.455599] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542922.932721] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542939.302022] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542940.521744] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542957.770686] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542966.548103] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8542966.846521] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543016.139915] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543025.931202] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543033.703304] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543034.325178] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543064.012787] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543077.513722] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543087.002630] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543095.147083] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543103.133489] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543113.768144] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "[8543121.843995] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "root@jgopsr000000161 PROD #</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col4\" class=\"data row109 col4\" >['device error', 'space']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow109_col5\" class=\"data row109 col5\" >['device error', 'space', 'directory index full', 'device dm-5', 'ext4-fs warning', 'enough space available', 'device', 'sensitive data', 'error message', 'ext4_dx_add_entry:2016', 'email_e102_processstatementserror za.co.absa.estatements.utils.emaildispatchutilities', '_ from_source_cadocsf_a2_jan12.xml']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row110\" class=\"row_heading level0 row110\" >110</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col0\" class=\"data row110 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col1\" class=\"data row110 col1\" >01788530</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col2\" class=\"data row110 col2\" >m222218dbss3001 Server Hung--need Root Cause Analysis</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col3\" class=\"data row110 col3\" >m222218dbss3001 hung and needed to be power cycled to regain access.  I've attached sosreport files</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col4\" class=\"data row110 col4\" >['m222218dbss3001 server hung', 'analysis', 'root']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow110_col5\" class=\"data row110 col5\" >['m222218dbss3001 server hung', 'analysis', 'root', 'm222218dbss3001 hung', 'sosreport files', 'access', 'power']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row111\" class=\"row_heading level0 row111\" >111</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col0\" class=\"data row111 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col1\" class=\"data row111 col1\" >02073758</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col2\" class=\"data row111 col2\" >kernel: EXT4-fs warning (device dm-20): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col3\" class=\"data row111 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We are seeing the error message \"kernel: EXT4-fs warning (device dm-20): ext4_dx_add_entry: Directory index full!\"  in /var/log/message file and dmesg command.  When we checked the files are not many and we have free Inode and disk space available. Could you please check this and update.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "kernel: EXT4-fs warning (device dm-20): ext4_dx_add_entry: Directory index full!\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "This is a production server and currently we are seeing the same error messages on four of our production servers. Please check and let us know how can we resolve this issue.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col4\" class=\"data row111 col4\" >['directory index full', 'ext4-fs warning', 'device', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow111_col5\" class=\"data row111 col5\" >['directory index full', 'ext4-fs warning', 'device', 'kernel', 'disk space available', 'directory index full', 'error message', 'free inode', 'ext4-fs warning', 'message file', 'device', 'many', 'log', 'kernel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row112\" class=\"row_heading level0 row112\" >112</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col0\" class=\"data row112 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col1\" class=\"data row112 col1\" >02456466</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col2\" class=\"data row112 col2\" >EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col3\" class=\"data row112 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!\n",
       "EXT4-fs warning (device dm-326): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col4\" class=\"data row112 col4\" >['directory index full', 'device dm-326', 'ext4-fs warning']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow112_col5\" class=\"data row112 col5\" >['directory index full', 'device dm-326', 'ext4-fs warning', 'directory index full', 'device dm-326', 'ext4-fs warning']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row113\" class=\"row_heading level0 row113\" >113</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col0\" class=\"data row113 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col1\" class=\"data row113 col1\" >01792679</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col2\" class=\"data row113 col2\" >not able to create or delete files under a directory</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col3\" class=\"data row113 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "not able to create or delete files under a directory but we can create under its subdirectory\n",
       "\n",
       "[root@ctlinmlsrv6 logs]# touch abc\n",
       "touch: cannot touch `abc': Permission denied\n",
       "[root@ctlinmlsrv6 logs]# touch abc\n",
       "touch: cannot touch `abc': Permission denied\n",
       "[root@ctlinmlsrv6 logs]# touch abc\n",
       "touch: cannot touch `abc': Permission denied\n",
       "[root@ctlinmlsrv6 logs]# mkdir abc\n",
       "mkdir: cannot create directory `abc': Permission denied\n",
       "[root@ctlinmlsrv6 logs]# pwd\n",
       "/data/chim/logs\n",
       "[root@ctlinmlsrv6 logs]# ls -ld /data/chim/logs\n",
       "drwxr-xr-x. 3 root root 12288 Nov 12 14:00 /data/chim/logs\n",
       "[root@ctlinmlsrv6 logs]# cd ..\n",
       "[root@ctlinmlsrv6 chim]# ls -ltr\n",
       "total 28\n",
       "drwxr-xr-x. 3 root root 12288 Nov 12 14:00 logs\n",
       "drwxr-xr-x. 3 root root  4096 Feb  7 06:10 challenge_agent\n",
       "drwxr-xr-x. 8 root root  4096 Feb 14 23:18 properties\n",
       "drwxr-x---. 3 root root  4096 Feb 14 23:39 utd\n",
       "drwxr-x---. 4 root root  4096 Feb 14 23:39 ccd\n",
       "[root@ctlinmlsrv6 chim]# cat /etc/redhat-release\n",
       "Red Hat Enterprise Linux Server release 6.8 (Santiago)\n",
       "[root@ctlinmlsrv6 chim]# cd logs\n",
       "[root@ctlinmlsrv6 logs]# ls\n",
       "1.log                                        chim-DocInterchange-ut-trace-2016-08-02.log\n",
       "chim-DocInterchange-ut                       chim-DocInterchange-ut-trace-2016-08-22.log\n",
       "chim-DocInterchange-ut-error-2016-06-13.log  chim-DocInterchange-ut-trace-2016-08-23.log\n",
       "chim-DocInterchange-ut-error-2016-06-18.log  chim-DocInterchange-ut-trace-2016-08-24.log\n",
       "chim-DocInterchange-ut-error-2016-06-20.log  chim-DocInterchange-ut-trace-2016-08-25.log\n",
       "chim-DocInterchange-ut-error-2016-06-21.log  chim-DocInterchange-ut-trace-2016-08-26.log\n",
       "chim-DocInterchange-ut-error-2016-06-22.log  chim-DocInterchange-ut-trace-2016-08-27.log\n",
       "chim-DocInterchange-ut-error-2016-06-23.log  chim-DocInterchange-ut-trace-2016-08-28.log\n",
       "chim-DocInterchange-ut-error-2016-06-29.log  chim-DocInterchange-ut-trace-2016-08-29.log\n",
       "chim-DocInterchange-ut-error-2016-06-30.log  chim-DocInterchange-ut-trace-2016-08-30.log\n",
       "chim-DocInterchange-ut-error-2016-07-05.log  chim-DocInterchange-ut-trace-2016-08-31.log\n",
       "chim-DocInterchange-ut-error-2016-07-07.log  chim-DocInterchange-ut-trace-2016-09-01.log\n",
       "chim-DocInterchange-ut-error-2016-07-08.log  chim-DocInterchange-ut-trace-2016-09-02.log\n",
       "chim-DocInterchange-ut-error-2016-07-09.log  chim-DocInterchange-ut-trace-2016-09-03.log\n",
       "chim-DocInterchange-ut-error-2016-07-10.log  chim-DocInterchange-ut-trace-2016-09-04.log\n",
       "chim-DocInterchange-ut-error-2016-07-11.log  chim-DocInterchange-ut-trace-2016-09-05.log\n",
       "chim-DocInterchange-ut-error-2016-07-12.log  chim-DocInterchange-ut-trace-2016-09-06.log\n",
       "chim-DocInterchange-ut-error-2016-07-13.log  chim-DocInterchange-ut-trace-2016-09-07.log\n",
       "chim-DocInterchange-ut-error-2016-07-19.log  chim-DocInterchange-ut-trace-2016-09-08.log\n",
       "chim-DocInterchange-ut-error-2016-07-26.log  chim-DocInterchange-ut-trace-2016-09-09.log\n",
       "chim-DocInterchange-ut-error-2016-08-01.log  chim-DocInterchange-ut-trace-2016-09-10.log\n",
       "chim-DocInterchange-ut-error-2016-08-02.log  chim-DocInterchange-ut-trace-2016-09-11.log\n",
       "chim-DocInterchange-ut-error-2016-08-21.log  chim-DocInterchange-ut-trace-2016-09-13.log\n",
       "chim-DocInterchange-ut-error-2016-09-05.log  chim-DocInterchange-ut-trace-2016-09-14.log\n",
       "chim-DocInterchange-ut-error-2016-09-13.log  chim-DocInterchange-ut-trace-2016-09-15.log\n",
       "chim-DocInterchange-ut-error-2016-11-02.log  chim-DocInterchange-ut-trace-2016-09-16.log\n",
       "chim-DocInterchange-ut-error-2016-11-10.log  chim-DocInterchange-ut-trace-2016-09-17.log\n",
       "chim-DocInterchange-ut-error.log             chim-DocInterchange-ut-trace-2016-09-18.log\n",
       "chim-DocInterchange-ut.log                   chim-DocInterchange-ut-trace-2016-09-19.log\n",
       "chim-DocInterchange-ut-trace-2016-06-18.log  chim-DocInterchange-ut-trace-2016-09-20.log\n",
       "chim-DocInterchange-ut-trace-2016-06-20.log  chim-DocInterchange-ut-trace-2016-09-21.log\n",
       "chim-DocInterchange-ut-trace-2016-06-22.log  chim-DocInterchange-ut-trace-2016-09-22.log\n",
       "chim-DocInterchange-ut-trace-2016-06-23.log  chim-DocInterchange-ut-trace-2016-09-23.log\n",
       "chim-DocInterchange-ut-trace-2016-06-29.log  chim-DocInterchange-ut-trace-2016-09-24.log\n",
       "chim-DocInterchange-ut-trace-2016-07-01.log  chim-DocInterchange-ut-trace-2016-09-25.log\n",
       "chim-DocInterchange-ut-trace-2016-07-04.log  chim-DocInterchange-ut-trace-2016-09-26.log\n",
       "chim-DocInterchange-ut-trace-2016-07-07.log  chim-DocInterchange-ut-trace-2016-09-27.log\n",
       "chim-DocInterchange-ut-trace-2016-07-08.log  chim-DocInterchange-ut-trace-2016-09-28.log\n",
       "chim-DocInterchange-ut-trace-2016-07-09.log  chim-DocInterchange-ut-trace-2016-09-29.log\n",
       "chim-DocInterchange-ut-trace-2016-07-10.log  chim-DocInterchange-ut-trace-2016-09-30.log\n",
       "chim-DocInterchange-ut-trace-2016-07-11.log  chim-DocInterchange-ut-trace-2016-10-01.log\n",
       "chim-DocInterchange-ut-trace-2016-07-12.log  chim-DocInterchange-ut-trace-2016-10-02.log\n",
       "chim-DocInterchange-ut-trace-2016-07-13.log  chim-DocInterchange-ut-trace-2016-10-04.log\n",
       "chim-DocIntercha</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col4\" class=\"data row113 col4\" >['directory', 'files', 'able']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow113_col5\" class=\"data row113 col5\" >['directory', 'files', 'able', 'root@ctlinmlsrv6 chim', '13.log chim', '01.log chim', '23.log chim', '22.log chim', '08.log chim', '07.log chim', '04.log chim', '02.log chim', '26.log chim']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row114\" class=\"row_heading level0 row114\" >114</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col0\" class=\"data row114 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col1\" class=\"data row114 col1\" >01959878</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col2\" class=\"data row114 col2\" >FileNotFoundException - no space left on device</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col3\" class=\"data row114 col3\" >We are getting the error  java.io.FileNotFoundException: /OPUS_data/symp/dataprocessing/PRODCLDATAPROCESSWS/10749150 - ORBS request payload - 1508800449642.xml (No space left on device)\n",
       "\n",
       "/OPUS_data is a mount on vm-wapp-prod04 and is shared to vm-wapp-prod05. It has 194G free (so not out of space) and 30434456 INodes free. It only happens occasionally and not all the time, pretty confused actually. Is there a limit to how many files can be in one folder?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col4\" class=\"data row114 col4\" >['device', 'space']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow114_col5\" class=\"data row114 col5\" >['device', 'space', 'request payload', 'many files', 'space', 'limit', 'g free', 'wapp', 'vm', 'dataprocessing', 'orbs', 'prodcldataprocessws/10749150']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row115\" class=\"row_heading level0 row115\" >115</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col0\" class=\"data row115 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col1\" class=\"data row115 col1\" >02478554</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col2\" class=\"data row115 col2\" >we are seeing XT4-fs warning (device dm-1): ext4_dx_add_entry: Directory index full! in /var/log/messages</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col3\" class=\"data row115 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "we are getting lots of the below message and it keeps on coming\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "it is prod, rhel 6.7</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col4\" class=\"data row115 col4\" >['directory index full', 'device dm-1', 'xt4-fs warning', 'messages', 'log']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow115_col5\" class=\"data row115 col5\" >['directory index full', 'device dm-1', 'xt4-fs warning', 'messages', 'log', 'below message', 'lots']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row116\" class=\"row_heading level0 row116\" >116</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col0\" class=\"data row116 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col1\" class=\"data row116 col1\" >02003664</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col2\" class=\"data row116 col2\" >ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col3\" class=\"data row116 col3\" >kernel: EXT4-fs warning (device dm-1): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col4\" class=\"data row116 col4\" >['directory index full']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow116_col5\" class=\"data row116 col5\" >['directory index full', 'directory index full', 'device dm-1', 'ext4-fs warning', 'kernel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row117\" class=\"row_heading level0 row117\" >117</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col0\" class=\"data row117 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col1\" class=\"data row117 col1\" >02124863</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col2\" class=\"data row117 col2\" >Constant \"kernel: EXT4-fs warning (device dm-22): ext4_dx_add_entry: Directory index full!\" messages found in our server logs.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col3\" class=\"data row117 col3\" >We are getting a lot of \"kernel: EXT4-fs warning (device dm-22): ext4_dx_add_entry: Directory index full!\" messages in our server logs. We would like to see what could be causing this and how to resolve it.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col4\" class=\"data row117 col4\" >['directory index full', 'ext4-fs warning', 'server logs', 'device', 'messages', 'kernel', 'constant']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow117_col5\" class=\"data row117 col5\" >['directory index full', 'ext4-fs warning', 'server logs', 'device', 'messages', 'kernel', 'constant', 'directory index full', 'ext4-fs warning', 'server logs', 'device', 'messages', 'kernel', 'lot']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row118\" class=\"row_heading level0 row118\" >118</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col0\" class=\"data row118 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col1\" class=\"data row118 col1\" >01984828</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col2\" class=\"data row118 col2\" >receiveing Nov 29 17:28:23 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full! error daily on server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col3\" class=\"data row118 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "receiveing Nov 29 17:28:23 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full! error daily on server, the disk is not in use however.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "receiveing Nov 29 17:28:23 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full! error daily on server\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Nov 29 17:28:23 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 29 19:22:31 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 29 21:16:12 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 29 21:44:04 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 29 22:09:17 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 29 23:00:09 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 29 23:43:57 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 30 00:46:33 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 30 18:03:16 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 30 20:58:52 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Nov 30 22:16:07 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Dec  1 01:19:58 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!\n",
       "Dec  1 01:44:08 lrdna1gx kernel: EXT3-fs warning (device dm-84): ext3_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col4\" class=\"data row118 col4\" >['directory index full', 'device dm-84', 'ext3-fs warning', 'lrdna1gx kernel', 'ext3_dx_add_entry', 'error', 'server', 'nov']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow118_col5\" class=\"data row118 col5\" >['directory index full', 'device dm-84', 'ext3-fs warning', 'lrdna1gx kernel', 'ext3_dx_add_entry', 'error', 'server', 'nov', 'directory index full', 'device dm-84', 'ext3-fs warning', 'lrdna1gx kernel', 'ext3_dx_add_entry', 'server', 'error', 'disk', 'nov', 'use']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row119\" class=\"row_heading level0 row119\" >119</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col0\" class=\"data row119 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col1\" class=\"data row119 col1\" >01984076</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col2\" class=\"data row119 col2\" >Crashed & came up automatically</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col3\" class=\"data row119 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Cause of auto crash\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "PROD\n",
       "\n",
       "[root@dal-dbblx007-08 crash]# uptime\n",
       " 12:47:02 up  1:41,  3 users,  load average: 2.62, 2.87, 2.81\n",
       "[root@dal-dbblx007-08 crash]# date\n",
       "Thu Nov 30 12:47:06 UTC 2017\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Today\n",
       "\n",
       "reboot   system boot  2.6.32-642.4.2.e Thu Nov 30 11:07 - 12:46  (01:39)\n",
       "reboot   system boot  2.6.32-642.4.2.e Sat Jun 17 04:31 - 12:46 (166+08:15)\n",
       "reboot   system boot  2.6.32-642.4.2.e Tue Apr  4 04:13 - 12:46 (240+08:33)\n",
       "reboot   system boot  2.6.32-431.29.2. Thu Oct 20 06:56 - 12:46 (406+05:50)\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "- sosreport  (/tmp/sosreport-admin-20171130123449-dal-dbblx007-08.tar.xz)\n",
       "- vmcore  crash file (/tmp/vmcore-dal-dbblx007-08.gz )\n",
       "- vmcore crash dmesg file (/tmp/vmcore-dmesg-dal-dbblx007-08.txt</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col4\" class=\"data row119 col4\" >['crashed']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow119_col5\" class=\"data row119 col5\" >['crashed', 'auto crash', 'cause']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row120\" class=\"row_heading level0 row120\" >120</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col0\" class=\"data row120 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col1\" class=\"data row120 col1\" >02051766</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col2\" class=\"data row120 col2\" >we have observed one of mount point provide frequent kernel warning message for directory indexi full. Need solution to this issue.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col3\" class=\"data row120 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "\"kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry: Directory index full \" this message coming on kernel log. \n",
       "please cross check the overall system and provide feedback, is this kernel issue can impact out production ?  also provide the way of fix this issue.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "/dev/mapper/nfs--icms-lv--nfs--icms lVM which is device dm-7\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "frequently\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "whatever information needed please let us know. initially i will update sosreport</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col4\" class=\"data row120 col4\" >['frequent kernel warning message', 'mount point', 'directory', 'full', 'solution', 'issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow120_col5\" class=\"data row120 col5\" >['frequent kernel warning message', 'mount point', 'directory', 'full', 'solution', 'issue', 'kernel issue', 'directory index full', 'kernel log', 'kernel', 'device dm-7', 'overall system', 'ext4-fs warning', 'issue', 'production', 'feedback']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row121\" class=\"row_heading level0 row121\" >121</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col0\" class=\"data row121 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col1\" class=\"data row121 col1\" >02087164</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col2\" class=\"data row121 col2\" >NFS issue</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col3\" class=\"data row121 col3\" >Dear Concern:\n",
       "Facing NFS issue in server end. Please find the attachment log below: \n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: nfsd          D 0000000000000000     0  5255      2 0x00000080\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: ffff88181307bb90 0000000000000046 ffff880c2b110b00 0000000000000000\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: ffff88181307bb30 ffffffffa0683836 ffff88181307bb30 ffffffff811ad870\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: ffff880c00f9f3c0 ffff881813260000 ffff881829c6dad8 ffff88181307bfd8\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: Call Trace:\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa0683836>] ? nfsd_setuser+0x126/0x2c0 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff811ad870>] ? d_obtain_alias+0xc0/0x230\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa01dd7bd>] ? exportfs_decode_fh+0xfd/0x2bc [exportfs]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff8153a2b6>] __mutex_lock_slowpath+0x96/0x210\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff81539ddb>] mutex_lock+0x2b/0x50\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff81191e14>] generic_file_llseek_size+0x44/0xd0\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa0694e00>] ? nfsd4_encode_dirent+0x0/0x2d0 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa0139a07>] ext4_dir_llseek+0x67/0xa0 [ext4]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff8119016a>] vfs_llseek+0x3a/0x40\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa067ee9e>] nfsd_readdir+0x6e/0x240 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa06934c4>] nfsd4_encode_readdir+0x144/0x250 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa068d2f5>] nfsd4_encode_operation+0x75/0x180 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa068b0d5>] nfsd4_proc_compound+0x195/0x490 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa0678405>] nfsd_dispatch+0xe5/0x230 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa056fc84>] svc_process_common+0x344/0x640 [sunrpc]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff810672b0>] ? default_wake_function+0x0/0x20\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa05702c0>] svc_process+0x110/0x160 [sunrpc]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa0678b32>] nfsd+0xc2/0x160 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffffa0678a70>] ? nfsd+0x0/0x160 [nfsd]\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff810a0fce>] kthread+0x9e/0xc0\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff8100c28a>] child_rip+0xa/0x20\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff810a0f30>] ? kthread+0x0/0xc0\n",
       "Apr 25 03:53:49 ICMS-APP-01 kernel: [<ffffffff8100c280>] ? child_rip+0x0/0x20\n",
       "Apr 25 03:54:06 ICMS-APP-01 kernel: nfs: server ICMS-NFS-Service not responding, still trying\n",
       "Apr 25 03:54:43 ICMS-APP-01 kernel: nfs: server ICMS-NFS-Service OK\n",
       "Apr 25 07:45:22 ICMS-APP-01 kernel: bond1: link status definitely down for interface eth4, disabling it\n",
       "Apr 25 07:45:22 ICMS-APP-01 kernel: ixgbe 0000:04:00.0: eth4: detected SFP+: 6\n",
       "Apr 25 07:45:25 ICMS-APP-01 kernel: ixgbe 0000:04:00.0: eth4: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n",
       "Apr 25 07:45:25 ICMS-APP-01 kernel: bond1: link status definitely up for interface eth4, 10000 Mbps full duplex\n",
       "Apr 25 07:45:28 ICMS-APP-01 kernel: bond1: link status definitely down for interface eth5, disabling it\n",
       "Apr 25 07:45:28 ICMS-APP-01 kernel: bond1: making interface eth4 the new active one\n",
       "Apr 25 07:45:28 ICMS-APP-01 kernel: ixgbe 0000:04:00.1: eth5: detected SFP+: 5\n",
       "Apr 25 07:45:29 ICMS-APP-01 kernel: ixgbe 0000:04:00.1: eth5: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n",
       "Apr 25 07:45:29 ICMS-APP-01 kernel: bond1: link status definitely up for interface eth5, 10000 Mbps full duplex\n",
       "Apr 25 10:22:29 ICMS-APP-01 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry: Directory index full!\n",
       "Apr 25 10:22:29 ICMS-APP-01 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry: Directory index full!\n",
       "Apr 25 10:22:29 ICMS-APP-01 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry: Directory index full!\n",
       "Apr 25 10:22:29 ICMS-APP-01 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry: Directory index full!\n",
       "Apr 25 10:22:29 ICMS-APP-01 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry: Directory index full!\n",
       "Thanks\n",
       "Mostafiz</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col4\" class=\"data row121 col4\" >['nfs issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow121_col5\" class=\"data row121 col5\" >['nfs issue', 'mbps full duplex apr', 'service ok apr', 'kernel', '_ mutex_lock_slowpath+0x96/0x210 apr', 'tx apr', 'icms', 'kthread+0x9e/0xc0 apr', 'mutex_lock+0x2b/0x50 apr', 'd_obtain_alias+0xc0/0x230 apr', 'apr']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row122\" class=\"row_heading level0 row122\" >122</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col0\" class=\"data row122 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col1\" class=\"data row122 col1\" >01717826</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col2\" class=\"data row122 col2\" >Server Got Rebooted - lrau1p17</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col3\" class=\"data row122 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Server (lrau1p17) Got rebooted around - 8th Oct 2016 - 17 hrs, We need to find/identify the reason of reboot on high priority\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "PRODUCTION\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "Server (lrau1p17) Got rebooted around - 8th Oct 2016 - 17 hrs.\n",
       "\n",
       "8 19:01:21 lrau1p17 kernel: qla2xxx [0000:06:00.0]-801c:0: Abort command issued nexus=0:0:126 --  1 2002.\n",
       "Oct  8 19:01:32 lrau1p17 kernel: device-mapper: multipath: Failing path 68:0.\n",
       "Oct  8 19:01:38 lrau1p17 multipathd: mpathne: sdbm - directio checker reports path is down\n",
       "Oct  8 19:01:44 lrau1p17 multipathd: mpathne: sdbm - directio checker reports path is down\n",
       "Oct  8 19:01:50 lrau1p17 multipathd: mpathne: sdbm - directio checker reports path is down\n",
       "Oct  8 19:01:55 lrau1p17 multipathd: mpathbn: sdcp - directio checker reports path is down\n",
       "Oct  8 19:01:55 lrau1p17 multipathd: checker failed path 69:208 in map mpathbn\n",
       "Oct  8 19:02:03 lrau1p17 multipathd: DM message failed [fail_path 69:208#012]\n",
       "\n",
       "Attached SOSreport for reference.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col4\" class=\"data row122 col4\" >['server got', 'lrau1p17']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow122_col5\" class=\"data row122 col5\" >['server got', 'lrau1p17', 'high priority', 'oct', '8th', 'got', 'reason', 'hrs', 'lrau1p17', 'server', 'reboot']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row123\" class=\"row_heading level0 row123\" >123</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col0\" class=\"data row123 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col1\" class=\"data row123 col1\" >01744009</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col2\" class=\"data row123 col2\" >mv: cannot create regular file `/var/www/html/pub/audit/archive/appluvirtual06b_audit-15112016-03:23:34.html.gz': No space left on device</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col3\" class=\"data row123 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "mv: cannot create regular file `/var/www/html/pub/audit/archive/appluvirtual06b_audit-15112016-03:23:34.html.gz': No space left on device\n",
       "\n",
       "The file couldnt be moved in spite of having enough space.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "RHEL 5.11 Satellite server.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col4\" class=\"data row123 col4\" >['regular file', 'pub', 'archive', 'html', 'www', '03:23:34.html.gz', 'audit', 'space', 'device', 'mv']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow123_col5\" class=\"data row123 col5\" >['regular file', 'pub', 'archive', 'html', 'www', '03:23:34.html.gz', 'audit', 'space', 'device', 'mv', 'regular file', 'enough space', 'file', 'pub', 'space', 'archive', 'html', 'www', '03:23:34.html.gz', 'audit']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row124\" class=\"row_heading level0 row124\" >124</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col0\" class=\"data row124 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col1\" class=\"data row124 col1\" >01997345</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col2\" class=\"data row124 col2\" >kernel: EXT4-fs warning (device dm-4): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col3\" class=\"data row124 col3\" >We are getting continues an errors in logs, kernel: EXT4-fs warning (device dm-4): ext4_dx_add_entry: Directory index full! we have already gone through solution doc https://access.redhat.com/solutions/29894\n",
       "Wants to resolve without reformat the file system.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col4\" class=\"data row124 col4\" >['directory index full', 'device dm-4', 'ext4-fs warning', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow124_col5\" class=\"data row124 col5\" >['directory index full', 'device dm-4', 'ext4-fs warning', 'kernel', 'solution doc https://access.redhat.com/solutions/29894', 'directory index full', 'device dm-4', 'ext4-fs warning', 'file system', 'kernel', 'logs', 'reformat', 'errors']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row125\" class=\"row_heading level0 row125\" >125</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col0\" class=\"data row125 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col1\" class=\"data row125 col1\" >01705654</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col2\" class=\"data row125 col2\" >OS panic reboot</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col3\" class=\"data row125 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "machine suddenly rebooted generating a vmcore\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "first time in months.\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "this is a machine the supports our network operations. it is critical to have a RCA and a solution for this panic reboot as soon as possible.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col4\" class=\"data row125 col4\" >['os panic reboot']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow125_col5\" class=\"data row125 col5\" >['os panic reboot', 'vmcore', 'machine']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row126\" class=\"row_heading level0 row126\" >126</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col0\" class=\"data row126 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col1\" class=\"data row126 col1\" >01852969</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col2\" class=\"data row126 col2\" >No space left on device - dayrhedsgp001 - /dsd_0</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col3\" class=\"data row126 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hi Team,\n",
       "\n",
       "We are facing issue of No space left on device , even when space is available. Inode space is also available. \n",
       "\n",
       "We could see the below error message in dmesg and /var/log/messages:\n",
       "\n",
       "\n",
       "EXT3-fs warning (device dm-71): ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "Production jobs are getting failed due to this issue.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col4\" class=\"data row126 col4\" >['dayrhedsgp001', 'device', 'space']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow126_col5\" class=\"data row126 col5\" >['dayrhedsgp001', 'device', 'space', 'directory index full', 'device dm-71', 'below error message', 'inode space', 'ext3-fs warning', 'device', 'production jobs', 'space', 'available', 'log']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row127\" class=\"row_heading level0 row127\" >127</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col0\" class=\"data row127 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col1\" class=\"data row127 col1\" >01757039</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col2\" class=\"data row127 col2\" >Performance issue - Hardware error in dmesg</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col3\" class=\"data row127 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[root@pruswipprodb1 /]#\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "n/a\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "need to analysis , couldn't see any hardware failure alerts in HP ILO.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col4\" class=\"data row127 col4\" >['hardware error', 'performance issue', 'dmesg']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow127_col5\" class=\"data row127 col5\" >['hardware error', 'performance issue', 'dmesg', 'machine check events', 'hardware error', 'root@pruswipprodb1']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row128\" class=\"row_heading level0 row128\" >128</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col0\" class=\"data row128 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col1\" class=\"data row128 col1\" >02364858</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col2\" class=\"data row128 col2\" >dvice error</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col3\" class=\"data row128 col3\" >I started seeing this error on our server.  What does it mean?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col4\" class=\"data row128 col4\" >['dvice error']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow128_col5\" class=\"data row128 col5\" >['dvice error', 'server', 'error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row129\" class=\"row_heading level0 row129\" >129</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col0\" class=\"data row129 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col1\" class=\"data row129 col1\" >02076874</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col2\" class=\"data row129 col2\" >EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col3\" class=\"data row129 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:15:54 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "Apr 12 18:16:08 psrgisdb02 kernel: EXT4-fs warning (device dm-2): ext4_dx_add_entry:2021: Directory index full!\n",
       "\n",
       "\n",
       "Environment is PROD\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Frequently the alert is getting generated\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "No business impact now</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col4\" class=\"data row129 col4\" >['directory index full', 'device dm-2', 'ext4-fs warning', 'ext4_dx_add_entry:2021']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow129_col5\" class=\"data row129 col5\" >['directory index full', 'device dm-2', 'ext4-fs warning', 'ext4_dx_add_entry:2021', 'directory index full', 'device dm-2', 'ext4-fs warning', 'psrgisdb02 kernel', 'ext4_dx_add_entry:2021', 'kernel', 'psrgisdb02', 'apr']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row130\" class=\"row_heading level0 row130\" >130</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col0\" class=\"data row130 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col1\" class=\"data row130 col1\" >01787276</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col2\" class=\"data row130 col2\" >gpnuatnap01: OS daemons went down including all the application processes</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col3\" class=\"data row130 col3\" >Need to check what have caused the daemons to shutdown improperly including the application processes running on it.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col4\" class=\"data row130 col4\" >['os daemons', 'application processes', 'gpnuatnap01']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow130_col5\" class=\"data row130 col5\" >['os daemons', 'application processes', 'gpnuatnap01', 'application processes', 'daemons']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row131\" class=\"row_heading level0 row131\" >131</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col0\" class=\"data row131 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col1\" class=\"data row131 col1\" >02001007</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col2\" class=\"data row131 col2\" >messages file getting filled</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col3\" class=\"data row131 col3\" >Messages file is getting filled due to below error logs:\n",
       "ec 28 08:44:36 wupra00a0219 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full\n",
       "It looks some directory index is full, how to identify which directory it is and how to resolve the issue.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col4\" class=\"data row131 col4\" >['messages file']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow131_col5\" class=\"data row131 col5\" >['messages file', 'directory index full', 'directory index', 'ext4-fs warning', 'error logs', 'wupra00a0219 kernel', 'device dm-3', 'directory', 'full', 'messages file', 'ec']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row132\" class=\"row_heading level0 row132\" >132</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col0\" class=\"data row132 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col1\" class=\"data row132 col1\" >01894225</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col2\" class=\"data row132 col2\" >cannot move no space left on device</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col3\" class=\"data row132 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We have 1 prod server and 1 storage partition it have. \n",
       "While trying to copy/move files from /ftpdata/home/6182079/inbox/20170719/20792212120170718T194831/documents to /ftpdata/CLEAN_UP_FOLDER/DOCUMENTS/ . we are geting error  'cannot move no space left on device' .\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Frequently\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Sos report , N number of users</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col4\" class=\"data row132 col4\" >['device', 'space']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow132_col5\" class=\"data row132 col5\" >['device', 'space', 'storage partition', 'prod server', 'clean_up_folder', 'documents', 'error', 'documents/', 'home/6182079', 'space', 'files', 'device']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row133\" class=\"row_heading level0 row133\" >133</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col0\" class=\"data row133 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col1\" class=\"data row133 col1\" >02271184</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col2\" class=\"data row133 col2\" >dfw-dbblx07c-05 got rebooted and need RCA</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col3\" class=\"data row133 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "dfw-dbblx07c-05 got rebooted and need RCA. The server is running production oracle database.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col4\" class=\"data row133 col4\" >['rca', 'dbblx07c-05', 'dfw']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow133_col5\" class=\"data row133 col5\" >['rca', 'dbblx07c-05', 'dfw', 'production oracle database', 'rca', 'server', 'dbblx07c-05', 'dfw']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row134\" class=\"row_heading level0 row134\" >134</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col0\" class=\"data row134 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col1\" class=\"data row134 col1\" >01894802</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col2\" class=\"data row134 col2\" >The oracle database server crashed suddenly</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col3\" class=\"data row134 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "The server crashed and seems to be \"recovering journal\", \" clearing orphaned inodes\" and doing a \" clean\" of the file systems. I have a screen shot attached.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col4\" class=\"data row134 col4\" >['oracle database server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow134_col5\" class=\"data row134 col5\" >['oracle database server', 'file systems', 'orphaned inodes', 'screen shot', 'clean', 'journal', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row135\" class=\"row_heading level0 row135\" >135</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col0\" class=\"data row135 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col1\" class=\"data row135 col1\" >01759421</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col2\" class=\"data row135 col2\" >Unable to create a file OR directory.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col3\" class=\"data row135 col3\" >It is ext4 filesystem.\n",
       "\n",
       "Contact number: +1 408-801-7421\n",
       "\n",
       "Unable to create a file OR directory.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col4\" class=\"data row135 col4\" >['directory', 'file', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow135_col5\" class=\"data row135 col5\" >['directory', 'file', 'unable', 'contact number', 'ext4 filesystem', '+1', 'unable', 'directory', 'file']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row136\" class=\"row_heading level0 row136\" >136</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col0\" class=\"data row136 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col1\" class=\"data row136 col1\" >02326437</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col2\" class=\"data row136 col2\" >Server does not boot. Black screen comes up with cursor blinking on top left corner.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col3\" class=\"data row136 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "During server reboot OS does not boot.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Issue happened first time during reboot earlier today.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "reboot was done approx 12:00 UK time.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "This is a test oracle server but a lot of teams rely on it heavily.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col4\" class=\"data row136 col4\" >['top left corner', 'black screen', 'cursor', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow136_col5\" class=\"data row136 col5\" >['top left corner', 'black screen', 'cursor', 'server', 'server reboot os']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row137\" class=\"row_heading level0 row137\" >137</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col0\" class=\"data row137 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col1\" class=\"data row137 col1\" >01856932</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col2\" class=\"data row137 col2\" >Dmesg Error_EXT4-fs warning (device dm-57): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col3\" class=\"data row137 col3\" >Hi Team , \n",
       "\n",
       "We have observered multiple error message mentioned in the problem statement .\n",
       "\n",
       "Could you please look into the attached sosreport and advice us as it is  very critical server for us.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col4\" class=\"data row137 col4\" >['directory index full', 'device dm-57', 'error_ext4-fs warning']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow137_col5\" class=\"data row137 col5\" >['directory index full', 'device dm-57', 'error_ext4-fs warning', 'multiple error message', 'problem statement', 'attached sosreport', 'critical server', 'team']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row138\" class=\"row_heading level0 row138\" >138</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col0\" class=\"data row138 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col1\" class=\"data row138 col1\" >01843291</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col2\" class=\"data row138 col2\" >Performance issue on application server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col3\" class=\"data row138 col3\" >We have one application server on which oracle weblogic is installed. We are observing slowness and performance issue on the same. As per oracle team, run queue is very high in vmstat, so after thie suggestion we have taken reboot the server, still observing slowness and high run queue on the same.\n",
       "Please find vmstat log and sos report attached\n",
       "\n",
       "[root@losapp ~]# vmstat 2\n",
       "procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----\n",
       " r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n",
       "15  0      0 19321692 4190348 4900392    0    0    36    44    8   97 23  3 73  0  0\n",
       "16  0      0 19314392 4190364 4901288    0    0     0  1168 28740 53932 45  7 48  0  0\n",
       "13  0      0 19287840 4190364 4901408    0    0     0   494 25193 33690 44  7 49  0  0\n",
       "17  0      0 19267912 4190372 4901640    0    0     0   640 23273 10585 44  6 49  0  0\n",
       "20  0      0 19265076 4190396 4901944    0    0     0   552 19434 6752 41  5 54  0  0\n",
       "16  0      0 19264472 4190396 4902124    0    0     0   684 19855 4734 45  4 50  1  0\n",
       "12  0      0 19258700 4190420 4902424    0    0     0   954 19884 8030 37  6 57  0  0\n",
       "17  0      0 19261936 4190420 4902516    0    0     0   116 16546 5303 34  5 61  0  0\n",
       "13  0      0 19238780 4190428 4902756    0    0     0   632 19896 5899 47  3 50  0  0\n",
       "15  0      0 19236892 4190444 4903112    0    0     0   510 19929 14847 40  5 55  0  0\n",
       "13  0      0 19237304 4190444 4903264    0    0     0    82 20118 37890 35  6 59  0  0\n",
       "\n",
       "\n",
       "Also suggest whether run queue value in range of 15-20 is high or normal.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col4\" class=\"data row138 col4\" >['application server', 'performance issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow138_col5\" class=\"data row138 col5\" >['application server', 'performance issue', 'high run queue', 'vmstat log', 'vmstat', 'queue value', 'oracle team', 'oracle weblogic', 'high', 'queue', 'application server', 'procs -----------memory----------']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row139\" class=\"row_heading level0 row139\" >139</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col0\" class=\"data row139 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col1\" class=\"data row139 col1\" >02393633</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col2\" class=\"data row139 col2\" >EXT3-fs (dm-126): warning: ext3_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col3\" class=\"data row139 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Its a 4 node Oracle Dbase Cluster and we are seeing the same error message on all four servers. The server is connected to the same Hitachi Array.\n",
       "\n",
       "May 29 13:33:03 dal-dbblx007-08 kernel: EXT3-fs (dm-126): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 13:33:03 dal-dbblx007-08 kernel: EXT3-fs (dm-126): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 13:35:12 dal-dbblx007-08 kernel: EXT3-fs (dm-126): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 13:35:12 dal-dbblx007-08 kernel: EXT3-fs (dm-126): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 13:35:12 dal-dbblx007-08 kernel: EXT3-fs (dm-126): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "May 29 12:21:03 dal-dbblx008-08 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 12:24:05 dal-dbblx008-08 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 12:24:05 dal-dbblx008-08 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 12:24:05 dal-dbblx008-08 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 12:24:05 dal-dbblx008-08 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 12:24:05 dal-dbblx008-08 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "May 29 19:26:59 dal-dbblx009-08 kernel: EXT3-fs (dm-102): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:37:04 dal-dbblx009-08 kernel: EXT3-fs (dm-102): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:37:04 dal-dbblx009-08 kernel: EXT3-fs (dm-102): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:37:04 dal-dbblx009-08 kernel: EXT3-fs (dm-102): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:37:04 dal-dbblx009-08 kernel: EXT3-fs (dm-102): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:37:04 dal-dbblx009-08 kernel: EXT3-fs (dm-102): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "\n",
       "May 29 19:28:22 dal-dbblx009-02 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:28:22 dal-dbblx009-02 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:29:00 dal-dbblx009-02 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:29:00 dal-dbblx009-02 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:29:00 dal-dbblx009-02 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:29:00 dal-dbblx009-02 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "May 29 19:29:00 dal-dbblx009-02 kernel: EXT3-fs (dm-304): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "In production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly since morning PST.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Yes.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col4\" class=\"data row139 col4\" >['directory index full', 'ext3_dx_add_entry', 'warning', 'dm-126', 'ext3-fs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow139_col5\" class=\"data row139 col5\" >['directory index full', 'ext3_dx_add_entry', 'warning', 'dm-126', 'ext3-fs', 'directory index full', 'same error message', 'node oracle dbase cluster', 'same hitachi array', 'dal', 'ext3_dx_add_entry', 'ext3-fs', 'kernel', 'warning', 'dm-304']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row140\" class=\"row_heading level0 row140\" >140</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col0\" class=\"data row140 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col1\" class=\"data row140 col1\" >01694970</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col2\" class=\"data row140 col2\" >Application not able to start on dcaldd145 server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col3\" class=\"data row140 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Application not able to start on dcaldd145 server, will provide the core dump report, please analyse and update the RCA for this issue\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Development\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "certain times\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "Will provide core dump and sosreport files</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col4\" class=\"data row140 col4\" >['dcaldd145 server', 'able', 'application']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow140_col5\" class=\"data row140 col5\" >['dcaldd145 server', 'able', 'application', 'core dump report', 'dcaldd145 server', 'rca', 'analyse', 'able', 'application', 'issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row141\" class=\"row_heading level0 row141\" >141</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col0\" class=\"data row141 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col1\" class=\"data row141 col1\" >01706752</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col2\" class=\"data row141 col2\" >No tengo mas inodos libres</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col3\" class=\"data row141 col3\" >Nos muestra error de que no hay mas inodos libres.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col4\" class=\"data row141 col4\" >['tengo mas', 'libres']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow141_col5\" class=\"data row141 col5\" >['tengo mas', 'libres', 'nos muestra error', 'hay mas', 'libres', 'que']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row142\" class=\"row_heading level0 row142\" >142</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col0\" class=\"data row142 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col1\" class=\"data row142 col1\" >01813420</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col2\" class=\"data row142 col2\" >After rebooting server nzxpdb451 database not coming up</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col3\" class=\"data row142 col3\" >Getting error as below \n",
       "\n",
       "Mar 18 09:32:00 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:00 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:00 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:00 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:00 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:01 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:01 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:01 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:01 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!\n",
       "Mar 18 09:32:01 nzxpdb451 kernel: EXT4-fs warning (device dm-9): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col4\" class=\"data row142 col4\" >['server nzxpdb451 database']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow142_col5\" class=\"data row142 col5\" >['server nzxpdb451 database', 'directory index full', 'ext4-fs warning', 'kernel', 'nzxpdb451', 'mar', 'device', 'error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row143\" class=\"row_heading level0 row143\" >143</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col0\" class=\"data row143 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col1\" class=\"data row143 col1\" >02262758</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col2\" class=\"data row143 col2\" >Warning message related to ext4 file system</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col3\" class=\"data row143 col3\" >We can see warning message on the server \n",
       "EXT4-fs warning (device dm-2): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col4\" class=\"data row143 col4\" >['ext4 file system', 'message']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow143_col5\" class=\"data row143 col5\" >['ext4 file system', 'message', 'device dm-2', 'directory index full', 'ext4-fs warning', 'server', 'message']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row144\" class=\"row_heading level0 row144\" >144</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col0\" class=\"data row144 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col1\" class=\"data row144 col1\" >02434900</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col2\" class=\"data row144 col2\" >Systems Were Unresponsive</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col3\" class=\"data row144 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Both systems started to become unresponsive and it was taking several minutes to even login as root and when attempting regular system commands they would also take minutes to return, if at all.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "We experienced this issue on both servers lnx20257 and lnx20258 inproduction\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "We had not noticed similar issues on these two servers in the recent past.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "We were notified at approximately 08:55pm Eastern Time on Tuesday 7/23 after the DBA team received an alert for a job that was running beyond it's scheduled time and they asked us to investigate.  They were unable to connect to the oracle database with sqlplus at that time.  That was when we noticed that it was taking several minutes to even login via SSH or the console as root and traditional commands like ps or top were taking several minutes to respond, if at all.  Because these were production systems and the relevant applications were inaccessible, the DBAs requested that we reboot each server one at a time, to get the databases back online as quickly as possible.  Thus, we were unable to gather very much relevant information at the time due to the slow response and need to reboot them quickly.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col4\" class=\"data row144 col4\" >['unresponsive', 'systems']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow144_col5\" class=\"data row144 col5\" >['unresponsive', 'systems', 'regular system commands', 'several minutes', 'minutes', 'root', 'unresponsive', 'systems']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row145\" class=\"row_heading level0 row145\" >145</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col0\" class=\"data row145 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col1\" class=\"data row145 col1\" >01819663</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col2\" class=\"data row145 col2\" >Performance issue (I/O slowness) - lrau1p17 and lrau1p18</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col3\" class=\"data row145 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Server response slow due to high I/O\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Frequnently</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col4\" class=\"data row145 col4\" >['o slowness', 'performance issue', 'lrau1p17', 'i', 'lrau1p18']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow145_col5\" class=\"data row145 col5\" >['o slowness', 'performance issue', 'lrau1p17', 'i', 'lrau1p18', 'server response slow', 'high i']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row146\" class=\"row_heading level0 row146\" >146</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col0\" class=\"data row146 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col1\" class=\"data row146 col1\" >02373418</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col2\" class=\"data row146 col2\" >Directory Index Full when recovering from backup</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col3\" class=\"data row146 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hello,\n",
       "\n",
       "we get an error (Directory Index Full) while trying to recover from backup.\n",
       "\n",
       "Failed to create /opt/lhm/jakarta-tomcat/temp/axis2-tmp-4106905634552952316.tmp/axis26938699199192754322logging.mar:\n",
       "No space left on device\n",
       "\n",
       "dmesg output (multiple): \n",
       "Apr 26 21:13:50 ewookkop001 kernel: EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "\n",
       "/var/log/messages (multiple):\n",
       "Apr 26 21:13:50 ewookkop001 kernel: EXT4-fs warning (device dm-5): ext4_dx_add_entry:2016: Directory index full!\n",
       "\n",
       "df -i show at this time: 3% inodes in use\n",
       "\n",
       "We check the filesystem. No errors:\n",
       "\n",
       "root@ewookkop001 [0] :~\n",
       "# e2fsck -p -f /dev/mapper/data-app\n",
       "/dev/mapper/data-app: 241054/9830400 Dateien (0.1% nicht zusammenhängend), 31208843/52428800 Blöcke\n",
       "\n",
       "root@ewookkop001 [0] :~\n",
       "# fsck.ext4 -f /dev/mapper/data-app\n",
       "e2fsck 1.42.9 (28-Dec-2013)\n",
       "Durchgang 1: Prüfe Inodes, Blocks und Größen\n",
       "Durchgang 2: Prüfe die Verzeichnisstruktur\n",
       "Durchgang 3: Prüfe Verzeichnis-Verknüpfungen\n",
       "Durchgang 4: Überprüfe die Referenzzähler\n",
       "Durchgang 5: Überprüfe die zusammengefasste Gruppeninformation\n",
       "/dev/mapper/data-app: 241054/9830400 Dateien (0.1% nicht zusammenhängend), 31208843/52428800 Blöcke\n",
       "\n",
       "This error occurs when we recover the files from backup.\n",
       "We delete all files in /opt/lhm and recover the files from backup. \n",
       "\n",
       "The Backup works on a testsystem (el7testvmp002).\n",
       "\n",
       "tune2fs output error system ewookkop001\n",
       "root@ewookkop001 [0] :~\n",
       "# tune2fs -l /dev/mapper/data-app \n",
       "tune2fs 1.42.9 (28-Dec-2013)\n",
       "Filesystem volume name:   <none>\n",
       "Last mounted on:          /opt/lhm\n",
       "Filesystem UUID:          eb48a0a2-fa76-4f7b-a35b-3336c1b624b6\n",
       "Filesystem magic number:  0xEF53\n",
       "Filesystem revision #:    1 (dynamic)\n",
       "Filesystem features:      has_journal ext_attr dir_index filetype needs_recovery meta_bg extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize\n",
       "Filesystem flags:         signed_directory_hash \n",
       "Default mount options:    user_xattr acl\n",
       "Filesystem state:         clean\n",
       "Errors behavior:          Continue\n",
       "Filesystem OS type:       Linux\n",
       "Inode count:              9830400\n",
       "Block count:              52428800\n",
       "Reserved block count:     1572922\n",
       "Free blocks:              21219957\n",
       "Free inodes:              9589346\n",
       "First block:              1\n",
       "Block size:               1024\n",
       "Fragment size:            1024\n",
       "Group descriptor size:    64\n",
       "Blocks per group:         8192\n",
       "Fragments per group:      8192\n",
       "Inodes per group:         1536\n",
       "Inode blocks per group:   192\n",
       "First meta block group:   96\n",
       "Flex block group size:    16\n",
       "Filesystem created:       Thu Aug  2 08:00:08 2018\n",
       "Last mount time:          Tue Apr 30 19:21:02 2019\n",
       "Last write time:          Tue Apr 30 19:21:02 2019\n",
       "Mount count:              1\n",
       "Maximum mount count:      -1\n",
       "Last checked:             Tue Apr 30 19:20:52 2019\n",
       "Check interval:           0 (<none>)\n",
       "Lifetime writes:          167 GB\n",
       "Reserved blocks uid:      0 (user root)\n",
       "Reserved blocks gid:      0 (group root)\n",
       "First inode:              11\n",
       "Inode size:               128\n",
       "Journal inode:            8\n",
       "First orphan inode:       5554177\n",
       "Default directory hash:   half_md4\n",
       "Directory Hash Seed:      9242b584-ac0c-43ee-bb43-3d2a4e263212\n",
       "Journal backup:           inode blocks\n",
       "\n",
       "tune2fs output working system \n",
       "root@el7testvmp002 [0] :/opt/lhm\n",
       "# tune2fs -l /dev/mapper/data-app \n",
       "tune2fs 1.42.9 (28-Dec-2013)\n",
       "Filesystem volume name:   <none>\n",
       "Last mounted on:          /opt/lhm\n",
       "Filesystem UUID:          61c35f2d-684d-4058-bb30-95cf1da09c92\n",
       "Filesystem magic number:  0xEF53\n",
       "Filesystem revision #:    1 (dynamic)\n",
       "Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super huge_file uninit_bg dir_nlink extra_isize\n",
       "Filesystem flags:         signed_directory_hash \n",
       "Default mount options:    user_xattr acl\n",
       "Filesystem state:         clean\n",
       "Errors behavior:          Continue\n",
       "Filesystem OS type:       Linux\n",
       "Inode count:              3276800\n",
       "Block count:              52428800\n",
       "Reserved block count:     0\n",
       "Free blocks:              51552191\n",
       "Free inodes:              3276789\n",
       "First block:              1\n",
       "Block size:               1024\n",
       "Fragment size:            1024\n",
       "Group descriptor size:    64\n",
       "Reserved GDT blocks:      256\n",
       "Blocks per group:         8192\n",
       "Fragments per group:      8192\n",
       "Inodes per group:         512\n",
       "Inode blocks per group:   128\n",
       "Flex block group size:    16\n",
       "Filesystem created:       Thu May  2 10:45:34 2019\n",
       "Last mount time:          Thu May  2 10:45:39 2019\n",
       "Last write time:          Thu May  2 10:45:39 2019\n",
       "Mount count:              1\n",
       "Maximum mount count:      -1\n",
       "Last checked:             Thu May  2 10:45:34 2019\n",
       "Check interval:           0 (<none>)\n",
       "Lifetime writes:          33 MB\n",
       "Reserved blocks uid:      0 (user root)\n",
       "Reserved blocks gid:      0 (group root)\n",
       "First inode:              11\n",
       "Inode size:               256\n",
       "Required extra isize:     28\n",
       "Desired extra isize:      28\n",
       "Journal inode:            8\n",
       "Default d</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col4\" class=\"data row146 col4\" >['directory index full', 'backup']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow146_col5\" class=\"data row146 col5\" >['directory index full', 'backup', 'has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super huge_file', 'user_xattr acl filesystem state', 'flex block group size', 'lhm filesystem uuid', 'tune2fs output error system ewookkop001 root@ewookkop001', 'filesystem magic number', 'filesystem os type', 'first meta block group', 'filesystem volume name', 'blocks und größen durchgang']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row147\" class=\"row_heading level0 row147\" >147</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col0\" class=\"data row147 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col1\" class=\"data row147 col1\" >01923971</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col2\" class=\"data row147 col2\" >Samba service is running & port 445 & 139 was not listening</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col3\" class=\"data row147 col3\" >Continuously getting directory index full message in logs\n",
       "\n",
       "Sep  3 02:46:37 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:46:39 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:46:40 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:46:40 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:46:40 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:46:40 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:46:40 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:47:11 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:47:11 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Sep  3 02:47:11 vvsl20315 kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col4\" class=\"data row147 col4\" >['samba service', 'port']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow147_col5\" class=\"data row147 col5\" >['samba service', 'port', 'directory index full message', 'directory index full', 'vvsl20315 kernel', 'device dm-3', 'ext4-fs warning', 'sep', 'logs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row148\" class=\"row_heading level0 row148\" >148</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col0\" class=\"data row148 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col1\" class=\"data row148 col1\" >01833866</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col2\" class=\"data row148 col2\" >Erro [11596368.850363] EXT4-fs warning (device dm-5): ext4_dx_add_entry:2018: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col3\" class=\"data row148 col3\" >Que tipo de problema/comportamento você está enfrentando? O que você espera encontrar?\n",
       "\n",
       "A apliacação de banco de dados está recebendo erro index full. Vou anexar as telas.\n",
       "\n",
       "[root@sibxp0022cld data]# df -i | grep \"/cassandra/dse-data/data\"\n",
       "/dev/mapper/sibxp0022vg-lv_cassdata  73400320 15502425 57897895   22% /cassandra/dse-data/data\n",
       "\n",
       "[root@sibxp0022cld data]# df -h | grep \"/cassandra/dse-data/data\"\n",
       "/dev/mapper/sibxp0022vg-lv_cassdata   1.1T  117G  941G  12% /cassandra/dse-data/data\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Estou com essa maquina fora do cluster</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col4\" class=\"data row148 col4\" >['directory index full', 'device dm-5', 'ext4-fs warning', 'ext4_dx_add_entry:2018', 'erro']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow148_col5\" class=\"data row148 col5\" >['directory index full', 'device dm-5', 'ext4-fs warning', 'ext4_dx_add_entry:2018', 'erro', 'banco de dados está recebendo erro index full', 'comportamento você está enfrentando', 'que você espera encontrar', 'data', 'que tipo', 'df -i', 'dse', 'apliacação', 'df', 'sibxp0022vg']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row149\" class=\"row_heading level0 row149\" >149</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col0\" class=\"data row149 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col1\" class=\"data row149 col1\" >02253193</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col2\" class=\"data row149 col2\" >Recent problem suggesting file system space issue</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col3\" class=\"data row149 col3\" >Developers on our staff related an issue in which a job was aborting when running a large count of report generations with a generic space issue message. Within the system log on the server I found:\n",
       "EXT3-fs warning (device dm-2): ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "I found RedHat doc https://access.redhat.com/solutions/29894  which was helpful. The customer archived files to remedy the immediate issue.  Can you tell me how we can monitor the system to see when we're approaching a critical limit on the indexing ?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col4\" class=\"data row149 col4\" >['file system space issue', 'recent problem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow149_col5\" class=\"data row149 col5\" >['file system space issue', 'recent problem', 'generic space issue message', 'directory index full', 'redhat doc https://access.redhat.com/solutions/29894', 'immediate issue', 'system log', 'ext3-fs warning', 'issue', 'large count', 'device dm-2', 'report generations']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row150\" class=\"row_heading level0 row150\" >150</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col0\" class=\"data row150 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col1\" class=\"data row150 col1\" >01791022</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col2\" class=\"data row150 col2\" >rsync is failing with no space left on device</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col3\" class=\"data row150 col3\" >rsync is failing with no space left on device . But we have enough space on the destination server.\n",
       "\n",
       "Please check and let us know the reason .</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col4\" class=\"data row150 col4\" >['device', 'space', 'rsync']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow150_col5\" class=\"data row150 col5\" >['device', 'space', 'rsync', 'enough space', 'destination server', 'space', 'device', 'rsync', 'reason']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row151\" class=\"row_heading level0 row151\" >151</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col0\" class=\"data row151 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col1\" class=\"data row151 col1\" >02262694</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col2\" class=\"data row151 col2\" >kernel: EXT3-fs warning (device dm-4): ext3_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col3\" class=\"data row151 col3\" >We are getting below error in messages logs continuously. Please advise for the same.\n",
       "\n",
       "kernel: EXT3-fs warning (device dm-4): ext3_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col4\" class=\"data row151 col4\" >['directory index full', 'device dm-4', 'ext3-fs warning', 'ext3_dx_add_entry', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow151_col5\" class=\"data row151 col5\" >['directory index full', 'device dm-4', 'ext3-fs warning', 'ext3_dx_add_entry', 'kernel', 'directory index full', 'device dm-4', 'ext3-fs warning', 'ext3_dx_add_entry', 'same', 'kernel', 'messages', 'error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row152\" class=\"row_heading level0 row152\" >152</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col0\" class=\"data row152 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col1\" class=\"data row152 col1\" >02000622</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col2\" class=\"data row152 col2\" >EXT4-fs Directory index full! warnings caused down time - need root cause</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col3\" class=\"data row152 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "EXT4-fs Directory index full! warnings caused down time - need root cause\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Dec 26 14:06:18 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:19 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:19 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:19 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:19 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:37 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:37 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:37 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:37 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:06:37 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:07:44 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:07:44 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:07:44 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:07:44 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:07:44 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:09:19 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full! \n",
       "Dec 26 14:09:19 clmoemdb1p kernel: EXT4-fs warning (device dm-26): ext4_dx_add_entry:2024: Directory index full!\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Once</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col4\" class=\"data row152 col4\" >['ext4-fs directory index full', 'need root cause', 'time', 'warnings']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow152_col5\" class=\"data row152 col5\" >['ext4-fs directory index full', 'need root cause', 'time', 'warnings', 'ext4-fs directory index full', 'need root cause', 'time', 'warnings']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row153\" class=\"row_heading level0 row153\" >153</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col0\" class=\"data row153 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col1\" class=\"data row153 col1\" >01907700</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col2\" class=\"data row153 col2\" >HIGH Memory utilization reported in the server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col3\" class=\"data row153 col3\" >HIGH Memory utilization reported in the server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col4\" class=\"data row153 col4\" >['high memory utilization', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow153_col5\" class=\"data row153 col5\" >['high memory utilization', 'server', 'high memory utilization', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row154\" class=\"row_heading level0 row154\" >154</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col0\" class=\"data row154 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col1\" class=\"data row154 col1\" >01947705</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col2\" class=\"data row154 col2\" >Messages logs flooded with warning (device dm-3): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col3\" class=\"data row154 col3\" >The /var/log/messages is flooded with below errors. Looking at the disk path dm-3, it points to one of the filesystems /apps. We suspect long filenames being used, however the app team says it was setup this way for a long while and these errors started reporting only recently. Need assistance to determine what could be causing this errors being logged and how to correct it.\n",
       "\n",
       "Oct  7 02:28:40 *** kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Oct  7 02:28:40 **** kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!\n",
       "Oct  7 02:28:40 *** kernel: EXT4-fs warning (device dm-3): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col4\" class=\"data row154 col4\" >['directory index full', 'device dm-3', 'messages logs', 'warning']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow154_col5\" class=\"data row154 col5\" >['directory index full', 'device dm-3', 'messages logs', 'warning', 'directory index full', 'device dm-3', 'disk path dm-3', 'ext4-fs warning', 'long while', 'long filenames', 'app team', 'errors', 'kernel', 'way']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row155\" class=\"row_heading level0 row155\" >155</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col0\" class=\"data row155 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col1\" class=\"data row155 col1\" >02340668</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col2\" class=\"data row155 col2\" >Mar 18 07:45:23 betapmdmz4 kernel: EXT4-fs warning (device dm-48): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col3\" class=\"data row155 col3\" >Although the files/dirs. have been cleaned up on the filesystem but still keeps receiving warning messages : ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col4\" class=\"data row155 col4\" >['directory index full', 'ext4-fs warning', 'device', 'kernel', 'betapmdmz4', 'mar']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow155_col5\" class=\"data row155 col5\" >['directory index full', 'ext4-fs warning', 'device', 'kernel', 'betapmdmz4', 'mar', 'directory index full', 'warning messages', 'filesystem', 'dirs', 'files']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row156\" class=\"row_heading level0 row156\" >156</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col0\" class=\"data row156 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col1\" class=\"data row156 col1\" >02254034</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col2\" class=\"data row156 col2\" >no space left” error while creating subfolders/files</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col3\" class=\"data row156 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "during a stress test last weekend. Although there is enough space and inodes on /app, system gives “no space left” error while creating subfolders/files inside a single directory after 300K folders/files. So it is becoming unstable/unhealthy after creating 300K subfolders/files inside a single directory.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "I have run a stress test and seen the same error. Please see CLI output below. Location of the file which I stressed is /app/DATA/TEST1/Sets_PROD/stress_test/\n",
       "I simply copied files from other folders (under /app/DATA/TEST1/Sets_PROD/Javier_all_Ranges/TAR/) to this folder, and it gave error when the total number of file inside the folder reaches 188363.\n",
       "[vd_jmoran@ss-l-t-obo02996 stress_test]$ cp -a /app/DATA/TEST1/Sets_PROD/Javier_all_Ranges/TAR/17957/. /app/DATA/TEST1/Sets_PROD/stress_test/\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179577071636-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179577897383-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179576798803-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179579551530-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179579909701-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179578775157-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179577866269-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179570291524-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179570121732-sp.tgz’: No space left on device\n",
       "cp: cannot create regular file ‘/app/DATA/TEST1/Sets_PROD/stress_test/./0000F0-HZNSTB-179570260208-sp.tgz’: No space left on device\n",
       "\n",
       "[vd_jmoran@ss-l-t-obo02996 stress_test]$ ls | wc -l\n",
       "188363\n",
       "[vd_jmoran@ss-l-t-obo02996 stress_test]$ df -i /app/DATA/TEST1/Sets_PROD/stress_test/\n",
       "Filesystem Inodes IUsed IFree IUse% Mounted on\n",
       "/dev/mapper/vgapp-lv_app 63045632 8358044 54687588 14% /app\n",
       "[vd_jmoran@ss-l-t-obo02996 stress_test]$ df -h /app/DATA/TEST1/Sets_PROD/stress_test/\n",
       "Filesystem Size Used Avail Use% Mounted on\n",
       "/dev/mapper/vgapp-lv_app 281G 72G 195G 28% /app</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col4\" class=\"data row156 col4\" >['files', 'subfolders', 'error', 'space']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow156_col5\" class=\"data row156 col5\" >['files', 'subfolders', 'error', 'space', 'stress test last weekend', 'k subfolders', 'enough space', 'single directory', 'k folders', 'files', 'space', 'subfolders', 'unstable', 'error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row157\" class=\"row_heading level0 row157\" >157</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col0\" class=\"data row157 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col1\" class=\"data row157 col1\" >02425826</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col2\" class=\"data row157 col2\" >System File systems went into read-only mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col3\" class=\"data row157 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hi\n",
       "System File systems (/var, /, /root) went into read-only mode and we had to reboot the server to bring it back online.\n",
       "Please refer the attached screenshots for more details about the errors.\n",
       "We did check with Cisco (hardware vendor) on this and they found nothing on hardware side and pointing towards OS.\n",
       "\n",
       "Could you please analyze the logs shared.\n",
       "\n",
       "Regards\n",
       "Tino</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col4\" class=\"data row157 col4\" >['system file systems', 'mode']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow157_col5\" class=\"data row157 col5\" >['system file systems', 'mode', 'system file systems', 'hardware vendor', 'hardware side', 'more details', 'regards tino', 'errors', 'logs', 'cisco', 'server', 'mode']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row158\" class=\"row_heading level0 row158\" >158</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col0\" class=\"data row158 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col1\" class=\"data row158 col1\" >02495596</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col2\" class=\"data row158 col2\" >/var/log/messages is getting keep growing due to this error \" kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\"</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col3\" class=\"data row158 col3\" >/var/log/messages is getting keep growing/full  due to this error \n",
       " i try to collect sosreprot.seems it is failed due no space left on device . But space are there\n",
       "\n",
       " Setting up archive ...\n",
       " Setting up plugins ...\n",
       " Running plugins. Please wait ...\n",
       "\n",
       "  Running 56/77: python...                   \n",
       " No space left on device while collecting plugin data\n",
       "\n",
       "[root@ny4pbxcc01 ~]#\n",
       " \n",
       "Error : \n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "Oct 16 00:45:39 ny4pbxcc01 kernel: EXT3-fs (dm-1): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "Inode Details : \n",
       "[root@ny4pbxcc01 log]# df -i \n",
       "Filesystem             Inodes    IUsed   IFree IUse% Mounted on\n",
       "/dev/sda2               96000    13467   82533   15% /\n",
       "tmpfs                 3057825        1 3057824    1% /dev/shm\n",
       "/dev/sda1              128016       53  127963    1% /boot\n",
       "/dev/mapper/sticore-lopt\n",
       "                       655360    61403  593957   10% /opt\n",
       "/dev/mapper/sticore-ltmp\n",
       "                       131072       56  131016    1% /tmp\n",
       "/dev/mapper/sticore-lusr\n",
       "                       327680    44360  283320   14% /usr\n",
       "/dev/mapper/sticore-lvar\n",
       "                       720896    24383  696513    4% /var\n",
       "/dev/mapper/sticore-lcrash\n",
       "                       131072       10  131062    1% /var/crash\n",
       "/dev/mapper/localvg-localvg1\n",
       "                     35840000 28955142 6884858   81% /localvg1</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col4\" class=\"data row158 col4\" >['directory index full', 'warning', 'dm-1', 'ext3_dx_add_entry', 'ext3-fs', 'error', 'kernel', 'messages', 'log']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow158_col5\" class=\"data row158 col5\" >['directory index full', 'warning', 'dm-1', 'ext3_dx_add_entry', 'ext3-fs', 'error', 'kernel', 'messages', 'log', 'directory index full', '%', 'ny4pbxcc01 kernel', 'ifree iuse% mounted', 'mapper', 'root@ny4pbxcc01 log', 'full', 'sticore', 'df -i', 'ext3_dx_add_entry']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row159\" class=\"row_heading level0 row159\" >159</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col0\" class=\"data row159 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col1\" class=\"data row159 col1\" >02173889</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col2\" class=\"data row159 col2\" >MPAPSFTLMV04 kernel: EXT4-fs warning (device dm-4): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col3\" class=\"data row159 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "As per error, error is for dm-3 device(vg_archive-lv_backup).\n",
       "\n",
       "We have already checked inode utilization(df -i) for this concerned device and inode utilization is 62%.\n",
       "\n",
       "Please help us to understand the reason that why we are getting directory index full error even when inode utilization is not 100%.\n",
       "\n",
       "Also, we are getting continous same error in /var/log/messages and due to that log messages file becoming larger and affecting file system utilization.\n",
       "\n",
       "Note:\n",
       "Error for concerned device is Application file system and is very critical one.\n",
       "\n",
       "Please help us to understand nature of error and provide solutions to fix the issue.\n",
       "------------------------------------------------------------------------------------------------------------------------\n",
       "Output of lsblk\n",
       "\n",
       "[root@MPAPSFTLMV04 ~]# lsblk\n",
       "NAME                                MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n",
       "sda                                   8:0    0   150G  0 disk\n",
       "├─sda1                                8:1    0     1G  0 part /boot\n",
       "└─sda2                                8:2    0 126.5G  0 part\n",
       "  ├─vgroot-swap (dm-0)              253:0    0    24G  0 lvm  [SWAP]\n",
       "  ├─vgroot-slash (dm-1)             253:1    0    30G  0 lvm  /\n",
       "  ├─vgroot-lv_varlogaudit (dm-6)    253:6    0     2G  0 lvm  /var/log/audit\n",
       "  ├─vgroot-lv_tmp (dm-7)            253:7    0     5G  0 lvm  /tmp\n",
       "  ├─vgroot-lv_varlog (dm-8)         253:8    0     5G  0 lvm  /var/log\n",
       "  ├─vgroot-lv_usr (dm-9)            253:9    0    10G  0 lvm  /usr\n",
       "  ├─vgroot-opt (dm-10)              253:10   0    10G  0 lvm  /opt\n",
       "  ├─vgroot-var (dm-11)              253:11   0    20G  0 lvm  /var\n",
       "  └─vgroot-home (dm-12)             253:12   0    20G  0 lvm  /home\n",
       "sdb                                   8:16   0   200G  0 disk\n",
       "└─vgapp-lv_INTERFACE (dm-5)         253:5    0   199G  0 lvm  /INTERFACEOLD\n",
       "sdc                                   8:32   0   400G  0 disk\n",
       "└─vg_archive-lv_backup (dm-4)       253:4    0   400G  0 lvm  /BACKUP\n",
       "sde                                   8:64   0    50G  0 disk\n",
       "└─vg_backup_test-lv_btest (dm-2)    253:2    0    48G  0 lvm\n",
       "sdd                                   8:48   0   400G  0 disk\n",
       "└─vginterface-lv_interface (dm-3)   253:3    0   599G  0 lvm  /INTERFACE\n",
       "sdf                                   8:80   0   200G  0 disk\n",
       "└─sdf1                                8:81   0   200G  0 part\n",
       "  └─vginterface-lv_interface (dm-3) 253:3    0   599G  0 lvm  /INTERFACE\n",
       "-----------------------------------------------------------------------------------------------------------------------\n",
       "Output of df -i\n",
       "\n",
       "[root@MPAPSFTLMV04 ~]# df -i\n",
       "Filesystem              Inodes    IUsed     IFree IUse% Mounted on\n",
       "/dev/mapper/vgroot-slash\n",
       "                       1966080    12371   1953709    1% /\n",
       "tmpfs                  1782159        1   1782158    1% /dev/shm\n",
       "/dev/sda1                65536       54     65482    1% /boot\n",
       "/dev/mapper/vgroot-home\n",
       "                       1310720     5139   1305581    1% /home\n",
       "/dev/mapper/vgroot-opt\n",
       "                        655360    30096    625264    5% /opt\n",
       "/dev/mapper/vgroot-lv_tmp\n",
       "                        327680      315    327365    1% /tmp\n",
       "/dev/mapper/vgroot-lv_usr\n",
       "                        655360   159493    495867   25% /usr\n",
       "/dev/mapper/vgroot-var\n",
       "                       1310720    30447   1280273    3% /var\n",
       "/dev/mapper/vgroot-lv_varlog\n",
       "                        327680      191    327489    1% /var/log\n",
       "/dev/mapper/vgroot-lv_varlogaudit\n",
       "                        131072       16    131056    1% /var/log/audit\n",
       "/dev/mapper/vgapp-lv_INTERFACE\n",
       "                      13041664 10162424   2879240   78% /INTERFACEOLD\n",
       "/dev/mapper/vginterface-lv_interface\n",
       "                     157024256 42634007 114390249   28% /INTERFACE\n",
       "/dev/mapper/vg_archive-lv_backup\n",
       "                     104857600 64281559  40576041   62% /BACKUP\n",
       "---------------------------------------------------------------------------------------------------------------------\n",
       "\n",
       "Attaching sosreport for the reference.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col4\" class=\"data row159 col4\" >['device dm-4', 'directory index full', 'ext4-fs warning', 'mpapsftlmv04 kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow159_col5\" class=\"data row159 col5\" >['device dm-4', 'directory index full', 'ext4-fs warning', 'mpapsftlmv04 kernel', '└ ─ vgroot', '─ vgroot', '└ ─ vginterface', 'g', '─ vginterface', '─ vgapp', '─ sda1', '─ sda2', '─ sdf1', 'log messages file']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row160\" class=\"row_heading level0 row160\" >160</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col0\" class=\"data row160 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col1\" class=\"data row160 col1\" >02286044</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col2\" class=\"data row160 col2\" >Server rebooted and looking for RCA</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col3\" class=\"data row160 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Server rebooted unexpectedly\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Data Base server\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Server time : Mon Jan  7 08:05 - 08:54\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "vmcore</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col4\" class=\"data row160 col4\" >['rca', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow160_col5\" class=\"data row160 col5\" >['rca', 'server', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row161\" class=\"row_heading level0 row161\" >161</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col0\" class=\"data row161 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col1\" class=\"data row161 col1\" >02466809</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col2\" class=\"data row161 col2\" >EXT4-fs warning (device dm-28): ext4_dx_add_entry:2016: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col3\" class=\"data row161 col3\" >Hello Support,\n",
       "We  are receiving following many kernel warning messages \n",
       "\n",
       "kern  :warn  : [Sun Sep  8 02:35:48 2019] EXT4-fs warning (device dm-28): ext4_dx_add_entry:2016: Directory index full!\n",
       "kern  :warn  : [Sun Sep  8 02:35:48 2019] EXT4-fs warning (device dm-28): ext4_dx_add_entry:2016: Directory index full!\n",
       "kern  :warn  : [Sun Sep  8 02:35:48 2019] EXT4-fs warning (device dm-28): ext4_dx_add_entry:2016: Directory index full!\n",
       " \n",
       "we have notice on the following path, number of files are keep increasing,currently its 7279261 files\n",
       "/u01/app/oracle/admin/ecmpn1/adump\n",
       "\n",
       "as per device dm-28 its indicating  lvm filesystem  /u01 , please check.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col4\" class=\"data row161 col4\" >['directory index full', 'device dm-28', 'ext4-fs warning', 'ext4_dx_add_entry:2016']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow161_col5\" class=\"data row161 col5\" >['directory index full', 'device dm-28', 'ext4-fs warning', 'ext4_dx_add_entry:2016', 'directory index full', 'device dm-28', 'ext4-fs warning', 'device', 'sun sep', 'ext4_dx_add_entry:2016', 'kern', 'many kernel', 'files', 'adump']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row162\" class=\"row_heading level0 row162\" >162</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col0\" class=\"data row162 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col1\" class=\"data row162 col1\" >02135275</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col2\" class=\"data row162 col2\" >ERROR - Standby Synchro on rdc1vldcora102.d1.ad.local</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col3\" class=\"data row162 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "we are facing issue with block size of FS and we found the logs as below:\n",
       "\n",
       "Jul  5 05:59:03 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 05:59:03 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 05:59:03 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 05:59:03 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 05:59:03 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 05:59:03 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 05:59:03 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 05:59:04 rdc1vldcora102 abrt-hook-ccpp[7791]: Process 7256 (rman) of user 502 killed by SIGSEGV - dumping core Jul  5 05:59:04 rdc1vldcora102 abrt-hook-ccpp: Process 7256 (rman) of user 502 killed by SIGSEGV - dumping core\n",
       "\n",
       "Current file count on partition /u01/app/oracle/audit is 946694.\n",
       "[root@rdc1vldcora102 audit]# ls -lrt * | wc -l\n",
       "946694\n",
       "[root@rdc1vldcora102 audit]# pwd\n",
       "/u01/app/oracle/audit\n",
       "[root@rdc1vldcora102 audit]#\n",
       "\n",
       "\n",
       "we have done housekeeping on the above filesystem\n",
       "since we are still getting the error.  \n",
       "[root@rdc1vldcora102 audit]# tail -f /var/log/messages | grep ext4_dx_add_entry Jul  5 09:37:51 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 09:37:51 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 09:37:51 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 09:37:51 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       "Jul  5 09:37:51 rdc1vldcora102 kernel: EXT4-fs warning (device dm-13): ext4_dx_add_entry:2016: Directory index full!\n",
       " [root@rdc1vldcora102 audit]#\n",
       "\n",
       " \n",
       " kindly suggest us how to proceed\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "PRE-PROD\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "we are facing this issue from today</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col4\" class=\"data row162 col4\" >['standby synchro', 'rdc1vldcora102.d1.ad.local', 'error']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow162_col5\" class=\"data row162 col5\" >['standby synchro', 'rdc1vldcora102.d1.ad.local', 'error', 'directory index full', 'root@rdc1vldcora102 audit', 'device dm-13', 'core jul', 'ext4-fs warning', 'ext4_dx_add_entry jul', 'jul', '# tail -f', 'audit', 'current file count']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row163\" class=\"row_heading level0 row163\" >163</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col0\" class=\"data row163 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col1\" class=\"data row163 col1\" >02005835</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col2\" class=\"data row163 col2\" >ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col3\" class=\"data row163 col3\" >[root@crmdbn2 log]# df -TH\n",
       "Filesystem           Type   Size  Used Avail Use% Mounted on\n",
       "/dev/mapper/rootvg-LogVol01\n",
       "                     ext4   106G  1.7G   99G   2% /\n",
       "tmpfs                tmpfs  271G  899M  271G   1% /dev/shm\n",
       "/dev/sdgy1           ext4   500M   44M  431M  10% /boot\n",
       "/dev/mapper/rootvg-LogVol02\n",
       "                     ext4    53G  7.4G   43G  15% /home\n",
       "/dev/mapper/rootvg-LogVol03\n",
       "                     ext4    11G   55M  9.9G   1% /opt\n",
       "/dev/mapper/rootvg-LogVol04\n",
       "                     ext4    11G   24M  9.9G   1% /tmp\n",
       "/dev/mapper/rootvg-LogVol07\n",
       "                     ext4   265G  105G  146G  42% /u01\n",
       "/dev/mapper/rootvg-LogVol05\n",
       "                     ext4    11G  2.8G  7.2G  29% /usr\n",
       "/dev/mapper/rootvg-LogVol06\n",
       "                     ext4    11G  1.6G  8.4G  16% /var\n",
       "/dev/mapper/rootvg-netbackuplv\n",
       "                     ext4    11G  3.6G  6.4G  36% /usr/openv\n",
       "/dev/mapper/ogg2-ogg2\n",
       "                     ext4   370G   70M  351G   1% /ogg2\n",
       "/dev/asm/backup-12   acfs   4.9T  4.5T  465G  91% /backup\n",
       "[root@crmdbn2 log]# \n",
       "[root@crmdbn2 log]# df -i\n",
       "Filesystem               Inodes      IUsed     IFree IUse% Mounted on\n",
       "/dev/mapper/rootvg-LogVol01\n",
       "                        6553600      12642   6540958    1% /\n",
       "tmpfs                  66142828        267  66142561    1% /dev/shm\n",
       "/dev/sdgy1               128016         39    127977    1% /boot\n",
       "/dev/mapper/rootvg-LogVol02\n",
       "                        3276800        185   3276615    1% /home\n",
       "/dev/mapper/rootvg-LogVol03\n",
       "                         655360         79    655281    1% /opt\n",
       "/dev/mapper/rootvg-LogVol04\n",
       "                         655360         51    655309    1% /tmp\n",
       "/dev/mapper/rootvg-LogVol07\n",
       "                       16384000   13410712   2973288   82% /u01\n",
       "/dev/mapper/rootvg-LogVol05\n",
       "                         655360      91092    564268   14% /usr\n",
       "/dev/mapper/rootvg-LogVol06\n",
       "                         655360       3607    651753    1% /var\n",
       "/dev/mapper/rootvg-netbackuplv\n",
       "                         655360       2575    652785    1% /usr/openv\n",
       "/dev/mapper/ogg2-ogg2\n",
       "                       22937600         10  22937590    1% /ogg2\n",
       "/dev/asm/backup-12   9539944448 8632435208 907509240   91% /backup\n",
       "[root@crmdbn2 log]# \n",
       "[root@crmdbn2 log]# tail -f messages\n",
       "Jan  8 10:32:30 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:30 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:30 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:30 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:30 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:39 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:39 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:39 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:39 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:32:39 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:33:29 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:33:29 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:33:29 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:33:29 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!\n",
       "Jan  8 10:33:29 crmdbn2 kernel: EXT4-fs warning (device dm-107): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col4\" class=\"data row163 col4\" >['directory index full']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow163_col5\" class=\"data row163 col5\" >['directory index full', '%', 'g', 'directory index full', 'mapper', 'crmdbn2 kernel', 'root@crmdbn2 log', 'rootvg', '# tail -f messages', 'df -i filesystem', 'device dm-107']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row164\" class=\"row_heading level0 row164\" >164</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col0\" class=\"data row164 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col1\" class=\"data row164 col1\" >01731431</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col2\" class=\"data row164 col2\" >seeing</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col3\" class=\"data row164 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Seeing these on /var/log/messages\n",
       " kernel: ioctl32(siebmtshmw:5971): Unknown cmd fd(109) cmd(00001268){t:12;sz:0} arg(5daf5924) on /siebelcrm/oracle/oradiag_siebelcrm/diag/clients/user_siebelcrm/host_2874303636_80/metadata/ADR_CONTROL.ams\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Application servers / Production environment\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Seeing this since over weekend\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "Seibel performanace is very poor and unstable</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col4\" class=\"data row164 col4\" >[]</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow164_col5\" class=\"data row164 col5\" >['unknown cmd', 'oradiag_siebelcrm', 'user_siebelcrm', 'clients', 'diag', 'cmd(00001268){t:12;sz:0', 'arg(5daf5924', 'oracle', 'host_2874303636_80', 'kernel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row165\" class=\"row_heading level0 row165\" >165</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col0\" class=\"data row165 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col1\" class=\"data row165 col1\" >02201107</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col2\" class=\"data row165 col2\" >Server running with High Load</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col3\" class=\"data row165 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Server is running with very high load. Need to investigate why the load on server is so high.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "We are observing below errors, \n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!\n",
       "\n",
       "EXT3-fs (dm-4): warning: ext3_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col4\" class=\"data row165 col4\" >['high load']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow165_col5\" class=\"data row165 col5\" >['high load', 'high load', 'load', 'high', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row166\" class=\"row_heading level0 row166\" >166</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col0\" class=\"data row166 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col1\" class=\"data row166 col1\" >02482059</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col2\" class=\"data row166 col2\" >kernel: EXT4-fs warning (device dm-11): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col3\" class=\"data row166 col3\" >Getting warning in the logs regarding index full for one of the directories need assistance on best way to resolve this while avoiding a major impact to the server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col4\" class=\"data row166 col4\" >['directory index full', 'ext4-fs warning', 'device', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow166_col5\" class=\"data row166 col5\" >['directory index full', 'ext4-fs warning', 'device', 'kernel', 'best way', 'index full', 'major impact', 'directories', 'assistance', 'logs', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row167\" class=\"row_heading level0 row167\" >167</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col0\" class=\"data row167 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col1\" class=\"data row167 col1\" >02305783</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col2\" class=\"data row167 col2\" >unable to run rm -rf command ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col3\" class=\"data row167 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "i had production deployment and one of my drive is full. I am getting error  after running command rm -rf * in that directory but i am getting error as ext4_dx_add_entry: Directory index full!.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "frequently\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "24*7</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col4\" class=\"data row167 col4\" >['rm -rf command', 'directory index full', 'ext4_dx_add_entry', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow167_col5\" class=\"data row167 col5\" >['rm -rf command', 'directory index full', 'ext4_dx_add_entry', 'unable', 'directory index full', 'command rm -rf', 'error', 'full', 'directory', 'production deployment', 'ext4_dx_add_entry', 'drive']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row168\" class=\"row_heading level0 row168\" >168</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col0\" class=\"data row168 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col1\" class=\"data row168 col1\" >01821317</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col2\" class=\"data row168 col2\" >ext[3/4]_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col3\" class=\"data row168 col3\" >below error getting while file generating through application script.\n",
       "ext[3/4]_dx_add_entry: Directory index full!\"?\n",
       "daily file triggered through script around 80000.\n",
       "please provide solution for the same</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col4\" class=\"data row168 col4\" >['directory index full', 'ext[3/4]_dx_add_entry']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow168_col5\" class=\"data row168 col5\" >['directory index full', 'ext[3/4]_dx_add_entry', 'directory index full', 'application script', 'daily file', 'script', 'file', 'ext[3/4]_dx_add_entry', 'error', 'solution', 'same']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row169\" class=\"row_heading level0 row169\" >169</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col0\" class=\"data row169 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col1\" class=\"data row169 col1\" >01890361</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col2\" class=\"data row169 col2\" >kernel: EXT4-fs warning (device dm-49): ext4_dx_add_entry: Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col3\" class=\"data row169 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "CMPEMPR1@gbwxorcrmp01a: The connection to the instance failed. Failed to establish Oracle JDBC connection protocol to Host [gbwxorcrmp01a.gbcaydc.baml.com-CMPEMPR1], user account [foglight5] . Reason : [ORA-00604: error occurred at recursive SQL level 1ORA-02002: error while writing to audit trailORA-09925: Unable to create audit trail file- Profile:OracleProfile{host= gbwxorcrmp01a.gbcaydc.baml.com\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Over 7 million files are being created by audit files under /oracle/admin/CMPEMPR/adump.\n",
       "Length of file name 49 characters.\n",
       "CMPEMPR1_ora_39129_20170704103755681840949342.xml\n",
       "# tune2fs -l /dev/mapper/volgrp02-lv_oracle|grep features\n",
       "Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery exten flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Eventually overtime audit would unable to be able to write to folder even though enough space is available. \n",
       "Filesystem                      Size  Used Avail Use% Mounted on\n",
       "/dev/mapper/volgrp02-lv_oracle  197G   24G  163G  13% /oracle\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Unable to take any downtime, How can stop the Directory index full! error messages being received in /var/log/messages even thought the Directory is ok.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col4\" class=\"data row169 col4\" >['directory index full', 'device dm-49', 'ext4-fs warning', 'kernel']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow169_col5\" class=\"data row169 col5\" >['directory index full', 'device dm-49', 'ext4-fs warning', 'kernel', 'oracle jdbc connection protocol', 'recursive sql level 1ora-02002', 'audit trail file-', 'audit trailora-09925', 'user account', 'error', 'oracleprofile{host= gbwxorcrmp01a.gbcaydc.baml.com', 'connection', 'gbwxorcrmp01a.gbcaydc.baml.com-cmpempr1', 'unable']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row170\" class=\"row_heading level0 row170\" >170</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col0\" class=\"data row170 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col1\" class=\"data row170 col1\" >02326820</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col2\" class=\"data row170 col2\" >Are there limitations on the count of files in a single directory for ext4 ?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col3\" class=\"data row170 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "When we check the system log , find follow error messages :\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning: 3795 callbacks suppressed\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "Feb 26 11:49:51 c12 kernel: EXT4-fs warning (device dm-7): ext4_dx_add_entry:2016: Directory index full!\n",
       "\n",
       "What is the effect of the errors?\n",
       "\n",
       "When I use \"df -i\" command , I find that the inode not be exhausted , and the disk space not be exhausted too .\n",
       "\n",
       "There are 10~20 million files under the disk .</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col4\" class=\"data row170 col4\" >['single directory', 'files', 'count', 'ext4', 'limitations']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow170_col5\" class=\"data row170 col5\" >['single directory', 'files', 'count', 'ext4', 'limitations', 'directory index full', 'c12 kernel', 'ext4-fs warning', 'device dm-7', 'df -i', 'disk space', 'feb', 'inode', 'ext4_dx_add_entry:2016', 'follow error messages']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row171\" class=\"row_heading level0 row171\" >171</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col0\" class=\"data row171 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col1\" class=\"data row171 col1\" >02235835</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col2\" class=\"data row171 col2\" >EXT4 Directory index full!</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col3\" class=\"data row171 col3\" >Hello,\n",
       "\n",
       "I have a lot of warnings, related to \"ext4 directory index full\", in /var/log/messages on a Red Hat 7.3 machine running Oracle :\n",
       "\n",
       "Oct 19 15:05:16 gmasora50 kernel: EXT4-fs warning (device dm-32): ext4_dx_add_entry:2018: Directory index full!\n",
       "Oct 19 15:05:16 gmasora50 kernel: EXT4-fs warning (device dm-32): ext4_dx_add_entry:2018: Directory index full!\n",
       "Oct 19 15:05:16 gmasora50 kernel: EXT4-fs warning (device dm-32): ext4_dx_add_entry:2018: Directory index full!\n",
       "Oct 19 15:05:16 gmasora50 kernel: EXT4-fs warning (device dm-32): ext4_dx_add_entry:2018: Directory index full!\n",
       "Oct 19 15:05:16 gmasora50 kernel: EXT4-fs warning (device dm-32): ext4_dx_add_entry:2018: Directory index full!\n",
       "\n",
       "\n",
       "Because of those \"directory index full\", I get the following message from Oracle :\n",
       "\n",
       "Fri Oct 19 15:05:16 2018\n",
       "Non critical error ORA-48180 caught while writing to trace file \"/u01/app/oracle/diag/rdbms/dwhs/DWHS/trace/DWHS_p101_108752.trc\"\n",
       "Error message: Linux-x86_64 Error: 28: No space left on device\n",
       "Additional information: 1\n",
       "Writing to the above trace file is disabled for now on...\n",
       "\n",
       "\n",
       "As I see, disk space and inode usage are ok for partition mounted under /u01 so I don't understand why a I get these messages. Could you please help me with that ?\n",
       "\n",
       "[root@gmasora50 ~]# df -i\n",
       "Filesystem                  Inodes   IUsed     IFree IUse% Mounted on\n",
       "/dev/mapper/rhel-root      3276800   45413   3231387    2% /\n",
       "devtmpfs                 264171468    2918 264168550    1% /dev\n",
       "tmpfs                    264174279      86 264174193    1% /dev/shm\n",
       "tmpfs                    264174279    4343 264169936    1% /run\n",
       "tmpfs                    264174279      16 264174263    1% /sys/fs/cgroup\n",
       "/dev/sda1                    65536     347     65189    1% /boot\n",
       "/dev/mapper/vg_01-lv_u02  32768000     203  32767797    1% /u02\n",
       "/dev/mapper/rhel-u01      24666112 1400662  23265450    6% /u01\n",
       "gmasdwh03:/dwhdata        26214400  564864  25649536    3% /dwhdata\n",
       "tmpfs                    264174279       1 264174278    1% /run/user/506\n",
       "tmpfs                    264174279       1 264174278    1% /run/user/1001\n",
       "/dev/ram0                 39256064      16  39256048    1% /ramdisk0\n",
       "tmpfs                    264174279       1 264174278    1% /run/user/99\n",
       "tmpfs                    264174279       1 264174278    1% /run/user/0\n",
       "[root@gmasora50 ~]# df -h\n",
       "Filesystem                Size  Used Avail Use% Mounted on\n",
       "/dev/mapper/rhel-root      50G  8.5G   39G  19% /\n",
       "devtmpfs                 1008G     0 1008G   0% /dev\n",
       "tmpfs                    1008G  652M 1008G   1% /dev/shm\n",
       "tmpfs                    1008G   52M 1008G   1% /run\n",
       "tmpfs                    1008G     0 1008G   0% /sys/fs/cgroup\n",
       "/dev/sda1                 976M  201M  709M  23% /boot\n",
       "/dev/mapper/vg_01-lv_u02  492G   89G  382G  19% /u02\n",
       "/dev/mapper/rhel-u01       92G   70G   19G  80% /u01\n",
       "gmasdwh03:/dwhdata        394G  212G  163G  57% /dwhdata\n",
       "tmpfs                     202G     0  202G   0% /run/user/506\n",
       "tmpfs                     202G     0  202G   0% /run/user/1001\n",
       "/dev/ram0                 590G  476G  115G  81% /ramdisk0\n",
       "tmpfs                     202G     0  202G   0% /run/user/99\n",
       "tmpfs                     202G     0  202G   0% /run/user/0\n",
       "[root@gmasora50 ~]# cat /etc/redhat-release\n",
       "Red Hat Enterprise Linux Server release 7.3 (Maipo)\n",
       "\n",
       "Please find sosreport in attachement.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col4\" class=\"data row171 col4\" >['ext4 directory index full']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow171_col5\" class=\"data row171 col5\" >['ext4 directory index full', 'red hat enterprise linux server release', 'ext4 directory index full', 'directory index full', 'g', '%', 'above trace file', 'non critical error ora-48180', 'device additional information', 'gmasora50 kernel', 'ext4-fs warning']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row172\" class=\"row_heading level0 row172\" >172</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col0\" class=\"data row172 col0\" >29894</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col1\" class=\"data row172 col1\" >01926822</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col2\" class=\"data row172 col2\" >Server lrau1p18 was hung . Rebooted the manually .</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col3\" class=\"data row172 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Server lrau1p18 was hung . Rebooted the manually .\n",
       "Attached is the last screen shot when the server is in hung state . Kindly look into this .\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Frequently</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col4\" class=\"data row172 col4\" >['server lrau1p18']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow172_col5\" class=\"data row172 col5\" >['server lrau1p18', 'hung state', 'last screen', 'server lrau1p18', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row173\" class=\"row_heading level0 row173\" >173</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col0\" class=\"data row173 col0\" >3415331</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col1\" class=\"data row173 col1\" >02493305</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col2\" class=\"data row173 col2\" >nfs mount points and cifs mount points not available with kernel kernel-3.10.0-1062.1.2.el7.x86_64</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col3\" class=\"data row173 col3\" >¿Qué problema/comportamiento le está causando dificultades? ¿Qué espera ver?\n",
       "\n",
       "after upgrading to new kernel nfs mount points and cifs mount points were not available with kernel kernel-3.10.0-1062.1.2.el7.x86_64\n",
       "also the interfaces where changed to ethX, since the mac address were not persisten \n",
       "or the network manager change them.\n",
       "eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
       "        inet 10.52.24.3  netmask 255.255.255.0  broadcast 10.52.24.255\n",
       "        inet6 fe80::a1f1:761a:a696:d37f  prefixlen 64  scopeid 0x20<link>\n",
       "        ether 00:50:56:95:c5:54  txqueuelen 1000  (Ethernet)\n",
       "        RX packets 2400  bytes 389473 (380.3 KiB)\n",
       "        RX errors 0  dropped 0  overruns 0  frame 0\n",
       "        TX packets 1907  bytes 320014 (312.5 KiB)\n",
       "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
       "\n",
       "eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
       "        inet 172.17.5.1  netmask 255.255.255.128  broadcast 172.17.5.127\n",
       "        inet6 fe80::5d31:ae3a:b224:8332  prefixlen 64  scopeid 0x20<link>\n",
       "        ether 00:50:56:95:50:6c  txqueuelen 1000  (Ethernet)\n",
       "        RX packets 25083  bytes 8092215 (7.7 MiB)\n",
       "        RX errors 0  dropped 0  overruns 0  frame 0\n",
       "        TX packets 24383  bytes 12841766 (12.2 MiB)\n",
       "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
       "\n",
       "eth2: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n",
       "        inet 10.57.22.78  netmask 255.255.255.192  broadcast 10.57.22.127\n",
       "        inet6 fe80::9b1e:9606:38b1:3328  prefixlen 64  scopeid 0x20<link>\n",
       "        ether 00:50:56:95:5e:37  txqueuelen 1000  (Ethernet)\n",
       "        RX packets 737  bytes 93745 (91.5 KiB)\n",
       "        RX errors 0  dropped 0  overruns 0  frame 0\n",
       "        TX packets 909  bytes 185048 (180.7 KiB)\n",
       "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
       "\n",
       "lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n",
       "        inet 127.0.0.1  netmask 255.0.0.0\n",
       "        inet6 ::1  prefixlen 128  scopeid 0x10<host>\n",
       "        loop  txqueuelen 1  (Local Loopback)\n",
       "        RX packets 1644  bytes 273913 (267.4 KiB)\n",
       "        RX errors 0  dropped 0  overruns 0  frame 0\n",
       "        TX packets 1644  bytes 273913 (267.4 KiB)\n",
       "        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n",
       "\n",
       "¿En dónde se está presentando el comportamiento? ¿En qué entorno?\n",
       "\n",
       "Server: spmxgtwy\n",
       "Production high priority\n",
       "\n",
       "¿Cuándo ocurre este comportamiento? ¿Con frecuencia? ¿Repetidamente? ¿En momentos determinados?\n",
       "\n",
       "just after upgrading to kernel kernel-3.10.0-1062.1.2.el7.x86_64\n",
       "lost moun points\n",
       "10.52.22.1:/conciliacion  130G  101G   30G  78% /conciliacion\n",
       "10.52.22.1:/Finanzas       50G   25G   26G  50% /Finanzas\n",
       "//10.52.24.8/tarjetas     101G   68G   34G  68% /tarjetas\n",
       "//10.52.24.8/tarjetasrp   101G   68G   34G  68% /tarjetasrp\n",
       "//10.52.24.8/tarjetassb   101G   68G   34G  68% /tarjetassb\n",
       "//10.52.24.8/tarjetaspu   101G   68G   34G  68% /tarjetaspu</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col4\" class=\"data row173 col4\" >['cifs mount points', 'nfs mount points', 'kernel kernel-3.10.0', 'available']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow173_col5\" class=\"data row173 col5\" >['cifs mount points', 'nfs mount points', 'kernel kernel-3.10.0', 'available', 'new kernel nfs mount points', 'comportamiento le está causando dificultades', 'está presentando el comportamiento', 'cuándo ocurre este comportamiento', 'cifs mount points', 'qué espera ver', 'spmxgtwy production high priority', 'rx errors', 'tx errors', 'qué entorno']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row174\" class=\"row_heading level0 row174\" >174</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col0\" class=\"data row174 col0\" >3415331</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col1\" class=\"data row174 col1\" >02254090</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col2\" class=\"data row174 col2\" >After patching server went to Emergency maintenance mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col3\" class=\"data row174 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Post patching of server, it's going to Emergency maintenance mode.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "when run yum update, production environment.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Certain time\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "we will provide required information.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col4\" class=\"data row174 col4\" >['emergency maintenance mode', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow174_col5\" class=\"data row174 col5\" >['emergency maintenance mode', 'server', 'emergency maintenance mode', 'server', 'patching']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row175\" class=\"row_heading level0 row175\" >175</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col0\" class=\"data row175 col0\" >3415331</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col1\" class=\"data row175 col1\" >02510697</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col2\" class=\"data row175 col2\" >CIFS sec=ntlmv2 mounting issue</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col3\" class=\"data row175 col3\" >while mounting the CIFS share getting permission denied error\n",
       "\n",
       "wmadm@hoelaiswms42 wmadm $ mount  /opt/wm/wmfs/strif\n",
       "Password for xsise121a@//BKKEPF02.AP.XOM.COM/STRIFACC$:  *************************\n",
       "mount error(13): Permission denied\n",
       "Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) \n",
       "\n",
       "i found these messages in logs...\n",
       "[1518021.482857] CIFS VFS: Server HOERNS02.NA.XOM.COM has not responded in 120 seconds. Reconnecting...\n",
       "[1518103.517659] CIFS VFS: Free previous auth_key.response = ffff8e6b1dbbef00\n",
       "[1588149.179632] No dialect specified on mount. Default has changed to a more secure dialect, SMB2.1 or later (e.g. SMB3), from CIFS (SMB1). To use the less secure SMB1 dialect to access old servers which do not support SMB3 (or SMB2.1) specify vers=1.0 on mount.\n",
       "[1588150.115728] CIFS VFS: Send error in SessSetup = -13\n",
       "[1588150.117091] CIFS VFS: cifs_mount failed w/return code = -13</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col4\" class=\"data row175 col4\" >['cifs sec', 'issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow175_col5\" class=\"data row175 col5\" >['cifs sec', 'issue', 'cifs vfs', 'cifs share', 'secure smb1 dialect', 'cifs', 'mount error(13', 'secure dialect', 'vfs', 'mount', 'man mount.cifs', 'dialect']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row176\" class=\"row_heading level0 row176\" >176</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col0\" class=\"data row176 col0\" >3446331</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col1\" class=\"data row176 col1\" >02480468</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col2\" class=\"data row176 col2\" >CIFS bug (crash in is_size_safe_to_change - bug 1712197) is crashing our RHEL8 server on daily basis</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col3\" class=\"data row176 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Our RHE8 servers are running 4.18.0-80.11.2.el8_0 kernel and we are experiencing regular crashes on them due to following CIFS bug:\n",
       "\n",
       "https://bugzilla.redhat.com/show_bug.cgi?format=multiple&id=1712197\n",
       "\n",
       "It does not look like this bug is fixed in the latest available RHEL 8.0 kernel - could you please provide us with some timeline here? We really need this fix badly, our RHEL8 boxes are unusable at the moment (we rely on kerberized SMB/CIFS heavily)\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Every day or so</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col4\" class=\"data row176 col4\" >['cifs bug', 'rhel8 server', 'bug', 'daily basis', 'is_size_safe_to_change', 'crash']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow176_col5\" class=\"data row176 col5\" >['cifs bug', 'rhel8 server', 'bug', 'daily basis', 'is_size_safe_to_change', 'crash', 'cifs bug', 'latest available rhel', 'kernel', 'bug', 'rhel8 boxes', 'regular crashes', 'kerberized smb', 'cifs', 'unusable', 'moment']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row177\" class=\"row_heading level0 row177\" >177</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col0\" class=\"data row177 col0\" >3446331</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col1\" class=\"data row177 col1\" >02165480</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col2\" class=\"data row177 col2\" >[abrt] kernel: general protection fault: 0000 [#1] SMP</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col3\" class=\"data row177 col3\" >Description of problem:\n",
       "Nothing special was running\n",
       "\n",
       "Additional info:\n",
       "count:          1\n",
       "reason:         general protection fault: 0000 [#1] SMP \n",
       "package:        not belong to any package\n",
       "pkg_vendor:     Red Hat, Inc.\n",
       "reporter:       libreport-2.1.11.1\n",
       "\n",
       "\n",
       "\n",
       "Truncated backtrace:\n",
       "general protection fault: 0000 [#1] SMP \n",
       "Modules linked in: xt_CHECKSUM iptable_mangle ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT nf_reject_ipv4 tun bridge stp llc ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter devlink cmac nls_utf8 cifs ccm dns_resolver vmw_vsock_vmci_transport vsock sb_edac iosf_mbi crc32_pclmul ghash_clmulni_intel ppdev vmw_balloon aesni_intel lrw gf128mul glue_helper ablk_helper cryptd sg e1000 joydev pcspkr vmw_vmci i2c_piix4 shpchp nfit libnvdimm parport_pc parport auth_rpcgss sunrpc ip_tables xfs libcrc32c sr_mod cdrom ata_generic pata_acpi vmwgfx sd_mod crc_t10dif crct10dif_generic drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm drm crct10dif_pclmul ata_piix crct10dif_common\n",
       " crc32c_intel serio_raw libata vmw_pvscsi i2c_core floppy dm_mirror dm_region_hash dm_log dm_mod\n",
       "CPU: 3 PID: 2961 Comm: cp Kdump: loaded Not tainted 3.10.0-862.3.2.el7.x86_64 #1\n",
       "Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 04/05/2016\n",
       "task: ffff9034d4a8cf10 ti: ffff9034f36e4000 task.ti: ffff9034f36e4000\n",
       "RIP: 0010:[<ffffffffc08f4920>]  [<ffffffffc08f4920>] is_size_safe_to_change+0x60/0xd0 [cifs]\n",
       "RSP: 0018:ffff9034f36e79a0  EFLAGS: 00010202\n",
       "RAX: 0000000000028001 RBX: 0000000000000231 RCX: dead0000000000f0\n",
       "RDX: dead000000000100 RSI: 0000000000000231 RDI: ffff903887f36038\n",
       "RBP: ffff9034f36e79b8 R08: ffff9030b6b8ebd0 R09: ffff9034f36e7be0\n",
       "R10: 0000000083f18801 R11: ffff902eb5faf800 R12: ffff9030b6b8eb98\n",
       "R13: ffff903887f36038 R14: ffff9034f36e7a60 R15: ffff9030b6b8ec30\n",
       "FS:  00007fb3574d3840(0000) GS:ffff9038bfd80000(0000) knlGS:0000000000000000\n",
       "CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n",
       "CR2: 00007fee5d21b880 CR3: 000000026ba7c000 CR4: 00000000000607e0\n",
       "Call Trace:\n",
       " [<ffffffffc08f5a19>] cifs_fattr_to_inode+0x139/0x5b0 [cifs]\n",
       " [<ffffffffc08f6c9b>] cifs_get_inode_info+0x20b/0x970 [cifs]\n",
       " [<ffffffffc08eef3c>] cifs_open+0x38c/0x7f0 [cifs]\n",
       " [<ffffffffb96187da>] do_dentry_open+0x1aa/0x2e0\n",
       " [<ffffffffb96d0a72>] ? security_inode_permission+0x22/0x30\n",
       " [<ffffffffc08eebb0>] ? cifs_write_from_iter.isra.30+0x4c0/0x4c0 [cifs]\n",
       " [<ffffffffb96189aa>] vfs_open+0x5a/0xb0\n",
       " [<ffffffffb9626e08>] ? may_open+0x68/0x120\n",
       " [<ffffffffb962ae5d>] do_last+0x1ed/0x12c0\n",
       " [<ffffffffb962c007>] path_openat+0xd7/0x640\n",
       " [<ffffffffb962da72>] ? user_path_at_empty+0x72/0xc0\n",
       " [<ffffffffb962db9d>] do_filp_open+0x4d/0xb0\n",
       " [<ffffffffb963b017>] ? __alloc_fd+0x47/0x170\n",
       " [<ffffffffb9619ea7>] do_sys_open+0x137/0x240\n",
       " [<ffffffffb9b2076f>] ? system_call_after_swapgs+0xbc/0x160\n",
       " [<ffffffffb9619fce>] SyS_open+0x1e/0x20\n",
       " [<ffffffffb9b2082f>] system_call_fastpath+0x1c/0x21\n",
       " [<ffffffffb9b2077b>] ? system_call_after_swapgs+0xc8/0x160\n",
       "Code: 1e 22 f9 49 8b 44 24 38 4d 8d 44 24 38 49 39 c0 48 8d 48 f0 74 30 8b 80 88 00 00 00 83 c0 01 a8 02 74 16 eb 3e 66 0f 1f 44 00 00 <8b> 92 88 00 00 00 83 c2 01 83 e2 02 75 2a 48 8b 51 10 49 39 d0 \n",
       "RIP  [<ffffffffc08f4920>] is_size_safe_to_change+0x60/0xd0 [cifs]\n",
       " RSP <ffff9034f36e79a0>\n",
       "\n",
       "Other report identifiers:\n",
       "RHTSupport: TIME=2018-08-20-08:40:48 URL=https://access.redhat.com/rs/cases/02165274\n",
       "\n",
       "sosreport and other files were attached as 'vmcore-127.0.0.1-2018-08-17-05:55:09.tar.gz' to the case.\n",
       "For more details about elements collected by ABRT see:\n",
       "https://access.redhat.com/articles/2134281</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col4\" class=\"data row177 col4\" >['general protection fault', 'kernel', 'abrt', 'smp']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow177_col5\" class=\"data row177 col5\" >['general protection fault', 'kernel', 'abrt', 'smp', 'ipt_masquerade nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_reject nf_reject_ipv4 tun bridge stp llc ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter devlink cmac', 'inc. vmware virtual platform/440bx desktop reference platform', 'crc32c_intel serio_raw libata vmw_pvscsi i2c_core floppy dm_mirror', 'drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm drm', 'iosf_mbi crc32_pclmul ghash_clmulni_intel ppdev vmw_balloon aesni_intel', 'ata_generic pata_acpi vmwgfx sd_mod crc_t10dif', 'cifs ccm', 'parport auth_rpcgss sunrpc ip_tables', 'joydev pcspkr vmw_vmci i2c_piix4', 'cifs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row178\" class=\"row_heading level0 row178\" >178</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col0\" class=\"data row178 col0\" >3446331</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col1\" class=\"data row178 col1\" >02069191</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col2\" class=\"data row178 col2\" >[bz1757872][abrt] kernel: general protection fault: 0000 [#1] SMP</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col3\" class=\"data row178 col3\" >Description of problem:\n",
       "during compiling/linking process\n",
       "\n",
       "Additional info:\n",
       "count:          1\n",
       "reason:         general protection fault: 0000 [#1] SMP \n",
       "package:        not belong to any package\n",
       "pkg_vendor:     Red Hat, Inc.\n",
       "reporter:       libreport-2.1.11.1\n",
       "\n",
       "\n",
       "\n",
       "Truncated backtrace:\n",
       "general protection fault: 0000 [#1] SMP \n",
       "Modules linked in: fuse sctp_diag sctp dccp_diag dccp tcp_diag udp_diag inet_diag unix_diag af_packet_diag netlink_diag binfmt_misc xt_CHECKSUM iptable_mangle ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT nf_reject_ipv4 tun bridge stp llc ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter nls_utf8 cifs dns_resolver sb_edac edac_core coretemp iosf_mbi crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd ppdev vmw_balloon pcspkr joydev sg parport_pc parport shpchp i2c_piix4 vmw_vmci nfsd nfs_acl lockd grace auth_rpcgss sunrpc ip_tables xfs libcrc32c sr_mod cdrom ata_generic pata_acpi vmwgfx sd_mod crc_t10dif crct10dif_generic drm_kms_helper syscopyarea\n",
       " sysfillrect sysimgblt fb_sys_fops ttm ata_piix serio_raw crct10dif_pclmul drm crct10dif_common crc32c_intel libata vmxnet3 vmw_pvscsi i2c_core floppy dm_mirror dm_region_hash dm_log dm_mod\n",
       "CPU: 1 PID: 9526 Comm: ld Not tainted 3.10.0-693.17.1.el7.x86_64 #1\n",
       "Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 04/14/2014\n",
       "task: ffff88034b7b8000 ti: ffff88042e98c000 task.ti: ffff88042e98c000\n",
       "RIP: 0010:[<ffffffffc055dc10>]  [<ffffffffc055dc10>] is_size_safe_to_change+0x60/0xd0 [cifs]\n",
       "RSP: 0018:ffff88042e98f9b0  EFLAGS: 00010202\n",
       "RAX: 0000000000008001 RBX: 00000000171cffd0 RCX: dead0000000000f0\n",
       "RDX: dead000000000100 RSI: 00000000171cffd0 RDI: ffff88042a97e038\n",
       "RBP: ffff88042e98f9c8 R08: ffff88035595bd40 R09: ffff88042e98fbf0\n",
       "R10: ffff88043fc5b940 R11: ffff88042e501000 R12: ffff88035595bd08\n",
       "R13: ffff88042a97e038 R14: ffff88042e98fa70 R15: ffff88035595bda0\n",
       "FS:  00007fc552fe5740(0000) GS:ffff88043fc40000(0000) knlGS:0000000000000000\n",
       "CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b\n",
       "CR2: 000000002ae72468 CR3: 0000000371ece000 CR4: 00000000000407e0\n",
       "DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n",
       "DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\n",
       "Call Trace:\n",
       " [<ffffffffc055ecf9>] cifs_fattr_to_inode+0x139/0x5b0 [cifs]\n",
       " [<ffffffffc055ff6b>] cifs_get_inode_info+0x20b/0x990 [cifs]\n",
       " [<ffffffffc0581eab>] ? smb2_open_file+0x10b/0x250 [cifs]\n",
       " [<ffffffffc05582ac>] cifs_open+0x37c/0x7e0 [cifs]\n",
       " [<ffffffff816ab2f4>] ? __schedule+0x424/0x9b0\n",
       " [<ffffffff81200627>] do_dentry_open+0x1a7/0x2e0\n",
       " [<ffffffff812b43cc>] ? security_inode_permission+0x1c/0x30\n",
       " [<ffffffffc0557f30>] ? cifs_write_from_iter.isra.30+0x4a0/0x4a0 [cifs]\n",
       " [<ffffffff812007fa>] vfs_open+0x5a/0xb0\n",
       " [<ffffffff8120e4e8>] ? may_open+0x68/0x110\n",
       " [<ffffffff8121195d>] do_last+0x1ed/0x12c0\n",
       " [<ffffffff81212af2>] path_openat+0xc2/0x490\n",
       " [<ffffffff811f4f52>] ? __mem_cgroup_commit_charge+0xe2/0x2f0\n",
       " [<ffffffff8121508b>] do_filp_open+0x4b/0xb0\n",
       " [<ffffffff8122233a>] ? __alloc_fd+0x8a/0x130\n",
       " [<ffffffff81201bc3>] do_sys_open+0xf3/0x1f0\n",
       " [<ffffffff816b8945>] ? system_call_after_swapgs+0x172/0x214\n",
       " [<ffffffff81201cde>] SyS_open+0x1e/0x20\n",
       " [<ffffffff816b89fd>] system_call_fastpath+0x16/0x1b\n",
       " [<ffffffff816b889d>] ? system_call_after_swapgs+0xca/0x214\n",
       "Code: 03 15 c1 49 8b 44 24 38 4d 8d 44 24 38 49 39 c0 48 8d 48 f0 74 30 8b 80 88 00 00 00 83 c0 01 a8 02 74 16 eb 3e 66 0f 1f 44 00 00 <8b> 92 88 00 00 00 83 c2 01 83 e2 02 75 2a 48 8b 51 10 49 39 d0 \n",
       "RIP  [<ffffffffc055dc10>] is_size_safe_to_change+0x60/0xd0 [cifs]\n",
       " RSP <ffff88042e98f9b0>\n",
       "\n",
       "sosreport and other files were attached as 'vmcore-127.0.0.1-2018-02-08-09:51:36.tar.gz' to the case.\n",
       "For more details about elements collected by ABRT see:\n",
       "https://access.redhat.com/articles/2134281</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col4\" class=\"data row178 col4\" >['general protection fault', 'kernel', 'bz1757872][abrt', 'smp']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow178_col5\" class=\"data row178 col5\" >['general protection fault', 'kernel', 'bz1757872][abrt', 'smp', 'ipt_reject nf_reject_ipv4 tun bridge stp llc ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter', 'cifs dns_resolver sb_edac edac_core coretemp iosf_mbi crc32_pclmul', 'i2c_piix4 vmw_vmci nfsd nfs_acl lockd grace auth_rpcgss sunrpc ip_tables', 'inc. vmware virtual platform/440bx desktop reference platform', 'tcp_diag udp_diag inet_diag unix_diag af_packet_diag netlink_diag binfmt_misc xt_checksum', 'iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack', '_ _', 'ata_generic pata_acpi vmwgfx sd_mod crc_t10dif', 'sysfillrect sysimgblt fb_sys_fops ttm ata_piix', 'ppdev vmw_balloon pcspkr joydev sg']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row179\" class=\"row_heading level0 row179\" >179</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col0\" class=\"data row179 col0\" >3446331</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col1\" class=\"data row179 col1\" >02444742</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col2\" class=\"data row179 col2\" >Kernel panic occurs very night - possibly related to CIFS operation</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col3\" class=\"data row179 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We are experiencing the following kernel panic, it seems to happen regularly every night:\n",
       "\n",
       "[84188.117679] general protection fault: 0000 [#1] SMP\n",
       "[84188.117707] Modules linked in: tcp_lp fuse macsec binfmt_misc tcp_diag udp_diag inet_diag unix_diag af_packet_diag netlink_diag xt_CHECKSUM iptable_mangle ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT nf_reject_ipv4 tun bridge stp llc devlink ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter cmac nls_utf8 cifs ccm dns_resolver bonding rfkill dell_wmi_descriptor dcdbas iTCO_wdt iTCO_vendor_support sb_edac intel_powerclamp coretemp intel_rapl iosf_mbi kvm_intel kvm mxm_wmi irqbypass crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd ipmi_ssif joydev pcspkr mei_me lpc_ich sg mei ipmi_si ipmi_devintf ipmi_msghandler wmi acpi_power_meter tpm_crb auth_rpcgss\n",
       "[84188.118031]  sunrpc ip_tables xfs libcrc32c sd_mod crc_t10dif crct10dif_generic mgag200 i2c_algo_bit drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm crct10dif_pclmul crct10dif_common crc32c_intel drm drm_panel_orientation_quirks ahci ixgbe libahci megaraid_sas libata mdio ptp pps_core dca dm_mirror dm_region_hash dm_log dm_mod\n",
       "[84188.118168] CPU: 11 PID: 172461 Comm: MATLAB Kdump: loaded Not tainted 3.10.0-957.10.1.el7.x86_64 #1\n",
       "[84188.118200] Hardware name: Dell Inc. PowerEdge M630/0R10KJ, BIOS 2.6.0 10/27/2017\n",
       "[84188.118226] task: ffff9a1588331040 ti: ffff99fa292bc000 task.ti: ffff99fa292bc000\n",
       "[84188.118251] RIP: 0010:[<ffffffffc07b3e20>]  [<ffffffffc07b3e20>] is_size_safe_to_change+0x60/0xd0 [cifs]\n",
       "[84188.118305] RSP: 0018:ffff99fa292bfcc8  EFLAGS: 00010206\n",
       "[84188.118324] RAX: 0000000000008001 RBX: 000000000000dbec RCX: dead0000000000f0\n",
       "[84188.118348] RDX: dead000000000100 RSI: 000000000000dbec RDI: ffff9a68ef7d8838\n",
       "[84188.118372] RBP: ffff99fa292bfce0 R08: ffff9a66094128e8 R09: 0000000000000001\n",
       "[84188.118396] R10: 0000000000000000 R11: ffff9a6609409140 R12: ffff9a66094128b0\n",
       "[84188.119487] R13: ffff9a68ef7d8838 R14: ffff99fa292bfd88 R15: ffff9a6609412948\n",
       "[84188.120418] FS:  00007fb353723700(0000) GS:ffff9a68fdf40000(0000) knlGS:0000000000000000\n",
       "[84188.121339] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n",
       "[84188.122256] CR2: 00007f40fcfb29c0 CR3: 0000004b873a8000 CR4: 00000000003607e0\n",
       "[84188.123166] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n",
       "[84188.124052] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n",
       "[84188.124951] Call Trace:\n",
       "[84188.125844]  [<ffffffffc07b4f19>] cifs_fattr_to_inode+0x139/0x5b0 [cifs]\n",
       "[84188.126756]  [<ffffffffc07c415d>] cifs_filldir+0x54d/0x600 [cifs]\n",
       "[84188.127665]  [<ffffffff92a56c70>] ? iterate_dir+0x130/0x130\n",
       "[84188.128587]  [<ffffffffc07c478f>] cifs_readdir+0x57f/0xc60 [cifs]\n",
       "[84188.129509]  [<ffffffff92a56c70>] ? iterate_dir+0x130/0x130\n",
       "[84188.130439]  [<ffffffff92a56bd7>] iterate_dir+0x97/0x130\n",
       "[84188.131361]  [<ffffffff92a570f2>] SyS_getdents+0xa2/0x130\n",
       "[84188.132277]  [<ffffffff92a56c70>] ? iterate_dir+0x130/0x130\n",
       "[84188.133201]  [<ffffffff92f75ddb>] system_call_fastpath+0x22/0x27\n",
       "[84188.134129] Code: 76 7b d2 49 8b 44 24 38 4d 8d 44 24 38 49 39 c0 48 8d 48 f0 74 30 8b 80 88 00 00 00 83 c0 01 a8 02 74 16 eb 3e 66 0f 1f 44 00 00 <8b> 92 88 00 00 00 83 c2 01 83 e2 02 75 2a 48 8b 51 10 49 39 d0\n",
       "[84188.136161] RIP  [<ffffffffc07b3e20>] is_size_safe_to_change+0x60/0xd0 [cifs]\n",
       "[84188.137171]  RSP <ffff99fa292bfcc8>\n",
       "\n",
       "I am attaching sosreport and crash dump.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col4\" class=\"data row179 col4\" >['cifs operation', 'kernel panic', 'night']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow179_col5\" class=\"data row179 col5\" >['cifs operation', 'kernel panic', 'night', 'ipt_masquerade nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_reject nf_reject_ipv4 tun bridge stp llc devlink ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter cmac', 'cifs ccm dns_resolver bonding rfkill dell_wmi_descriptor', 'i2c_algo_bit drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm', 'megaraid_sas libata mdio ptp pps_core dca dm_mirror', 'macsec binfmt_misc tcp_diag udp_diag inet_diag unix_diag af_packet_diag', 'coretemp intel_rapl iosf_mbi kvm_intel kvm mxm_wmi', 'mei_me lpc_ich sg mei ipmi_si ipmi_devintf', 'crc32c_intel drm drm_panel_orientation_quirks ahci ixgbe', 'dcdbas itco_wdt itco_vendor_support sb_edac intel_powerclamp', 'dell inc. poweredge m630/0r10kj']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row180\" class=\"row_heading level0 row180\" >180</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col0\" class=\"data row180 col0\" >3991451</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col1\" class=\"data row180 col1\" >02445634</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col2\" class=\"data row180 col2\" >cifs mount error</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col3\" class=\"data row180 col3\" >Was bzw. welches Verhalten bereitet Ihnen Probleme? Was ist dabei unerwartet?\n",
       "\n",
       "Update from RHEL 7.6 to RHEL 7.7 leads to an mount error(13): Permission denied when mounting a smb share.\n",
       "\n",
       "mount -t cifs //adzh-srlp-fs01.intern.cube.ch/tarbase /mnt/tarbase -o username=transferdmz,ro,file_mode=0777,dir_mode=0777,password=XXXXXXXX --verbose\n",
       "mount.cifs kernel mount options: ip=172.21.17.36,unc=\\\\adzh-srlp-fs01.intern.cube.ch\\tarbase,file_mode=0777,dir_mode=0777,user=transferdmz,pass=********\n",
       "mount error(13): Permission denied\n",
       "Refer to the mount.cifs(8) manual page (e.g. man mount.cifs)\n",
       "\n",
       "Booting the former kernel 3.10.0-957.27.2.el7.x86_64 solves the issue. Same mount also works on RHEL 8.\n",
       "\n",
       "Our /etc/fstab\n",
       "//adzh-srlp-fs01.intern.cube.ch/tarbase\t/mnt/tarbase\tcifs\tcredentials=/root/.smbcredentials,ro,file_mode=0777,dir_mode=0777\t0\t0\n",
       "\n",
       "cat .smbcredentials \n",
       "username=transferdmz\n",
       "password=xxxxxxx\n",
       "\n",
       "Wo tritt dieses Verhalten auf? In welcher Umgebung?\n",
       "\n",
       "Production\n",
       "\n",
       "Wann tritt dieses Verhalten auf? Häufig? Wiederholt? Zu bestimmten Zeiten?\n",
       "\n",
       "Every time when try to mount a smb share.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col4\" class=\"data row180 col4\" >['cifs mount error']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow180_col5\" class=\"data row180 col5\" >['cifs mount error', 'wann tritt dieses verhalten auf', 'mount.cifs kernel mount options', 'dieses verhalten auf', 'verhalten bereitet ihnen probleme', 'mount -t cifs', 'mount error(13', 'same mount', 'transferdmz password', 'ist dabei unerwartet', 'man mount.cifs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row181\" class=\"row_heading level0 row181\" >181</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col0\" class=\"data row181 col0\" >3991451</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col1\" class=\"data row181 col1\" >02232626</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col2\" class=\"data row181 col2\" >[BZ 1710421] CIFS mounts stopped working after kernel update</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col3\" class=\"data row181 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "We have a CIFS multiuser mount set up on 4 dev servers, and 7 prod servers.  The servers are RHEL 7, they all mount the same share from a CIFS server.  This has been working fins since January.  However when we updated the kernel and rebooted our dev servers, the CIFS mount stopped working.  The initial mount as root works ok, and root can see files.  But when users try to view files in the mount, they get permission denied.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "This worked until upgrading kernel.\n",
       "\n",
       "  Previous kernel: \t3.10.0-693.17.1.el7.x86_64  \n",
       "  Current kernel:\t        3.10.0-862.14.4.el7.x86_64\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "It always fails.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Our dev servers are where our customers develop and update web sites, so this is really considered production.  Also we are going to start updating kernel and rebooting prod servers this weekend.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col4\" class=\"data row181 col4\" >['kernel update', 'cifs mounts', 'bz']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow181_col5\" class=\"data row181 col5\" >['kernel update', 'cifs mounts', 'bz', 'cifs multiuser mount', 'cifs mount', 'dev servers', 'initial mount', 'cifs server', 'prod servers', 'mount', 'servers', 'same share', 'root']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row182\" class=\"row_heading level0 row182\" >182</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col0\" class=\"data row182 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col1\" class=\"data row182 col1\" >02444918</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col2\" class=\"data row182 col2\" >Once of the filesystem is utilizing heavily</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col3\" class=\"data row182 col3\" >Hello team,\n",
       "\n",
       "Once of the file system /opt/zimbra has high utilization. The actual size of the filesystem utilization is only 23GB where as in df -g output we see that its is 233GB utilized. This is hampering the mail service. Kindly help us in this regards on prioroty.\n",
       "\n",
       "Regards,\n",
       "ServerTeam</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col4\" class=\"data row182 col4\" >['filesystem']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow182_col5\" class=\"data row182 col5\" >['filesystem', 'high utilization', 'filesystem utilization', 'df -g output', 'actual size', 'gb', 'file system', 'mail service', 'regards', 'zimbra', 'prioroty']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row183\" class=\"row_heading level0 row183\" >183</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col0\" class=\"data row183 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col1\" class=\"data row183 col1\" >02047491</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col2\" class=\"data row183 col2\" >server rebooted</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col3\" class=\"data row183 col3\" >Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "EXT3-fs: checktime reached, running e2fsck is recommended\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "EXT3-fs: checktime reached, running e2fsck is recommended</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col4\" class=\"data row183 col4\" >['server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow183_col5\" class=\"data row183 col5\" >['server', 'e2fsck', 'checktime', 'ext3-fs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row184\" class=\"row_heading level0 row184\" >184</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col0\" class=\"data row184 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col1\" class=\"data row184 col1\" >02088794</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col2\" class=\"data row184 col2\" >Both servers unable to login</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col3\" class=\"data row184 col3\" >Hi Team,\n",
       "\n",
       "Below servers unable to login. Checked through jump server and got the below output.\n",
       "\n",
       "gpd-973-7ab1, gpd-a9e-ff03 – fs read only mode\n",
       "\n",
       "Please kindly investigate further and help to resolve this issue ASAP.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col4\" class=\"data row184 col4\" >['servers unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow184_col5\" class=\"data row184 col5\" >['servers unable', 'jump server', 'below output', 'servers unable', 'gpd-973', 'a9e', 'ff03', 'mode', 'team', 'issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row185\" class=\"row_heading level0 row185\" >185</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col0\" class=\"data row185 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col1\" class=\"data row185 col1\" >02340161</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col2\" class=\"data row185 col2\" >a0310pvasdb01: Filesystem Errors in Logs and High Swap Usage</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col3\" class=\"data row185 col3\" >We are running a 2 node active-failover cluster on RHEL 6.5 running Oracle Databases. We are facing an issue where the swap usage gets very high. We changed the value of vm.swappiness to 20. After which the swap usage got better, but after a few days again swap usage began to increase again. Furthermore we observed filesystem errors in the server logs.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col4\" class=\"data row185 col4\" >['high swap usage', 'filesystem errors', 'logs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow185_col5\" class=\"data row185 col5\" >['high swap usage', 'filesystem errors', 'logs', 'swap usage', 'oracle databases', 'failover cluster', 'usage', 'filesystem errors', 'node active', 'few days', 'server logs', 'rhel', 'issue']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row186\" class=\"row_heading level0 row186\" >186</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col0\" class=\"data row186 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col1\" class=\"data row186 col1\" >01757039</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col2\" class=\"data row186 col2\" >Performance issue - Hardware error in dmesg</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col3\" class=\"data row186 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[Hardware Error]: Machine check events logged\n",
       "[root@pruswipprodb1 /]#\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "n/a\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "need to analysis , couldn't see any hardware failure alerts in HP ILO.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col4\" class=\"data row186 col4\" >['hardware error', 'performance issue', 'dmesg']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow186_col5\" class=\"data row186 col5\" >['hardware error', 'performance issue', 'dmesg', 'machine check events', 'hardware error', 'root@pruswipprodb1']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row187\" class=\"row_heading level0 row187\" >187</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col0\" class=\"data row187 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col1\" class=\"data row187 col1\" >01705652</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col2\" class=\"data row187 col2\" >Strange Errors in message.log.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col3\" class=\"data row187 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "I am seeing filesystem mount and umount messages in message.log from 18th Sep log. What could be the reason behind that.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "a 3rd party monitoring clients service stopped and started after that.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "hasn't happened before\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "Sep 18 16:24:53 IITMDBSPSV01 avahi-daemon[8422]: Withdrawing address record for 10.10.192.129 on bond0.\n",
       "Sep 18 16:24:53 IITMDBSPSV01 GblResRM[5494]: (Recorded using libct_ffdc.a cv 2):::Error ID: :::Reference ID:  :::Template ID: 0:::De\n",
       "tails File:  :::Location: RSCT,ServiceIP.C,1.70,2128                    :::GBLRESRM_IPOFFLINE IBM.ServiceIP removed address. IBM.Ser\n",
       "viceIP 10.10.192.129 \n",
       "Sep 18 16:24:53 IITMDBSPSV01 avahi-daemon[8422]: Leaving mDNS multicast group on interface bond0.IPv4 with address 10.10.192.129.\n",
       "Sep 18 16:24:53 IITMDBSPSV01 avahi-daemon[8422]: Joining mDNS multicast group on interface bond0.IPv4 with address 10.10.192.121.\n",
       "Sep 18 16:24:53 IITMDBSPSV01 kernel: kjournald starting.  Commit interval 5 seconds\n",
       "Sep 18 16:24:53 IITMDBSPSV01 kernel: EXT3-fs warning: mounting fs with errors, running e2fsck is recommended\n",
       "Sep 18 16:24:53 IITMDBSPSV01 kernel: EXT3 FS on dm-0, internal journal\n",
       "Sep 18 16:24:53 IITMDBSPSV01 kernel: EXT3-fs: mounted filesystem with ordered data mode.\n",
       "Sep 18 16:24:53 IITMDBSPSV01 avahi-daemon[8422]: Registering new address record for 10.10.192.129 on bond0.\n",
       "Sep 18 16:24:53 IITMDBSPSV01 avahi-daemon[8422]: Withdrawing address record for 10.10.192.129 on bond0.\n",
       "Sep 18 16:24:53 IITMDBSPSV01 avahi-daemon[8422]: Registering new address record for 10.10.192.129 on bond0.\n",
       "Sep 18 16:24:53 IITMDBSPSV01 GblResRM[5494]: (Recorded using libct_ffdc.a cv 2):::Error ID: :::Reference ID:  :::Template ID: 0:::De\n",
       "tails File:  :::Location: RSCT,ServiceIP.C,1.70,1991                    :::GBLRESRM_IPONLINE IBM.ServiceIP assigned address to devic\n",
       "e. IBM.ServiceIP 10.10.192.129 bond0:0 \n",
       "Sep 18 16:25:00 IITMDBSPSV01 /usr/sbin/rsct/sapolicies/db2/db2_monitor.ksh[9533]: Returning 2 (db2inst1, 0)\n",
       "Sep 18 16:25:00 IITMDBSPSV01 itm6ctrl_spagent:[9547]: Stop command issued\n",
       "Sep 18 16:25:01 IITMDBSPSV01 itm6ctrl_tdwproxy:[9544]: Stop command issued\n",
       "Sep 18 16:25:01 IITMDBSPSV01 db2_start.ksh[9908]: Entered /usr/sbin/rsct/sapolicies/db2/db2_start.ksh, db2inst1, 0\n",
       "Sep 18 16:25:01 IITMDBSPSV01 /usr/sbin/rsct/sapolicies/db2/db2_monitor.ksh[9965]: Returning 2 (db2inst1, 0)\n",
       "Sep 18 16:25:02 IITMDBSPSV01 db2_start.ksh[10293]: About to call db2gcf -u -p 0, db2inst1</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col4\" class=\"data row187 col4\" >['strange errors', 'message.log']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow187_col5\" class=\"data row187 col5\" >['strange errors', 'message.log', '18th sep log', 'umount messages', 'filesystem mount', 'message.log', 'reason']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row188\" class=\"row_heading level0 row188\" >188</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col0\" class=\"data row188 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col1\" class=\"data row188 col1\" >02415747</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col2\" class=\"data row188 col2\" >Possible currupt file system need to run fsck</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col3\" class=\"data row188 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Netbackup can not bring the mount point online\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Netbackup\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "High, no backups</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col4\" class=\"data row188 col4\" >['possible currupt file system', 'fsck']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow188_col5\" class=\"data row188 col5\" >['possible currupt file system', 'fsck', 'mount point', 'netbackup']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row189\" class=\"row_heading level0 row189\" >189</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col0\" class=\"data row189 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col1\" class=\"data row189 col1\" >01838579</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col2\" class=\"data row189 col2\" >issue with root file system</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col3\" class=\"data row189 col3\" >The machine went to maintenance mode and i have removed it from maintenance mode and mounted it to rw mode and commented the root and restarted it and the server is back up and running.\n",
       "Could you please analyse the sos report and let us know what happened.\n",
       "As a temporary solution we have make the server up,since it is production impacting</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col4\" class=\"data row189 col4\" >['root file system', 'issue']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow189_col5\" class=\"data row189 col5\" >['root file system', 'issue', 'maintenance mode', 'sos report', 'temporary solution', 'server', 'rw', 'root', 'machine', 'production']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row190\" class=\"row_heading level0 row190\" >190</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col0\" class=\"data row190 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col1\" class=\"data row190 col1\" >02068652</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col2\" class=\"data row190 col2\" >ログ調査依頼</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col3\" class=\"data row190 col3\" >問題となっている不具合や動作は何ですか? 期待される動作はどのようなものですか?\n",
       "\n",
       "以下のログについて意味、原因、対応要否、対応方法をご教授ください。\n",
       "\n",
       "kernel: kern.warning EXT4-fs (dm-25): warning: maximal mount count reached, running e2fsck is recommended\n",
       "kernel: kern.info EXT4-fs (dm-25): mounted filesystem with ordered data mode. Opts: \n",
       "kernel: kern.warning EXT4-fs (dm-27): warning: maximal mount count reached, running e2fsck is recommended\n",
       "kernel: kern.info EXT4-fs (dm-27): mounted filesystem with ordered data mode. Opts: \n",
       "kernel: kern.warning EXT4-fs (dm-28): warning: maximal mount count reached, running e2fsck is recommended\n",
       "kernel: kern.info EXT4-fs (dm-28): mounted filesystem with ordered data mode. Opts: \n",
       "kernel: kern.warning EXT4-fs (dm-29): warning: maximal mount count reached, running e2fsck is recommended\n",
       "kernel: kern.info EXT4-fs (dm-29): mounted filesystem with ordered data mode. Opts: \n",
       "kernel: kern.warning EXT4-fs (dm-30): warning: maximal mount count reached, running e2fsck is recommended\n",
       "kernel: kern.info EXT4-fs (dm-30): mounted filesystem with ordered data mode. Opts: \n",
       "kernel: kern.warning EXT4-fs (dm-23): warning: maximal mount count reached, running e2fsck is recommended\n",
       "kernel: kern.info EXT4-fs (dm-23): mounted filesystem with ordered data mode. Opts: \n",
       "kernel: kern.warning EXT4-fs (dm-24): warning: maximal mount count reached, running e2fsck is recommended\n",
       "kernel: kern.info EXT4-fs (dm-24): mounted filesystem with ordered data mode. Opts: \n",
       "\n",
       "また、(dm-xx)の値は何を区別、意味しているのでしょうか。\n",
       "\n",
       "以上、宜しくお願いいたします。</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col4\" class=\"data row190 col4\" >['ログ調査依頼']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow190_col5\" class=\"data row190 col5\" >['ログ調査依頼', 'maximal mount count', 'kern.warning ext4-fs', 'kern.info ext4-fs', 'data mode', 'ext4-fs', 'kernel', 'filesystem', 'opts', 'e2fsck', 'warning']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row191\" class=\"row_heading level0 row191\" >191</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col0\" class=\"data row191 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col1\" class=\"data row191 col1\" >02420863</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col2\" class=\"data row191 col2\" >NFS mounts cannot mount on boot, dropping host into rescue mode</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col3\" class=\"data row191 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "NFS fails to mount on boot, but after up, mounts fine.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Prod\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Every time reboot\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "No impact at this time.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col4\" class=\"data row191 col4\" >['rescue mode', 'nfs mounts', 'host', 'boot']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow191_col5\" class=\"data row191 col5\" >['rescue mode', 'nfs mounts', 'host', 'boot', 'mounts fine', 'boot', 'nfs']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row192\" class=\"row_heading level0 row192\" >192</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col0\" class=\"data row192 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col1\" class=\"data row192 col1\" >02250747</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col2\" class=\"data row192 col2\" >在menssage有报错， kernel: EXT4-fs (dm-8): warning: mounting fs with errors, running e2fsck is recommended</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col3\" class=\"data row192 col3\" >在menssage有报错，麻烦帮忙看一下，请帮忙看一下是否影响系统</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col4\" class=\"data row192 col4\" >['dm-8', 'ext4-fs', 'kernel', '在menssage有报错', 'errors', 'e2fsck']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow192_col5\" class=\"data row192 col5\" >['dm-8', 'ext4-fs', 'kernel', '在menssage有报错', 'errors', 'e2fsck']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row193\" class=\"row_heading level0 row193\" >193</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col0\" class=\"data row193 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col1\" class=\"data row193 col1\" >02475104</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col2\" class=\"data row193 col2\" >Alex01dc01(Virtual Machine Disk Unavailable)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col3\" class=\"data row193 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hi,\n",
       "\n",
       "We have an Openstack cluster (alex01dc01) deployed using undercloud node, our cluster contains:\n",
       "- Director Node.\n",
       "- 3 Controllers Nodes.\n",
       "- 6 Ceph Nodes.\n",
       "- 22 OVS-DPDK Compute Nodes.\n",
       "- 4 OVSbld Compute Nodes.\n",
       "- 2 OVSrack Compute Nodes.\n",
       "\n",
       "\n",
       "The problem >>our network elements triggered alarms (RU Disk unavailable & VM Disk unavailable) which cause traffic degredation .... after initial analysis we got that there is error in file system which changed into read-only so the VMs can't be mounted\n",
       "\n",
       "need your support to check this as these alarms repeated and affect the traffic\n",
       "\n",
       "Kindly find attached the file system is error and change to read-only, so VM can't be mounted.\n",
       "\n",
       "Please bear in mind that this is a Teleco-cloud Openstack cluster and carry a live Teleco-cloud traffic, so when RU reset that is impact the service.\n",
       "\n",
       "Kind regards,\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Live environment.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Repeatedly\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "degradation and missing KPIs.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col4\" class=\"data row193 col4\" >['alex01dc01(virtual machine disk unavailable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow193_col5\" class=\"data row193 col5\" >['alex01dc01(virtual machine disk unavailable', 'ovsbld compute nodes', 'ru disk unavailable', 'dpdk compute nodes', 'ovsrack compute nodes', 'vm disk unavailable', 'cloud openstack cluster', 'cloud traffic', 'ceph nodes', 'controllers nodes', 'openstack cluster']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row194\" class=\"row_heading level0 row194\" >194</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col0\" class=\"data row194 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col1\" class=\"data row194 col1\" >02349010</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col2\" class=\"data row194 col2\" >Unable to extend the fileystem</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col3\" class=\"data row194 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hi \n",
       "we have tried to extend the filesystem online , but the filesystem extension was stuck and hung,  hence rebooted the cluster , and tried to manually run the  resize2fs  post reboot , but system was asking for fsck and the fsck did not continue for long and process died\n",
       "\n",
       "Now we have mounted the filesystem back , however the added space is not shown in df -h output\n",
       "Could you please let us know how to extend this FS ? can it be done online now ?\n",
       "why the lvextend -r hot hung , before running we have scanned this disk on both the system and it was detected the disk\n",
       "\n",
       "during FS extention\n",
       "[root@scscimapp01 ~]# lvextend -r -l +12799 /dev/vgdata/lvol01\n",
       "  Extending logical volume lvol01 to 599.98 GiB\n",
       "  Logical volume lvol01 successfully resized\n",
       "resize2fs 1.41.12 (17-May-2010)\n",
       "Filesystem at /dev/mapper/vgdata-lvol01 is mounted on /TDM_APPS; on-line resizing required\n",
       "old desc_blocks = 35, new_desc_blocks = 38\n",
       "Performing an on-line resize of /dev/mapper/vgdata-lvol01 to 157281280 (4k) blocks.\n",
       "\n",
       "Post reboot resize2fs tried , but fsck struck for long time\n",
       "\n",
       "[root@scscimapp01 ~]# resize2fs /dev/vgdata/lvol01\n",
       "resize2fs 1.41.12 (17-May-2010)\n",
       "Please run 'e2fsck -f /dev/vgdata/lvol01' first.\n",
       "\n",
       "[root@scscimapp01 ~]#\n",
       "[root@scscimapp01 ~]# e2fsck -f /dev/vgdata/lvol01\n",
       "e2fsck 1.41.12 (17-May-2010)\n",
       "/dev/vgdata/lvol01: recovering journal\n",
       "Clearing orphaned inode 21962760 (uid=27, gid=27, mode=0100600, size=0)\n",
       "Clearing orphaned inode 21962758 (uid=27, gid=27, mode=0100600, size=143625)\n",
       "Clearing orphaned inode 21962756 (uid=27, gid=27, mode=0100600, size=0)\n",
       "Clearing orphaned inode 21962755 (uid=27, gid=27, mode=0100600, size=0)\n",
       "Clearing orphaned inode 21962754 (uid=27, gid=27, mode=0100600, size=0)\n",
       "Pass 1: Checking inodes, blocks, and sizes\n",
       "\n",
       "[root@scscimapp01 ~]# df -h /TDM_APPS\n",
       "Filesystem            Size  Used Avail Use% Mounted on\n",
       "/dev/mapper/vgdata-lvol01\n",
       "                      548G  458G   63G  88% /TDM_APPS\n",
       "[root@scscimapp01 ~]# lvdisplay /dev/vgdata/lvol01\n",
       "  --- Logical volume ---\n",
       "  LV Path                /dev/vgdata/lvol01\n",
       "  LV Name                lvol01\n",
       "  VG Name                vgdata\n",
       "  LV UUID                BZsoRb-ujqO-sPeZ-IVIc-07RP-qs6p-cPvkQB\n",
       "  LV Write Access        read/write\n",
       "  LV Creation host, time ,\n",
       "  LV Status              available\n",
       "  # open                 1\n",
       "  LV Size                599.98 GiB\n",
       "  Current LE             153595\n",
       "  Segments               5\n",
       "  Allocation             inherit\n",
       "  Read ahead sectors     auto\n",
       "  - currently set to     256\n",
       "  Block device           253:6\n",
       "\n",
       "[root@scscimapp01 ~]#</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col4\" class=\"data row194 col4\" >['fileystem', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow194_col5\" class=\"data row194 col5\" >['fileystem', 'unable', 'lvol01 resize2fs 1.41.12', 'logical volume lvol01', 'lvol01 e2fsck', 'lvol01', 'lv write access', 'lv creation host', 'lv size', 'lv name', 'lv status', 'vgdata']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row195\" class=\"row_heading level0 row195\" >195</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col0\" class=\"data row195 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col1\" class=\"data row195 col1\" >01932366</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col2\" class=\"data row195 col2\" >ext4文件系统报错</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col3\" class=\"data row195 col3\" >\"您遇到了什么问题？您所期望获得的结果是什么？\"\n",
       "\n",
       "EXT4-fs (dm-2): warning: mounting fs with errors, running e2fsck\n",
       "is recommended。这个报错该如何分析，怎样解决?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col4\" class=\"data row195 col4\" >['ext4文件系统报错']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow195_col5\" class=\"data row195 col5\" >['ext4文件系统报错', '您遇到了什么问题？您所期望获得的结果是什么 ？', 'dm-2', 'ext4-fs', 'errors', 'e2fsck']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row196\" class=\"row_heading level0 row196\" >196</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col0\" class=\"data row196 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col1\" class=\"data row196 col1\" >02451641</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col2\" class=\"data row196 col2\" >Server was rebooted on 03rd August 2019 and stuck on grub.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col3\" class=\"data row196 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Server was rebooted on 3rd August 2019 and stuck in the grub.\n",
       "RCA need for stuck in grub.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "This is virtual  server.\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "3rd August 2019</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col4\" class=\"data row196 col4\" >['03rd august', 'grub', 'server']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow196_col5\" class=\"data row196 col5\" >['03rd august', 'grub', 'server', '3rd august', 'rca', 'grub', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row197\" class=\"row_heading level0 row197\" >197</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col0\" class=\"data row197 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col1\" class=\"data row197 col1\" >01887281</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col2\" class=\"data row197 col2\" >Multiple filesystem errors</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col3\" class=\"data row197 col3\" >dmesg output.\n",
       "\n",
       "XT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=16934756, block=67731574\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13475711, block=53871095\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13667293, block=54657213\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13459455, block=53805567\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15885426, block=63537223\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13676904, block=54690070\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13467530, block=53838328\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15885880, block=63537251\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15895526, block=63570110\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=18895414, block=75563299\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13472043, block=53870866\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15899390, block=63570351\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=18895349, block=75563295\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=18895413, block=75563299\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=17720431, block=70877254\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15886491, block=63537289\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=17720737, block=70877274\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13676905, block=54690070\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13655837, block=54591985\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15894285, block=63570032\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15744235, block=62947790\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=17728464, block=70910012\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13675792, block=54690000\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15887216, block=63537334\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13466772, block=53838281\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13672402, block=54657533\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13468853, block=53870667\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15890720, block=63537553\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13460358, block=53837880\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13487825, block=53936365\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13467304, block=53838314\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13468262, block=53870630\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=17733702, block=70910340\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13654662, block=54591912\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=13489803, block=53936488\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=16144155, block=64553329\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=17076792, block=68288803\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=15222430, block=60883049\n",
       "end_request: I/O error, dev dm-0, sector 443893032\n",
       "end_request: I/O error, dev dm-0, sector 443893032\n",
       "end_request: I/O error, dev dm-0, sector 436289856\n",
       "end_request: I/O error, dev dm-0, sector 436290880\n",
       "end_request: I/O error, dev dm-0, sector 436289856\n",
       "end_request: I/O error, dev dm-0, sector 432023160\n",
       "end_request: I/O error, dev dm-0, sector 432023160\n",
       "end_request: I/O error, dev dm-0, sector 328517632\n",
       "end_request: I/O error, dev dm-0, sector 328517632\n",
       "end_request: I/O error, dev dm-0, sector 432234480\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=16144097, block=64553326\n",
       "EXT4-fs error (device dm-2): __ext4_get_inode_loc: unable to read inode block - inode=16144306, block=64553339\n",
       "EXT4-fs error (device dm-2): __ext4_</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col4\" class=\"data row197 col4\" >['multiple filesystem errors']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow197_col5\" class=\"data row197 col5\" >['multiple filesystem errors', '_ _ ext4_get_inode_loc', '_ _', 'block=75563299 ext4-fs error', 'block=54690070 ext4-fs error', 'block=64553339 ext4-fs error', 'block=68288803 ext4-fs error', 'block=64553329 ext4-fs error', 'block=53936488 ext4-fs error', 'block=54591912 ext4-fs error', 'block=70910340 ext4-fs error']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row198\" class=\"row_heading level0 row198\" >198</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col0\" class=\"data row198 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col1\" class=\"data row198 col1\" >02493552</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col2\" class=\"data row198 col2\" >/toms_share became read-only</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col3\" class=\"data row198 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Filesystem /toms_share went into read only mode\n",
       "\n",
       "[root@ngnisprdappls001 toms_share]# mkdir abc\n",
       "mkdir: cannot create directory `abc': Read-only file system\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "apps impacted. Cannot write or create any directory or files\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "It started at around 5:36AM\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Apps impacted as they cannot write/create any more directory/files</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col4\" class=\"data row198 col4\" >[]</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow198_col5\" class=\"data row198 col5\" >['mkdir abc mkdir', 'abc', 'file system', 'mode', 'directory', 'toms_share', 'read', 'filesystem']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row199\" class=\"row_heading level0 row199\" >199</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col0\" class=\"data row199 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col1\" class=\"data row199 col1\" >01854880</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col2\" class=\"data row199 col2\" >filesystem wen in to readonly, need RCA</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col3\" class=\"data row199 col3\" >Can you please look in to the below issue, as AUT2DEV database related file system became read only mode, below are the file system details.\n",
       "\n",
       "/opt/local/aut2ddbs\n",
       "\n",
       "/opt/local/aut2ddbs/oradata/data1\n",
       "\n",
       "/opt/local/aut2ddbs/oradata/arch001</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col4\" class=\"data row199 col4\" >['filesystem wen', 'rca']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow199_col5\" class=\"data row199 col5\" >['filesystem wen', 'rca', 'file system details', 'file system', 'aut2dev database', 'local', 'below issue', 'aut2ddbs', 'oradata', 'mode', 'data1', 'arch001']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row200\" class=\"row_heading level0 row200\" >200</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col0\" class=\"data row200 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col1\" class=\"data row200 col1\" >02441407</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col2\" class=\"data row200 col2\" >Linux boxes not booting after storage loss</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col3\" class=\"data row200 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "A storage array went down that some Linux hosts were using for boot and Oracle.   The systems can be recovered, but when they are rebooted they go right back into emergency mode.\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Linux boxes which experienced the loss\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Reboot\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Systems will not boot properly without manual intervention.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col4\" class=\"data row200 col4\" >['storage loss', 'linux boxes']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow200_col5\" class=\"data row200 col5\" >['storage loss', 'linux boxes', 'linux hosts', 'emergency mode', 'storage array', 'oracle', 'boot', 'systems']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row201\" class=\"row_heading level0 row201\" >201</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col0\" class=\"data row201 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col1\" class=\"data row201 col1\" >02485497</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col2\" class=\"data row201 col2\" >requalar boot fail every weekend</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col3\" class=\"data row201 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "we cannot boot up the server normally\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "failure when reboot . PRD\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "every weekend\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "yes impact when when</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col4\" class=\"data row201 col4\" >['requalar boot', 'weekend']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow201_col5\" class=\"data row201 col5\" >['requalar boot', 'weekend', 'server']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row202\" class=\"row_heading level0 row202\" >202</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col0\" class=\"data row202 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col1\" class=\"data row202 col1\" >02017886</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col2\" class=\"data row202 col2\" >EXT4-fs warning: checktime reached, running e2fsck is recommended</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col3\" class=\"data row202 col3\" >This is the warning message received in dmesg tool, wanted to know is there any harmful for stored data. How much important to perform e2fsck scan?.  \n",
       "\n",
       "EXT4-fs (sda1): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (sda7): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (dm-0): warning: checktime reached, running e2fsck is recommended\n",
       "EXT4-fs (dm-0): recovery complete\n",
       "EXT4-fs (dm-0): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (dm-1): warning: checktime reached, running e2fsck is recommended\n",
       "EXT4-fs (dm-1): recovery complete\n",
       "EXT4-fs (dm-1): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (dm-3): warning: checktime reached, running e2fsck is recommended\n",
       "EXT4-fs (dm-3): recovery complete\n",
       "EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (dm-4): warning: checktime reached, running e2fsck is recommended\n",
       "EXT4-fs (dm-4): recovery complete\n",
       "EXT4-fs (dm-4): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (sdc): warning: checktime reached, running e2fsck is recommended\n",
       "EXT4-fs (sdc): recovery complete\n",
       "EXT4-fs (sdc): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (sdd1): recovery complete\n",
       "EXT4-fs (sdd1): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (sdd2): recovery complete\n",
       "EXT4-fs (sdd2): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (sdb1): warning: checktime reached, running e2fsck is recommended\n",
       "EXT4-fs (sdb1): recovery complete\n",
       "EXT4-fs (sdb1): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (sdb2): warning: checktime reached, running e2fsck is recommended\n",
       "EXT4-fs (sdb2): recovery complete\n",
       "EXT4-fs (sdb2): mounted filesystem with ordered data mode. Opts:\n",
       "EXT4-fs (sde): recovery complete\n",
       "EXT4-fs (sde): mounted filesystem with ordered data mode. Opts:\n",
       "Adding 4200960k swap on /dev/sda6.  Priority:-1 extents:1 across:4200960k\n",
       "ip_tables: (C) 2000-2006 Netfilter Core Team\n",
       "nf_conntrack version 0.5.0 (16384 buckets, 65536 max)\n",
       "NET: Registered protocol family 10\n",
       "lo: Disabled Privacy Extensions</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col4\" class=\"data row202 col4\" >['ext4-fs warning', 'e2fsck', 'checktime']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow202_col5\" class=\"data row202 col5\" >['ext4-fs warning', 'e2fsck', 'checktime', 'recovery complete ext4-fs', 'ext4-fs', 'data mode', 'netfilter core team', 'disabled privacy extensions', 'data', 'protocol family', 'filesystem', 'opts', 'version 0.5.0']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row203\" class=\"row_heading level0 row203\" >203</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col0\" class=\"data row203 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col1\" class=\"data row203 col1\" >01955566</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col2\" class=\"data row203 col2\" >ext4文件系统EXT4-fs error (device dm-4)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col3\" class=\"data row203 col3\" >Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)\n",
       "Oct 19 13:07:32 QFWA0524 kernel: EXT4-fs error (device dm-4): __ext4_ext_check_block: bad header/extent in inode #4851571: invalid magic - magic 3f3c, entries 28024, max 8300(0), depth 25974(0)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col4\" class=\"data row203 col4\" >['device dm-4', 'ext4文件系统ext4-fs error']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow203_col5\" class=\"data row203 col5\" >['device dm-4', 'ext4文件系统ext4-fs error', '_ _ ext4_ext_check_block', 'invalid magic', 'magic 3f3c', 'inode #', 'device dm-4', 'bad header', 'ext4-fs error', 'max 8300(0', 'qfwa0524 kernel', 'extent']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row204\" class=\"row_heading level0 row204\" >204</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col0\" class=\"data row204 col0\" >740323</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col1\" class=\"data row204 col1\" >01854822</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col2\" class=\"data row204 col2\" >Bad entry in directory (problem in dirs and file)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col3\" class=\"data row204 col3\" >Displays the Ext4-fs error message (device sc1): ext4_add_entry, bad entry in directory,  \n",
       "When the inum is searched, it shows the path of files or directories with problems, fsck has been executed and the OS has been restarted, but the messages continue to appear.\n",
       "\n",
       "The contact for support: itvirtual@avianca.com,  josegerardo.villalta@avianca.com</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col4\" class=\"data row204 col4\" >['bad entry', 'file', 'dirs', 'problem', 'directory']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow204_col5\" class=\"data row204 col5\" >['bad entry', 'file', 'dirs', 'problem', 'directory', 'ext4-fs error message', 'device sc1', 'bad entry', 'fsck', 'problems', 'ext4_add_entry', 'messages', 'files', 'path', 'directory']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row205\" class=\"row_heading level0 row205\" >205</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col0\" class=\"data row205 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col1\" class=\"data row205 col1\" >01814280</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col2\" class=\"data row205 col2\" >Remote CIFS directory refuses to mount. Permission Denied.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col3\" class=\"data row205 col3\" >CIFS mount is no longer working.\n",
       "When trying to mount (either via fstab with auth file or manual without the authfile) we are getting permission denied.\n",
       "\n",
       "We have ensured that ports are open.\n",
       "We have ensured rolled back the patches on the windows server and even restored the server from a known good backup and yet it still fails.\n",
       "\n",
       "We have tested CIFS ports and they are open and smbclient (using the designated service account) works as expected.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col4\" class=\"data row205 col4\" >['remote cifs directory', 'permission denied']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow205_col5\" class=\"data row205 col5\" >['remote cifs directory', 'permission denied', 'windows server', 'cifs ports', 'auth file', 'server', 'open', 'service account', 'cifs mount', 'good backup', 'smbclient', 'ports']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row206\" class=\"row_heading level0 row206\" >206</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col0\" class=\"data row206 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col1\" class=\"data row206 col1\" >02379980</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col2\" class=\"data row206 col2\" >Mount point issue | NT_STATUS_ACCOUNT_LOCKED_OUT</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col3\" class=\"data row206 col3\" >Hello,\n",
       "\n",
       "sosreport of hslep-dbcia server attached,  Using Windows samba Server, and client is RHEL 6.10,\n",
       "when mount this directory on rhel 6.10, then it shows error NT_STATUS_ACCOUNT_LOCKED_OUT.\n",
       "Kindly suggest.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col4\" class=\"data row206 col4\" >['mount point issue | nt_status_account_locked_out']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow206_col5\" class=\"data row206 col5\" >['mount point issue | nt_status_account_locked_out', 'dbcia server', 'rhel', 'server', 'error nt_status_account_locked_out', 'directory', 'windows', 'client', 'mount', 'hslep', 'sosreport']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row207\" class=\"row_heading level0 row207\" >207</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col0\" class=\"data row207 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col1\" class=\"data row207 col1\" >02097524</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col2\" class=\"data row207 col2\" >CIFS Mounts are not mounting automatically after OS patching</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col3\" class=\"data row207 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "mount error(13): Permission denied\n",
       "Refer to the mount.cifs(8) manual page (e.g. man mount.cifs)</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col4\" class=\"data row207 col4\" >['os patching', 'cifs mounts']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow207_col5\" class=\"data row207 col5\" >['os patching', 'cifs mounts', 'manual page', 'man mount.cifs', 'mount error(13', 'mount.cifs(8', 'refer', 'permission']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row208\" class=\"row_heading level0 row208\" >208</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col0\" class=\"data row208 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col1\" class=\"data row208 col1\" >02088927</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col2\" class=\"data row208 col2\" >CIFS cannot be mounted</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col3\" class=\"data row208 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Not able to mount the CIFS from NAS on the Linux server\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Getting the error \"Permission Denied\" and in messages we are getting \"NT_STATUS_ACCOUNT_LOCKED_OUT\"\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "When trying to mount the CIFS\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Customer requires it to transfer the data</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col4\" class=\"data row208 col4\" >['cifs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow208_col5\" class=\"data row208 col5\" >['cifs', 'linux server', 'nas', 'cifs', 'able']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row209\" class=\"row_heading level0 row209\" >209</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col0\" class=\"data row209 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col1\" class=\"data row209 col1\" >02464919</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col2\" class=\"data row209 col2\" >Update Kernel Error</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col3\" class=\"data row209 col3\" >¿Qué problema/comportamiento le está causando dificultades? ¿Qué espera ver?\n",
       "\n",
       "Actualmente el servidor cuenta con la version kernel-3.10.0-514, éste está detectado como una vulnerabilidad, hicimos un upgrade de kernel y el protocolo CIFS fué cambiado, por lo que el servidor comenzó a dar problemas. ADjunto la captura del error.\n",
       "Favor su colaboracion con el procedimiento mediante el cual podamos hacer un upgrade del kernel sin obtener este error o si podemos justificarla.\n",
       "\n",
       "¿Cuándo ocurre este comportamiento? ¿Con frecuencia? ¿Repetidamente? ¿En momentos determinados?\n",
       "\n",
       "Las alertas de problema comenzaron a ser diarias</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col4\" class=\"data row209 col4\" >['kernel error']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow209_col5\" class=\"data row209 col5\" >['kernel error', 'mediante el cual podamos hacer un', 'favor su colaboracion con el procedimiento', 'del kernel sin obtener este error', 'por lo que el servidor comenzó', 'adjunto la captura del error', 'hicimos un upgrade de kernel', 'éste está detectado como una vulnerabilidad', 'comportamiento le está causando dificultades', 'y el', 'actualmente el']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row210\" class=\"row_heading level0 row210\" >210</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col0\" class=\"data row210 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col1\" class=\"data row210 col1\" >01928324</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col2\" class=\"data row210 col2\" >Unable to mount CIFS share on RHEL 6.x servers</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col3\" class=\"data row210 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Since last night we have been unable to mount CIFS shares on our RHEL 6.x servers. We are getting the following error. We are able to mount the same share on RHEL 7.x servers so the credentials and configuration are correct. We need some help in understanding how to fix it mount issues.\n",
       "\n",
       "\n",
       "mount.cifs kernel mount options: ip=x.x.x.x,unc=\\\\b1-pnetfs02\\TGutility1,vers=2.1,credentials=/root/cifs/.cifscredentials_InvSync,uid=54322,gid=54327,ver=1,user=*****,domain=ultainc.lcl,prefixpath=QA4/MANH/InvSync855,pass=********\n",
       "mount error(13): Permission denied\n",
       "Refer to the mount.cifs(8) manual page (e.g. man mount.cifs)\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "This has been happening in our Production environment since this morning\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "We have been unable to mount the shares since this morning\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "This started happening last night after our production patching cycle. Servers that were and weren't patched are having the same issue. This is currently affecting multiple distribution centers</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col4\" class=\"data row210 col4\" >['rhel 6.x servers', 'cifs share', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow210_col5\" class=\"data row210 col5\" >['rhel 6.x servers', 'cifs share', 'unable', 'mount.cifs kernel mount options', 'rhel 6.x servers', 'mount issues', 'mount error(13', 'ip =', 'man mount.cifs', 'cifs shares', 'same share', 'servers', 'rhel']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row211\" class=\"row_heading level0 row211\" >211</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col0\" class=\"data row211 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col1\" class=\"data row211 col1\" >01710089</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col2\" class=\"data row211 col2\" >Unable to list the samba share  smbstatus,</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col3\" class=\"data row211 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Unable to list the samba share\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Unable to list the samba share using smbstatus\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Create RHEL 5.8 channel in our satellite server\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "Unable to list the samba share using smbstatus</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col4\" class=\"data row211 col4\" >['samba share', 'smbstatus', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow211_col5\" class=\"data row211 col5\" >['samba share', 'smbstatus', 'unable', 'samba share', 'unable']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row212\" class=\"row_heading level0 row212\" >212</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col0\" class=\"data row212 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col1\" class=\"data row212 col1\" >02200831</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col2\" class=\"data row212 col2\" >Intermittent connection issue while sending the files via sftp</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col3\" class=\"data row212 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Intermittent connection issue while sending the files via sftp\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "around 2:38 PM EST on 9th october</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col4\" class=\"data row212 col4\" >['intermittent connection issue', 'sftp', 'files']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow212_col5\" class=\"data row212 col5\" >['intermittent connection issue', 'sftp', 'files', 'intermittent connection issue', 'sftp', 'files']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row213\" class=\"row_heading level0 row213\" >213</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col0\" class=\"data row213 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col1\" class=\"data row213 col1\" >02021034</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col2\" class=\"data row213 col2\" >CIFS umount from particular server</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col3\" class=\"data row213 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "My name is Pradeep , I have query related issue \n",
       "issue : On 22 Jan : around 10 AM -12 PM====CIFS  : //10.60.1.189/coi     2.0T  1.7T  323G  85% /PAN_INDIA\n",
       "Unfortunately unmounted from particuar 10.60.6.124 and business user let me know reason why unmounted CIFS mount point . I have attached SOS reports for your reference. \n",
       "Noted : We have confirmed form server support there is using NT ID into password non-expiry policy so they confirm that password never expired to NT ID. \n",
       "\n",
       "How to check log that why CIFS mount point suddenly unmounted from target system.    \n",
       "\n",
       "Best & Regards'\n",
       "Pradeep Kumar\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "production\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "this is first time issue happened and there is security incident.\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "I have attached SOS reports</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col4\" class=\"data row213 col4\" >['particular server', 'cifs umount']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow213_col5\" class=\"data row213 col5\" >['particular server', 'cifs umount', 'unmounted cifs mount point', 'password non - expiry policy', 'cifs mount point', 'nt id', 'form server support', 'unmounted', 'related issue', 'target system', 'pradeep kumar', 'business user']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row214\" class=\"row_heading level0 row214\" >214</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col0\" class=\"data row214 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col1\" class=\"data row214 col1\" >02175580</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col2\" class=\"data row214 col2\" >Unable to mount the cifs mount point getting error host is down</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col3\" class=\"data row214 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "we are unable to mount  the cifs mount point in VM hslecc-app6.\n",
       "Getting below error:\n",
       "-bash-4.1# mount -t cifs -o  defaults,_netdev,uid=fppadm,gid=sapsys,credentials=/home/.saba,domain=hdfcsldm //10.60.2.118/SAPtoP2P /SAPtoP2P\n",
       "mount error(13): Permission denied\n",
       "Refer to the mount.cifs(8) manual page (e.g. man mount.cifs)\n",
       "-bash-4.1#\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "Its a production server mount point\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "At certain times\n",
       "\n",
       "What information can you provide around timeframes and the business impact?\n",
       "\n",
       "Its Production SAP server mount point and due to mount point issue  our backup activity on hold.</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col4\" class=\"data row214 col4\" >['cifs mount point', 'error host', 'unable']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow214_col5\" class=\"data row214 col5\" >['cifs mount point', 'error host', 'unable', 'cifs mount point', 'mount -t cifs', 'mount error(13', 'manual page', 'vm hslecc', 'man mount.cifs', 'mount.cifs(8', 'fppadm', 'refer', 'gid']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row215\" class=\"row_heading level0 row215\" >215</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col0\" class=\"data row215 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col1\" class=\"data row215 col1\" >02178675</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col2\" class=\"data row215 col2\" >CIFS account gets locked out post reboot</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col3\" class=\"data row215 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "Hi, CIFS account gets locked post reboot. After unlocking, the CIFS mount works fine from the command line or mount -a.  Can you diagnose this behaviour?</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col4\" class=\"data row215 col4\" >['post reboot', 'cifs account']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow215_col5\" class=\"data row215 col5\" >['post reboot', 'cifs account', 'cifs mount', 'mount -a', 'cifs account', 'post reboot', 'command line', 'behaviour']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row216\" class=\"row_heading level0 row216\" >216</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col0\" class=\"data row216 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col1\" class=\"data row216 col1\" >01758608</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col2\" class=\"data row216 col2\" >CIFS filesystem inconsistency</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col3\" class=\"data row216 col3\" >What problem/issue/behavior are you having trouble with?  What do you expect to see?\n",
       "\n",
       "we have noticed that suddenly CIFS filesystem was unmounted on Production system\n",
       "\n",
       "Where are you experiencing the behavior?  What environment?\n",
       "\n",
       "we have noticed that suddenly CIFS filesystem was unmounted on Production system\n",
       "\n",
       "When does the behavior occur? Frequently?  Repeatedly?   At certain times?\n",
       "\n",
       "Frequently\n",
       "\n",
       "What information can you provide around timeframes and urgency?\n",
       "\n",
       "we have noticed that suddenly CIFS filesystem was unmounted on Production system</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col4\" class=\"data row216 col4\" >['cifs filesystem inconsistency']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow216_col5\" class=\"data row216 col5\" >['cifs filesystem inconsistency', 'production system', 'cifs filesystem']</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dclevel0_row217\" class=\"row_heading level0 row217\" >217</th>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col0\" class=\"data row217 col0\" >797553</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col1\" class=\"data row217 col1\" >02237349</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col2\" class=\"data row217 col2\" >Montage CIFS fait planter le systeme</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col3\" class=\"data row217 col3\" >Quelle sorte de problème/comportement inattendu rencontrez-vous ? A quoi vous attendez-vous normalement ?\n",
       "\n",
       "nous avons perdu un montage cifs pour une raison inconnue, ce dernier fait planter le système.\n",
       "Malgré un remontage du point de montage le système le load système est très important avec beaucoup d'I/O wait.\n",
       "\n",
       "Où rencontrez-vous ce comportement inattendu ? Dans quel environnement ?\n",
       "\n",
       "production\n",
       "\n",
       "Quand rencontrez-vous ce comportement ? Fréquemment ? Régulièrement ? À certains moments ?\n",
       "\n",
       "c'est la première fois que cela plante le serveur.\n",
       "\n",
       "Quelles informations pouvez-vous donner sur les échéances à respecter et sur l'impact sur le business ?\n",
       "\n",
       "Production HS</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col4\" class=\"data row217 col4\" >['planter le systeme', 'montage cifs']</td>\n",
       "                        <td id=\"T_cec29410_1bc7_11ea_a1c2_d094665342dcrow217_col5\" class=\"data row217 col5\" >['planter le systeme', 'montage cifs', 'malgré un remontage du point de montage le système le load système est très', \"respecter et sur l'impact sur le business\", \"c'est la première fois que cela plante le serveur\", 'ce dernier fait planter le système', 'vous donner sur les échéances', 'vous ce comportement', 'quoi vous attendez', 'vous normalement', 'un montage cifs', 'vous']</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb9fb827d68>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.style.set_properties(**{'text-align': 'left'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229.93262028694153"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Keywords for KCS/rule"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#The first 4/5 of the cases used to calculate, and the rest 1/5 of the cases used to evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['resource_display_id__c'] == '1614393'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历df\n",
    "rule_tag_dict = {}\n",
    "for key in kcs_rule_dict.keys():\n",
    "    temp_df = df[df['resource_display_id__c'] == key]\n",
    "    case_count = temp_df.shape[0]\n",
    "    train_count = int(case_count * (4/5))\n",
    "    temp_train_df = temp_df[:train_count-1]\n",
    "    kcs_tag = []\n",
    "    for description_phrases in temp_train_df['description_key_phrases']:\n",
    "        kcs_tag = kcs_tag + description_phrases    \n",
    "    rule_tag_dict[key] = kcs_tag   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'740323': ['filesystem',\n",
       "  'high utilization',\n",
       "  'filesystem utilization',\n",
       "  'df -g output',\n",
       "  'actual size',\n",
       "  'gb',\n",
       "  'file system',\n",
       "  'mail service',\n",
       "  'regards',\n",
       "  'zimbra',\n",
       "  'prioroty',\n",
       "  'server',\n",
       "  'e2fsck',\n",
       "  'checktime',\n",
       "  'ext3-fs',\n",
       "  'servers unable',\n",
       "  'jump server',\n",
       "  'below output',\n",
       "  'servers unable',\n",
       "  'gpd-973',\n",
       "  'a9e',\n",
       "  'ff03',\n",
       "  'mode',\n",
       "  'team',\n",
       "  'issue',\n",
       "  'high swap usage',\n",
       "  'filesystem errors',\n",
       "  'logs',\n",
       "  'swap usage',\n",
       "  'oracle databases',\n",
       "  'failover cluster',\n",
       "  'usage',\n",
       "  'filesystem errors',\n",
       "  'node active',\n",
       "  'few days',\n",
       "  'server logs',\n",
       "  'rhel',\n",
       "  'issue',\n",
       "  'hardware error',\n",
       "  'performance issue',\n",
       "  'dmesg',\n",
       "  'machine check events',\n",
       "  'hardware error',\n",
       "  'root@pruswipprodb1',\n",
       "  'strange errors',\n",
       "  'message.log',\n",
       "  '18th sep log',\n",
       "  'umount messages',\n",
       "  'filesystem mount',\n",
       "  'message.log',\n",
       "  'reason',\n",
       "  'possible currupt file system',\n",
       "  'fsck',\n",
       "  'mount point',\n",
       "  'netbackup',\n",
       "  'root file system',\n",
       "  'issue',\n",
       "  'maintenance mode',\n",
       "  'sos report',\n",
       "  'temporary solution',\n",
       "  'server',\n",
       "  'rw',\n",
       "  'root',\n",
       "  'machine',\n",
       "  'production',\n",
       "  'ログ調査依頼',\n",
       "  'maximal mount count',\n",
       "  'kern.warning ext4-fs',\n",
       "  'kern.info ext4-fs',\n",
       "  'data mode',\n",
       "  'ext4-fs',\n",
       "  'kernel',\n",
       "  'filesystem',\n",
       "  'opts',\n",
       "  'e2fsck',\n",
       "  'warning',\n",
       "  'rescue mode',\n",
       "  'nfs mounts',\n",
       "  'host',\n",
       "  'boot',\n",
       "  'mounts fine',\n",
       "  'boot',\n",
       "  'nfs',\n",
       "  'dm-8',\n",
       "  'ext4-fs',\n",
       "  'kernel',\n",
       "  '在menssage有报错',\n",
       "  'errors',\n",
       "  'e2fsck',\n",
       "  'alex01dc01(virtual machine disk unavailable',\n",
       "  'ovsbld compute nodes',\n",
       "  'ru disk unavailable',\n",
       "  'dpdk compute nodes',\n",
       "  'ovsrack compute nodes',\n",
       "  'vm disk unavailable',\n",
       "  'cloud openstack cluster',\n",
       "  'cloud traffic',\n",
       "  'ceph nodes',\n",
       "  'controllers nodes',\n",
       "  'openstack cluster',\n",
       "  'fileystem',\n",
       "  'unable',\n",
       "  'lvol01 resize2fs 1.41.12',\n",
       "  'logical volume lvol01',\n",
       "  'lvol01 e2fsck',\n",
       "  'lvol01',\n",
       "  'lv write access',\n",
       "  'lv creation host',\n",
       "  'lv size',\n",
       "  'lv name',\n",
       "  'lv status',\n",
       "  'vgdata',\n",
       "  'ext4文件系统报错',\n",
       "  '您遇到了什么问题？您所期望获得的结果是什么 ？',\n",
       "  'dm-2',\n",
       "  'ext4-fs',\n",
       "  'errors',\n",
       "  'e2fsck',\n",
       "  '03rd august',\n",
       "  'grub',\n",
       "  'server',\n",
       "  '3rd august',\n",
       "  'rca',\n",
       "  'grub',\n",
       "  'server',\n",
       "  'multiple filesystem errors',\n",
       "  '_ _ ext4_get_inode_loc',\n",
       "  '_ _',\n",
       "  'block=75563299 ext4-fs error',\n",
       "  'block=54690070 ext4-fs error',\n",
       "  'block=64553339 ext4-fs error',\n",
       "  'block=68288803 ext4-fs error',\n",
       "  'block=64553329 ext4-fs error',\n",
       "  'block=53936488 ext4-fs error',\n",
       "  'block=54591912 ext4-fs error',\n",
       "  'block=70910340 ext4-fs error',\n",
       "  'mkdir abc mkdir',\n",
       "  'abc',\n",
       "  'file system',\n",
       "  'mode',\n",
       "  'directory',\n",
       "  'toms_share',\n",
       "  'read',\n",
       "  'filesystem'],\n",
       " '24651': ['multiple issues',\n",
       "  'server',\n",
       "  'fs issues',\n",
       "  'multiple errors',\n",
       "  'server',\n",
       "  'xen host',\n",
       "  'lv',\n",
       "  'able',\n",
       "  'file system',\n",
       "  'root cause cause',\n",
       "  'file system',\n",
       "  'issue',\n",
       "  'solution',\n",
       "  'system',\n",
       "  'file system',\n",
       "  'mode',\n",
       "  'g',\n",
       "  'appsvol00 rootvg',\n",
       "  'move log copy%',\n",
       "  'ao',\n",
       "  'rootvg',\n",
       "  'san lun .this',\n",
       "  'attr psize',\n",
       "  'round',\n",
       "  'robin',\n",
       "  'mpath/360060e80100a84200530204a00000065 sanvg',\n",
       "  'cluster',\n",
       "  'drnclinux kernel',\n",
       "  'maximal mount count',\n",
       "  'ext3-fs warning',\n",
       "  'error ext3-fs',\n",
       "  'apr',\n",
       "  'data mode',\n",
       "  'commit interval',\n",
       "  'ext3 fs',\n",
       "  'internal journal',\n",
       "  'e2fsck',\n",
       "  'cluster service',\n",
       "  'lbblxs01 kernel',\n",
       "  'lbblxs01 clurgmgrd[22102',\n",
       "  'lbblxs01 clurgmgrd',\n",
       "  'messages file oct',\n",
       "  'lbblxs01 multipathd',\n",
       "  'journal oct',\n",
       "  'lbblxs01 smbd[27994',\n",
       "  'lbblxs01 avahi',\n",
       "  'lbblxs01 smbd[27999',\n",
       "  'lbblxs01 smbd[27998',\n",
       "  'file system',\n",
       "  'file system issue',\n",
       "  'file system',\n",
       "  'feb',\n",
       "  'server',\n",
       "  'read',\n",
       "  'sap',\n",
       "  'gmt',\n",
       "  'filesystem entrou',\n",
       "  'filesystem',\n",
       "  'causa raíz e os procedimentos para evitar isso',\n",
       "  'nenhuma alteração foi realizada',\n",
       "  'storage ibm ds',\n",
       "  'ibm ds storage',\n",
       "  'esse filesystem entrou',\n",
       "  'set 04:12am',\n",
       "  'sep 04:12am',\n",
       "  'vezes desde ontem',\n",
       "  'está em uso',\n",
       "  'filesystem',\n",
       "  'ext3-fs error',\n",
       "  'filesystem',\n",
       "  'ext3-fs error',\n",
       "  'mdsildb2 kernel',\n",
       "  'device dm-0',\n",
       "  'unlinked inode',\n",
       "  'dir #',\n",
       "  'cat messages | grep ext',\n",
       "  'filesystem error',\n",
       "  'mar',\n",
       "  'ext3_lookup',\n",
       "  'dir',\n",
       "  'mount',\n",
       "  'customer',\n",
       "  'mount point',\n",
       "  'tries',\n",
       "  'reboots',\n",
       "  'oracle',\n",
       "  'disk missing',\n",
       "  'disk expansion',\n",
       "  'disk',\n",
       "  'disk corruption',\n",
       "  'root cause',\n",
       "  'disk corruption',\n",
       "  'root case',\n",
       "  'esesslx0579.ss.sw.ericsson.se',\n",
       "  'refer',\n",
       "  'request',\n",
       "  'mode',\n",
       "  'read',\n",
       "  'fs',\n",
       "  'happening',\n",
       "  'mounts | grep -i ro',\n",
       "  'mount fs issue',\n",
       "  'same issue reoccur',\n",
       "  'grid ext3 ro',\n",
       "  'db file system',\n",
       "  '# cat',\n",
       "  'ukortdhr',\n",
       "  'mode',\n",
       "  'read',\n",
       "  'mapper',\n",
       "  'dr activity',\n",
       "  'mode',\n",
       "  'read',\n",
       "  'filesystem',\n",
       "  'dr activity',\n",
       "  'immediate action',\n",
       "  'server rg546',\n",
       "  'read',\n",
       "  'filesystem',\n",
       "  'mode',\n",
       "  'impact',\n",
       "  'services',\n",
       "  'delay',\n",
       "  'customer',\n",
       "  'file system',\n",
       "  'mode',\n",
       "  'file system',\n",
       "  'mode',\n",
       "  'inode error',\n",
       "  'hung state',\n",
       "  'same error',\n",
       "  'inode error',\n",
       "  'servers',\n",
       "  'reboot',\n",
       "  'week',\n",
       "  'ext3_lookup',\n",
       "  'unlinked inode',\n",
       "  'device dm-37',\n",
       "  'dir #',\n",
       "  'ext3_lookup',\n",
       "  'error',\n",
       "  'li03b03 kernel',\n",
       "  'ext3-fs error',\n",
       "  'device dm-37',\n",
       "  'unlinked inode',\n",
       "  'dir #',\n",
       "  'jun',\n",
       "  'ext3_lookup',\n",
       "  'message',\n",
       "  'log',\n",
       "  'messages',\n",
       "  'console screenshot',\n",
       "  'server',\n",
       "  'hung state',\n",
       "  'console sever',\n",
       "  'killable process',\n",
       "  'kernel panic',\n",
       "  'memory',\n",
       "  'time',\n",
       "  'rhel',\n",
       "  'sever',\n",
       "  'error',\n",
       "  'team',\n",
       "  'server',\n",
       "  '% unable',\n",
       "  'files',\n",
       "  'file system rm',\n",
       "  'file system',\n",
       "  'jboss-5.1.0.ga',\n",
       "  'rm',\n",
       "  'jar',\n",
       "  'jbossorg',\n",
       "  'eula.txt',\n",
       "  'lgpl.html',\n",
       "  'versions.xml',\n",
       "  'error'],\n",
       " '1614393': ['block size',\n",
       "  'mount xfs',\n",
       "  'function',\n",
       "  'k',\n",
       "  'sudo mkfs.xfs -b',\n",
       "  'sudo mount',\n",
       "  'sunit=1 blks',\n",
       "  'syncdata02 mount',\n",
       "  'rtextents=0 ds',\n",
       "  'ftype=1 log',\n",
       "  'internal log',\n",
       "  'sudo mkdir',\n",
       "  'agsize=10485600 blks',\n",
       "  'bsize=16384',\n",
       "  'k blocksize',\n",
       "  'partitioning',\n",
       "  'k block size',\n",
       "  'k blksize lvm',\n",
       "  'file system',\n",
       "  'k blocksize',\n",
       "  'santosh +61',\n",
       "  'multiple disks',\n",
       "  'issue',\n",
       "  'able',\n",
       "  'vm',\n",
       "  'rhel',\n",
       "  'xfs system',\n",
       "  'block size',\n",
       "  'ci=0 ftype=1 log',\n",
       "  'finobt=0 spinodes=0 data',\n",
       "  'bsize=4096',\n",
       "  'count=1 realtime',\n",
       "  'sunit=1 blks',\n",
       "  'mapper',\n",
       "  'xfs system',\n",
       "  'lv_data01',\n",
       "  'sectsz=4096',\n",
       "  'blockdev --getra',\n",
       "  'specific block size',\n",
       "  'xfs filesystem',\n",
       "  'logical volume',\n",
       "  'correct syntax',\n",
       "  'specific block size',\n",
       "  'sdh meta',\n",
       "  'ci=0 ftype=1 log',\n",
       "  'xfs file system',\n",
       "  'block size',\n",
       "  'rtextents=0 data blocks',\n",
       "  'size=4096 -b size=8192',\n",
       "  'mkfs -t xfs',\n",
       "  'internal log',\n",
       "  'count=1 realtime',\n",
       "  'xfs file system',\n",
       "  'k block size',\n",
       "  'system',\n",
       "  'rhel',\n",
       "  '16k block size file system',\n",
       "  '16k block size',\n",
       "  'block size 16k',\n",
       "  'bytes sector size',\n",
       "  'default block size',\n",
       "  'block size',\n",
       "  'o size',\n",
       "  'xfs file system',\n",
       "  'mount -a mount',\n",
       "  'block device',\n",
       "  'xfs block size',\n",
       "  'page size limitation',\n",
       "  'kb',\n",
       "  'unable',\n",
       "  'xfs block size',\n",
       "  'config_page_size_8 kb directive',\n",
       "  'block size',\n",
       "  'xfs filesystem',\n",
       "  'alternative work',\n",
       "  'https://access.redhat.com/solutions/1614393',\n",
       "  'pagesize',\n",
       "  'rhel',\n",
       "  'k',\n",
       "  'dependent',\n",
       "  'mkfs.xfs -f -b',\n",
       "  'function',\n",
       "  'size=32k',\n",
       "  'mount',\n",
       "  'kernel page size cache',\n",
       "  'mkfs.xfs -f -b size=32k',\n",
       "  'oracle stripe size',\n",
       "  'xfs block size',\n",
       "  'getconf page_size',\n",
       "  'mapper',\n",
       "  'purpose',\n",
       "  'vg_07-oradata06a',\n",
       "  '32k',\n",
       "  'order',\n",
       "  'mount xfs file system',\n",
       "  'volume',\n",
       "  'unable',\n",
       "  'blocksize',\n",
       "  'xfs file system',\n",
       "  'sas application performance',\n",
       "  'block size',\n",
       "  'xfs',\n",
       "  'bs',\n",
       "  'possible',\n",
       "  'de los plazos y el impacto comercial',\n",
       "  'está presentando el comportamiento',\n",
       "  'comportamiento le está causando dificultades',\n",
       "  'qué información puede',\n",
       "  'qué espera ver',\n",
       "  'cuándo ocurre este comportamiento',\n",
       "  'qué entorno',\n",
       "  'qué problema',\n",
       "  'san environment',\n",
       "  'low general performance',\n",
       "  'partition table issue',\n",
       "  'bloxk size',\n",
       "  'uefi',\n",
       "  'lv_tmp -b size=16384 --size=2048 logvol swap',\n",
       "  '-b size=16384 --size=2048 logvol',\n",
       "  'lv_root -b size=16384 --size=6144 logvol',\n",
       "  '@host.shortname %',\n",
       "  '--size=2048 logvol',\n",
       "  'lv_var -b size=16384',\n",
       "  '%',\n",
       "  'vda --asprimary part',\n",
       "  'size=16384 --size=2048',\n",
       "  'vda --all part',\n",
       "  'kickstart block size',\n",
       "  'vg_name pv.01 logvol swap',\n",
       "  '--size=4096 logvol',\n",
       "  'snap_part logvol',\n",
       "  '4k block size',\n",
       "  'logvol',\n",
       "  'efi_part part pv.01',\n",
       "  'block size',\n",
       "  'diskpart.cfg zerombr clearpart',\n",
       "  'start #',\n",
       "  'partition table',\n",
       "  'bigger block size',\n",
       "  'xfs',\n",
       "  'k',\n",
       "  'mount',\n",
       "  'tb san disks',\n",
       "  'backup agent',\n",
       "  'backup software',\n",
       "  'disk pool',\n",
       "  'backup',\n",
       "  'block size',\n",
       "  'disks',\n",
       "  'function',\n",
       "  'xfa',\n",
       "  'host',\n",
       "  'best practise',\n",
       "  'oracle db',\n",
       "  'asm',\n",
       "  'xfs',\n",
       "  'parameters',\n",
       "  'tb oracle db',\n",
       "  'oracle db operation',\n",
       "  'oracle asm',\n",
       "  'hp dl580 gen9',\n",
       "  'best practise',\n",
       "  'rhel',\n",
       "  'file systems',\n",
       "  'guide',\n",
       "  'xfs',\n",
       "  'poc',\n",
       "  'k block size',\n",
       "  'xfs fs',\n",
       "  'ci=0 ftype=1 log',\n",
       "  'finobt=0 spinodes=0 data',\n",
       "  'lv_data meta',\n",
       "  'bsize=4096',\n",
       "  'count=1 realtime',\n",
       "  'blks',\n",
       "  'vg_data',\n",
       "  'sectsz=512',\n",
       "  'mapper',\n",
       "  'lv_data',\n",
       "  'format os',\n",
       "  'fs',\n",
       "  'application',\n",
       "  'non os related fs',\n",
       "  'k format',\n",
       "  'os',\n",
       "  'client',\n",
       "  'new request',\n",
       "  'steps',\n",
       "  'server',\n",
       "  'block size',\n",
       "  'rhel',\n",
       "  '16kb',\n",
       "  'general question',\n",
       "  'issue',\n",
       "  'sosreport',\n",
       "  'logical volume',\n",
       "  'block size',\n",
       "  'requirement',\n",
       "  'mkfs.xfs -f -b size=8192',\n",
       "  'mkfs.xfs -b size=8192',\n",
       "  'ci=0 ftype=1 log',\n",
       "  'bytes block size',\n",
       "  'vg02-sdisk01vol isize=512',\n",
       "  'mount fs',\n",
       "  'mapper',\n",
       "  'internal log',\n",
       "  'vg02-sdisk01vol',\n",
       "  'bsize=8192',\n",
       "  'file system',\n",
       "  'x86_64 hardware',\n",
       "  'oracle database',\n",
       "  'blocks',\n",
       "  'optimal file system',\n",
       "  'oracle databases',\n",
       "  'oracle consultants',\n",
       "  'x86_64 hardware',\n",
       "  'file systems',\n",
       "  'block sizes',\n",
       "  'error function',\n",
       "  'block size',\n",
       "  'xfs',\n",
       "  'root inode chunk phase',\n",
       "  'ci=0 ftype=1 log',\n",
       "  'check inode connectivity',\n",
       "  'internal log',\n",
       "  'mkfs.xfs -f -b size=64k',\n",
       "  'disk000 mount',\n",
       "  '# mount',\n",
       "  'cv_cvmatpg_disk000',\n",
       "  'scan filesystem freespace',\n",
       "  'xfs filesystem',\n",
       "  'rhel6 blocksize',\n",
       "  'xfs',\n",
       "  'sunit=0 blks',\n",
       "  '# mount',\n",
       "  'lv_msdp /msdp',\n",
       "  'internal log',\n",
       "  'block size',\n",
       "  'application requirement',\n",
       "  'sunit=0',\n",
       "  'xfs fs',\n",
       "  'mapper',\n",
       "  'sectsz=512',\n",
       "  'block size larger',\n",
       "  'ext4 filesystem',\n",
       "  'block size larger',\n",
       "  'ext4 filesystem',\n",
       "  'performance issue',\n",
       "  'xfs tuning',\n",
       "  '-d su=64k -d sw=4',\n",
       "  'xfs blocksize',\n",
       "  'blade b200m4',\n",
       "  'xfs filesystem',\n",
       "  'max performance',\n",
       "  'metal system',\n",
       "  'linux bare',\n",
       "  'preferred blocksize',\n",
       "  'iscsi luns',\n",
       "  'logical volume'],\n",
       " '2065483': ['pldandbd3 kernel',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'error',\n",
       "  'general performance issues',\n",
       "  'vm',\n",
       "  'tibco system',\n",
       "  'netapp nfs share',\n",
       "  'tibco support',\n",
       "  'rpc',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'system',\n",
       "  'nfs',\n",
       "  'attachment',\n",
       "  'sosreport',\n",
       "  'lru record',\n",
       "  'kernel',\n",
       "  'fdsuv09734 kernel',\n",
       "  'lru record',\n",
       "  'fdsuv09734',\n",
       "  'oct',\n",
       "  'root user',\n",
       "  'normal user',\n",
       "  'more time',\n",
       "  'ssh',\n",
       "  'unable',\n",
       "  'multiple server',\n",
       "  'issue',\n",
       "  'days',\n",
       "  'last',\n",
       "  'production server',\n",
       "  'issue',\n",
       "  'redhat gfs2 cluster environment',\n",
       "  'node server',\n",
       "  'production',\n",
       "  'assistance',\n",
       "  'large messages',\n",
       "  'nfs server',\n",
       "  'messages',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'client',\n",
       "  'va_01_02 kernel',\n",
       "  'socket jul',\n",
       "  'veritas nas',\n",
       "  'rpc',\n",
       "  'jul',\n",
       "  'tcp tunings',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'nas',\n",
       "  'rhel',\n",
       "  'soft lockups',\n",
       "  'issues',\n",
       "  'issue',\n",
       "  'few days',\n",
       "  'dead',\n",
       "  'service rpcbind status',\n",
       "  'service rpcbind start',\n",
       "  'service nfs restart',\n",
       "  'rpcbind error',\n",
       "  'few days',\n",
       "  'pid file',\n",
       "  'dead',\n",
       "  'message',\n",
       "  'problem',\n",
       "  'messages',\n",
       "  'nfs utility hungs',\n",
       "  'large file',\n",
       "  'client side',\n",
       "  'server side',\n",
       "  'different mount option',\n",
       "  'nfs client',\n",
       "  'large file transfer',\n",
       "  'small file',\n",
       "  'server',\n",
       "  'change',\n",
       "  'device',\n",
       "  'advice',\n",
       "  'domain',\n",
       "  'vms',\n",
       "  'storage domain export_domain',\n",
       "  'data center mca',\n",
       "  'export domain',\n",
       "  'domain',\n",
       "  'regards kapil kataria',\n",
       "  'nfs server',\n",
       "  'virtual machines',\n",
       "  'data',\n",
       "  'rhevm portal',\n",
       "  'vms',\n",
       "  'server',\n",
       "  'root cause',\n",
       "  'hung',\n",
       "  'issue',\n",
       "  'server',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'other documentations',\n",
       "  'sos report',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'regards',\n",
       "  'tom',\n",
       "  'help',\n",
       "  'server hung',\n",
       "  'hung',\n",
       "  'server',\n",
       "  'rca',\n",
       "  'power',\n",
       "  'server hung',\n",
       "  'hung',\n",
       "  'server',\n",
       "  'rca',\n",
       "  'power',\n",
       "  'large erros',\n",
       "  'log',\n",
       "  'fragment',\n",
       "  'messages',\n",
       "  'rpc',\n",
       "  \"user pid=4671 uid=0 auid=4294967295 ses=4294967295 msg='op\",\n",
       "  \"uid=0 auid=4294967295 ses=4294967295 msg='op\",\n",
       "  \"auid=4294967295 ses=4294967295 msg='op\",\n",
       "  \"ses=4294967295 msg='op\",\n",
       "  'uid=0 auid=4294967295',\n",
       "  'cesprdbilla003 type',\n",
       "  'crypto_key_user msg',\n",
       "  'cesprdbilla003 audispd',\n",
       "  'cesprdbilla004 type',\n",
       "  'user pid=4671',\n",
       "  'tlnckapv0018 kernel',\n",
       "  'fragment',\n",
       "  'large',\n",
       "  'rpc',\n",
       "  'dec',\n",
       "  'fix',\n",
       "  'nfs mount',\n",
       "  'nfs client',\n",
       "  'nfs',\n",
       "  'vers=3,defaults',\n",
       "  'tlnckapv0018 kernel',\n",
       "  'rpr',\n",
       "  'tlnckapv0018',\n",
       "  'sap',\n",
       "  'gbptlsrvrpr:/sapmnt',\n",
       "  'profile',\n",
       "  'nfs share',\n",
       "  'cluster',\n",
       "  'inaccessible',\n",
       "  'cat messages | grep nfs',\n",
       "  'cat messages | grep time',\n",
       "  'nfs cluster logs',\n",
       "  'nfs share folder',\n",
       "  'linux client vm',\n",
       "  'usffentprd08 kernel',\n",
       "  'nov',\n",
       "  'nfs',\n",
       "  'vm',\n",
       "  'client logs',\n",
       "  'large xxx',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'server',\n",
       "  'fragment error',\n",
       "  'snap shot',\n",
       "  'rpc',\n",
       "  '3rd aug',\n",
       "  'server inaccessible',\n",
       "  'rca',\n",
       "  'var log message',\n",
       "  'root user',\n",
       "  'nmon logs',\n",
       "  '3rd aug',\n",
       "  'root cause',\n",
       "  'normal user',\n",
       "  'server',\n",
       "  'aug',\n",
       "  'root',\n",
       "  'logs',\n",
       "  'hung',\n",
       "  'operational',\n",
       "  'server',\n",
       "  'vmware environment',\n",
       "  'server',\n",
       "  'hung state',\n",
       "  'snapshot',\n",
       "  'reboot',\n",
       "  'operational',\n",
       "  'system',\n",
       "  'days',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'e01 t beat.name',\n",
       "  'e01 t source',\n",
       "  'awdkgeg9kss6m8qdvoqy t _ index',\n",
       "  'e01 t input_type',\n",
       "  'e01 t syslog_message',\n",
       "  'e01 t beat.version',\n",
       "  't _ type',\n",
       "  'log t message',\n",
       "  'kernel t syslog_timestamp',\n",
       "  'syslog t beat.hostname',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'dmesg rpc',\n",
       "  'kernel log',\n",
       "  'rpc',\n",
       "  'fragment',\n",
       "  'following message',\n",
       "  'large',\n",
       "  'bash',\n",
       "  'packet corrupt error',\n",
       "  'bad packet length',\n",
       "  'https://access.redhat.com/solutions/142643 restarted ssh service',\n",
       "  'enabled ssh protocl',\n",
       "  'outgoing ssh connection',\n",
       "  'bad packet length',\n",
       "  'other host',\n",
       "  'servers tools',\n",
       "  'eda tools',\n",
       "  'error',\n",
       "  'host',\n",
       "  'above',\n",
       "  'nfs mount point 10.1.14.253:/fs_tibco_ems',\n",
       "  'production server',\n",
       "  'latency issue',\n",
       "  'nfs mount point 10.1.14.253:/fs_tibco_ems',\n",
       "  'production server',\n",
       "  'latency issue',\n",
       "  'large errors',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'sla13294 kernel',\n",
       "  'nfs team',\n",
       "  'nfs share',\n",
       "  'nfs shares',\n",
       "  'dec',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'messages errors',\n",
       "  'rpc',\n",
       "  'time application',\n",
       "  'unresponsive',\n",
       "  'simple commands',\n",
       "  'several minutes',\n",
       "  'ssh connections',\n",
       "  'business hours',\n",
       "  'server',\n",
       "  'access',\n",
       "  'unresponsive',\n",
       "  'console',\n",
       "  'server',\n",
       "  'possible reason',\n",
       "  'same',\n",
       "  'system',\n",
       "  'today',\n",
       "  'gluster volume',\n",
       "  'nfs mount',\n",
       "  'incorrect entries',\n",
       "  'directory',\n",
       "  'archive/ | wc',\n",
       "  'incorrect entries',\n",
       "  'gluster volume',\n",
       "  'node n',\n",
       "  '# ls',\n",
       "  'nfs mount',\n",
       "  'gpx1-c201',\n",
       "  'ingest',\n",
       "  'iptv',\n",
       "  'ftphomes',\n",
       "  'nas home directories',\n",
       "  'user',\n",
       "  'daemons',\n",
       "  'issue',\n",
       "  'nfslock',\n",
       "  'realy',\n",
       "  'processo rpciod com consumo elevado',\n",
       "  'cpu',\n",
       "  'quais informações você pode fornecer sobre períodos e',\n",
       "  'onde você está enfrentando esse tipo',\n",
       "  'comportamento você está enfrentando',\n",
       "  'apache está apresentando diversos processos',\n",
       "  'servidor está muito elevado',\n",
       "  'comportamento está ocorrendo',\n",
       "  'que você espera encontrar',\n",
       "  'servidor cliente nfs rodando',\n",
       "  'quando este comportamento ocorre',\n",
       "  'foi identificada falta ou contenção',\n",
       "  'linux servers',\n",
       "  'ssh',\n",
       "  'issue',\n",
       "  'multiple servers',\n",
       "  'linux servers',\n",
       "  'incorrect password',\n",
       "  'same password',\n",
       "  'multiple people',\n",
       "  'server',\n",
       "  'sporadic problems',\n",
       "  'example',\n",
       "  'console',\n",
       "  'issue',\n",
       "  'vmcore file',\n",
       "  'system',\n",
       "  'actual end customer information',\n",
       "  'customer details',\n",
       "  'l3 case',\n",
       "  'customer',\n",
       "  'vmcore file',\n",
       "  'additional details',\n",
       "  'hardware information',\n",
       "  'system model name',\n",
       "  'additional information',\n",
       "  'case',\n",
       "  'nfs overload',\n",
       "  'rpc',\n",
       "  'scan',\n",
       "  'fragment',\n",
       "  'large',\n",
       "  'nfs overload protection',\n",
       "  'nfs server',\n",
       "  'other use cases',\n",
       "  'nfs',\n",
       "  'non reproducible https://access.redhat.com/support/cases/#/case/01988040',\n",
       "  'large packets',\n",
       "  'issue similar',\n",
       "  'simple read',\n",
       "  'test environment',\n",
       "  'issue',\n",
       "  'large errors',\n",
       "  'hadoop application',\n",
       "  'issue',\n",
       "  'fragment',\n",
       "  'rpc',\n",
       "  'regulary',\n",
       "  'lrdne22epapd1i kernel',\n",
       "  'large errors',\n",
       "  'large',\n",
       "  'fragment',\n",
       "  'aug',\n",
       "  'rpc',\n",
       "  'hadoop application',\n",
       "  'issue',\n",
       "  'asap',\n",
       "  'regulary',\n",
       "  'syslog server salogp12(nfs client',\n",
       "  'various nfs mounts',\n",
       "  'issue intermittent',\n",
       "  'issues',\n",
       "  'rhel5.11',\n",
       "  'nfsv4 state recovery directory nfsd',\n",
       "  'syslog server salogp12(nfs client',\n",
       "  'various nfs mounts',\n",
       "  'server sawasp05',\n",
       "  'nfs servers',\n",
       "  'server saepap03',\n",
       "  'server sawasp07',\n",
       "  'nfs',\n",
       "  'grace period jbd',\n",
       "  'grace period rpc',\n",
       "  'nfs version',\n",
       "  'mount -t nfs epnfl',\n",
       "  'client epnfl',\n",
       "  'nfs server',\n",
       "  'epnfl',\n",
       "  'fstab entry',\n",
       "  'nfs',\n",
       "  'other servers',\n",
       "  '-t',\n",
       "  'hmsoft',\n",
       "  'q001s0:/nfsdata'],\n",
       " '797553': ['remote cifs directory',\n",
       "  'permission denied',\n",
       "  'windows server',\n",
       "  'cifs ports',\n",
       "  'auth file',\n",
       "  'server',\n",
       "  'open',\n",
       "  'service account',\n",
       "  'cifs mount',\n",
       "  'good backup',\n",
       "  'smbclient',\n",
       "  'ports',\n",
       "  'mount point issue | nt_status_account_locked_out',\n",
       "  'dbcia server',\n",
       "  'rhel',\n",
       "  'server',\n",
       "  'error nt_status_account_locked_out',\n",
       "  'directory',\n",
       "  'windows',\n",
       "  'client',\n",
       "  'mount',\n",
       "  'hslep',\n",
       "  'sosreport',\n",
       "  'os patching',\n",
       "  'cifs mounts',\n",
       "  'manual page',\n",
       "  'man mount.cifs',\n",
       "  'mount error(13',\n",
       "  'mount.cifs(8',\n",
       "  'refer',\n",
       "  'permission',\n",
       "  'cifs',\n",
       "  'linux server',\n",
       "  'nas',\n",
       "  'cifs',\n",
       "  'able',\n",
       "  'kernel error',\n",
       "  'mediante el cual podamos hacer un',\n",
       "  'favor su colaboracion con el procedimiento',\n",
       "  'del kernel sin obtener este error',\n",
       "  'por lo que el servidor comenzó',\n",
       "  'adjunto la captura del error',\n",
       "  'hicimos un upgrade de kernel',\n",
       "  'éste está detectado como una vulnerabilidad',\n",
       "  'comportamiento le está causando dificultades',\n",
       "  'y el',\n",
       "  'actualmente el',\n",
       "  'rhel 6.x servers',\n",
       "  'cifs share',\n",
       "  'unable',\n",
       "  'mount.cifs kernel mount options',\n",
       "  'rhel 6.x servers',\n",
       "  'mount issues',\n",
       "  'mount error(13',\n",
       "  'ip =',\n",
       "  'man mount.cifs',\n",
       "  'cifs shares',\n",
       "  'same share',\n",
       "  'servers',\n",
       "  'rhel',\n",
       "  'samba share',\n",
       "  'smbstatus',\n",
       "  'unable',\n",
       "  'samba share',\n",
       "  'unable',\n",
       "  'intermittent connection issue',\n",
       "  'sftp',\n",
       "  'files',\n",
       "  'intermittent connection issue',\n",
       "  'sftp',\n",
       "  'files',\n",
       "  'particular server',\n",
       "  'cifs umount',\n",
       "  'unmounted cifs mount point',\n",
       "  'password non - expiry policy',\n",
       "  'cifs mount point',\n",
       "  'nt id',\n",
       "  'form server support',\n",
       "  'unmounted',\n",
       "  'related issue',\n",
       "  'target system',\n",
       "  'pradeep kumar',\n",
       "  'business user'],\n",
       " '3415331': ['cifs mount points',\n",
       "  'nfs mount points',\n",
       "  'kernel kernel-3.10.0',\n",
       "  'available',\n",
       "  'new kernel nfs mount points',\n",
       "  'comportamiento le está causando dificultades',\n",
       "  'está presentando el comportamiento',\n",
       "  'cuándo ocurre este comportamiento',\n",
       "  'cifs mount points',\n",
       "  'qué espera ver',\n",
       "  'spmxgtwy production high priority',\n",
       "  'rx errors',\n",
       "  'tx errors',\n",
       "  'qué entorno'],\n",
       " '3446331': ['cifs bug',\n",
       "  'rhel8 server',\n",
       "  'bug',\n",
       "  'daily basis',\n",
       "  'is_size_safe_to_change',\n",
       "  'crash',\n",
       "  'cifs bug',\n",
       "  'latest available rhel',\n",
       "  'kernel',\n",
       "  'bug',\n",
       "  'rhel8 boxes',\n",
       "  'regular crashes',\n",
       "  'kerberized smb',\n",
       "  'cifs',\n",
       "  'unusable',\n",
       "  'moment',\n",
       "  'general protection fault',\n",
       "  'kernel',\n",
       "  'abrt',\n",
       "  'smp',\n",
       "  'ipt_masquerade nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_reject nf_reject_ipv4 tun bridge stp llc ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter devlink cmac',\n",
       "  'inc. vmware virtual platform/440bx desktop reference platform',\n",
       "  'crc32c_intel serio_raw libata vmw_pvscsi i2c_core floppy dm_mirror',\n",
       "  'drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm drm',\n",
       "  'iosf_mbi crc32_pclmul ghash_clmulni_intel ppdev vmw_balloon aesni_intel',\n",
       "  'ata_generic pata_acpi vmwgfx sd_mod crc_t10dif',\n",
       "  'cifs ccm',\n",
       "  'parport auth_rpcgss sunrpc ip_tables',\n",
       "  'joydev pcspkr vmw_vmci i2c_piix4',\n",
       "  'cifs'],\n",
       " '4455551': [],\n",
       " '3991451': [],\n",
       " '29894': ['sosreport',\n",
       "  'rca',\n",
       "  'root',\n",
       "  'sosreport',\n",
       "  'analysis',\n",
       "  'host',\n",
       "  'continous',\n",
       "  'initio application filesystem',\n",
       "  'directory index full',\n",
       "  'ext4 filesystem',\n",
       "  'apps block size',\n",
       "  'filesystem',\n",
       "  'device dm-40',\n",
       "  'ext4-fs warning',\n",
       "  'gftemdapai1lnp kernel',\n",
       "  'gftemdapai1lnp_vg',\n",
       "  'ifree iuse%',\n",
       "  'directory index full',\n",
       "  'error message',\n",
       "  'kernel.log',\n",
       "  'log',\n",
       "  'ext4_dx_add_entry',\n",
       "  'lot',\n",
       "  'local filesystem',\n",
       "  'same filesystem',\n",
       "  'server application',\n",
       "  'files',\n",
       "  'lot',\n",
       "  'due',\n",
       "  'exhaustion',\n",
       "  'error',\n",
       "  'ext4-fs warnings',\n",
       "  'messages',\n",
       "  'log',\n",
       "  'directory index full',\n",
       "  'pvsvr0186 kernel',\n",
       "  'ext4-fs warning',\n",
       "  'jun',\n",
       "  'device',\n",
       "  'ext4_dx_add_entry:2018',\n",
       "  'messages',\n",
       "  'log',\n",
       "  'following',\n",
       "  'server',\n",
       "  'jobs',\n",
       "  'issues',\n",
       "  'aggprd0a option',\n",
       "  'directory index full',\n",
       "  'other similar server',\n",
       "  'option',\n",
       "  'others servers',\n",
       "  'ext4-fs warning',\n",
       "  'device dm-18',\n",
       "  'aggprd0c',\n",
       "  'job',\n",
       "  'servers',\n",
       "  'directory index full',\n",
       "  'device dm-1',\n",
       "  'ext4-fs warning',\n",
       "  'kernel',\n",
       "  'directory index full',\n",
       "  'device dm-1',\n",
       "  'ext4-fs warning',\n",
       "  'kernel',\n",
       "  'miscellaneous errors',\n",
       "  'directory index',\n",
       "  'open files',\n",
       "  'large amount',\n",
       "  'system performance',\n",
       "  'support',\n",
       "  'server.we',\n",
       "  'messages',\n",
       "  'issue',\n",
       "  'log',\n",
       "  'null root',\n",
       "  'more root',\n",
       "  'csreports',\n",
       "  'bhca',\n",
       "  'bin',\n",
       "  'telstra',\n",
       "  'mm_au.exp',\n",
       "  'root@maupmlx010v log',\n",
       "  '| wc -l',\n",
       "  'system performance',\n",
       "  'device dm-5',\n",
       "  'directory index full',\n",
       "  'ext3-fs warning',\n",
       "  'node1 kernel',\n",
       "  'ext3_dx_add_entry',\n",
       "  'email',\n",
       "  'type ext3',\n",
       "  'binfmt_misc type binfmt_misc',\n",
       "  'shm type tmpfs',\n",
       "  'oracleasm type oracleasmfs',\n",
       "  'pts type devpts',\n",
       "  'type proc',\n",
       "  'type sysfs',\n",
       "  'type',\n",
       "  'rw',\n",
       "  'system mount',\n",
       "  'device error',\n",
       "  'space',\n",
       "  'directory index full',\n",
       "  'device dm-5',\n",
       "  'ext4-fs warning',\n",
       "  'enough space available',\n",
       "  'device',\n",
       "  'sensitive data',\n",
       "  'error message',\n",
       "  'ext4_dx_add_entry:2016',\n",
       "  'email_e102_processstatementserror za.co.absa.estatements.utils.emaildispatchutilities',\n",
       "  '_ from_source_cadocsf_a2_jan12.xml',\n",
       "  'm222218dbss3001 server hung',\n",
       "  'analysis',\n",
       "  'root',\n",
       "  'm222218dbss3001 hung',\n",
       "  'sosreport files',\n",
       "  'access',\n",
       "  'power',\n",
       "  'directory index full',\n",
       "  'ext4-fs warning',\n",
       "  'device',\n",
       "  'kernel',\n",
       "  'disk space available',\n",
       "  'directory index full',\n",
       "  'error message',\n",
       "  'free inode',\n",
       "  'ext4-fs warning',\n",
       "  'message file',\n",
       "  'device',\n",
       "  'many',\n",
       "  'log',\n",
       "  'kernel',\n",
       "  'directory index full',\n",
       "  'device dm-326',\n",
       "  'ext4-fs warning',\n",
       "  'directory index full',\n",
       "  'device dm-326',\n",
       "  'ext4-fs warning',\n",
       "  'directory',\n",
       "  'files',\n",
       "  'able',\n",
       "  'root@ctlinmlsrv6 chim',\n",
       "  '13.log chim',\n",
       "  '01.log chim',\n",
       "  '23.log chim',\n",
       "  '22.log chim',\n",
       "  '08.log chim',\n",
       "  '07.log chim',\n",
       "  '04.log chim',\n",
       "  '02.log chim',\n",
       "  '26.log chim',\n",
       "  'device',\n",
       "  'space',\n",
       "  'request payload',\n",
       "  'many files',\n",
       "  'space',\n",
       "  'limit',\n",
       "  'g free',\n",
       "  'wapp',\n",
       "  'vm',\n",
       "  'dataprocessing',\n",
       "  'orbs',\n",
       "  'prodcldataprocessws/10749150',\n",
       "  'directory index full',\n",
       "  'device dm-1',\n",
       "  'xt4-fs warning',\n",
       "  'messages',\n",
       "  'log',\n",
       "  'below message',\n",
       "  'lots',\n",
       "  'directory index full',\n",
       "  'directory index full',\n",
       "  'device dm-1',\n",
       "  'ext4-fs warning',\n",
       "  'kernel',\n",
       "  'directory index full',\n",
       "  'ext4-fs warning',\n",
       "  'server logs',\n",
       "  'device',\n",
       "  'messages',\n",
       "  'kernel',\n",
       "  'constant',\n",
       "  'directory index full',\n",
       "  'ext4-fs warning',\n",
       "  'server logs',\n",
       "  'device',\n",
       "  'messages',\n",
       "  'kernel',\n",
       "  'lot',\n",
       "  'directory index full',\n",
       "  'device dm-84',\n",
       "  'ext3-fs warning',\n",
       "  'lrdna1gx kernel',\n",
       "  'ext3_dx_add_entry',\n",
       "  'error',\n",
       "  'server',\n",
       "  'nov',\n",
       "  'directory index full',\n",
       "  'device dm-84',\n",
       "  'ext3-fs warning',\n",
       "  'lrdna1gx kernel',\n",
       "  'ext3_dx_add_entry',\n",
       "  'server',\n",
       "  'error',\n",
       "  'disk',\n",
       "  'nov',\n",
       "  'use',\n",
       "  'crashed',\n",
       "  'auto crash',\n",
       "  'cause',\n",
       "  'frequent kernel warning message',\n",
       "  'mount point',\n",
       "  'directory',\n",
       "  'full',\n",
       "  'solution',\n",
       "  'issue',\n",
       "  'kernel issue',\n",
       "  'directory index full',\n",
       "  'kernel log',\n",
       "  'kernel',\n",
       "  'device dm-7',\n",
       "  'overall system',\n",
       "  'ext4-fs warning',\n",
       "  'issue',\n",
       "  'production',\n",
       "  'feedback',\n",
       "  'nfs issue',\n",
       "  'mbps full duplex apr',\n",
       "  'service ok apr',\n",
       "  'kernel',\n",
       "  '_ mutex_lock_slowpath+0x96/0x210 apr',\n",
       "  'tx apr',\n",
       "  'icms',\n",
       "  'kthread+0x9e/0xc0 apr',\n",
       "  'mutex_lock+0x2b/0x50 apr',\n",
       "  'd_obtain_alias+0xc0/0x230 apr',\n",
       "  'apr',\n",
       "  'server got',\n",
       "  'lrau1p17',\n",
       "  'high priority',\n",
       "  'oct',\n",
       "  '8th',\n",
       "  'got',\n",
       "  'reason',\n",
       "  'hrs',\n",
       "  'lrau1p17',\n",
       "  'server',\n",
       "  'reboot',\n",
       "  'regular file',\n",
       "  'pub',\n",
       "  'archive',\n",
       "  'html',\n",
       "  'www',\n",
       "  '03:23:34.html.gz',\n",
       "  'audit',\n",
       "  'space',\n",
       "  'device',\n",
       "  'mv',\n",
       "  'regular file',\n",
       "  'enough space',\n",
       "  'file',\n",
       "  'pub',\n",
       "  'space',\n",
       "  'archive',\n",
       "  'html',\n",
       "  'www',\n",
       "  '03:23:34.html.gz',\n",
       "  'audit',\n",
       "  'directory index full',\n",
       "  'device dm-4',\n",
       "  'ext4-fs warning',\n",
       "  'kernel',\n",
       "  'solution doc https://access.redhat.com/solutions/29894',\n",
       "  'directory index full',\n",
       "  'device dm-4',\n",
       "  'ext4-fs warning',\n",
       "  'file system',\n",
       "  'kernel',\n",
       "  'logs',\n",
       "  'reformat',\n",
       "  'errors',\n",
       "  'os panic reboot',\n",
       "  'vmcore',\n",
       "  'machine',\n",
       "  'dayrhedsgp001',\n",
       "  'device',\n",
       "  'space',\n",
       "  'directory index full',\n",
       "  'device dm-71',\n",
       "  'below error message',\n",
       "  'inode space',\n",
       "  'ext3-fs warning',\n",
       "  'device',\n",
       "  'production jobs',\n",
       "  'space',\n",
       "  'available',\n",
       "  'log',\n",
       "  'hardware error',\n",
       "  'performance issue',\n",
       "  'dmesg',\n",
       "  'machine check events',\n",
       "  'hardware error',\n",
       "  'root@pruswipprodb1',\n",
       "  'dvice error',\n",
       "  'server',\n",
       "  'error',\n",
       "  'directory index full',\n",
       "  'device dm-2',\n",
       "  'ext4-fs warning',\n",
       "  'ext4_dx_add_entry:2021',\n",
       "  'directory index full',\n",
       "  'device dm-2',\n",
       "  'ext4-fs warning',\n",
       "  'psrgisdb02 kernel',\n",
       "  'ext4_dx_add_entry:2021',\n",
       "  'kernel',\n",
       "  'psrgisdb02',\n",
       "  'apr',\n",
       "  'os daemons',\n",
       "  'application processes',\n",
       "  'gpnuatnap01',\n",
       "  'application processes',\n",
       "  'daemons',\n",
       "  'messages file',\n",
       "  'directory index full',\n",
       "  'directory index',\n",
       "  'ext4-fs warning',\n",
       "  'error logs',\n",
       "  'wupra00a0219 kernel',\n",
       "  'device dm-3',\n",
       "  'directory',\n",
       "  'full',\n",
       "  'messages file',\n",
       "  'ec',\n",
       "  'device',\n",
       "  'space',\n",
       "  'storage partition',\n",
       "  'prod server',\n",
       "  'clean_up_folder',\n",
       "  'documents',\n",
       "  'error',\n",
       "  'documents/',\n",
       "  'home/6182079',\n",
       "  'space',\n",
       "  'files',\n",
       "  'device',\n",
       "  'rca',\n",
       "  'dbblx07c-05',\n",
       "  'dfw',\n",
       "  'production oracle database',\n",
       "  'rca',\n",
       "  'server',\n",
       "  'dbblx07c-05',\n",
       "  'dfw',\n",
       "  'oracle database server',\n",
       "  'file systems',\n",
       "  'orphaned inodes',\n",
       "  'screen shot',\n",
       "  'clean',\n",
       "  'journal',\n",
       "  'server',\n",
       "  'directory',\n",
       "  'file',\n",
       "  'unable',\n",
       "  'contact number',\n",
       "  'ext4 filesystem',\n",
       "  '+1',\n",
       "  'unable',\n",
       "  'directory',\n",
       "  'file',\n",
       "  'top left corner',\n",
       "  'black screen',\n",
       "  'cursor',\n",
       "  'server',\n",
       "  'server reboot os',\n",
       "  'directory index full',\n",
       "  'device dm-57',\n",
       "  'error_ext4-fs warning',\n",
       "  'multiple error message',\n",
       "  'problem statement',\n",
       "  'attached sosreport',\n",
       "  'critical server',\n",
       "  'team',\n",
       "  'application server',\n",
       "  'performance issue',\n",
       "  'high run queue',\n",
       "  'vmstat log',\n",
       "  'vmstat',\n",
       "  'queue value',\n",
       "  'oracle team',\n",
       "  'oracle weblogic',\n",
       "  'high',\n",
       "  'queue',\n",
       "  'application server',\n",
       "  'procs -----------memory----------',\n",
       "  'directory index full',\n",
       "  'ext3_dx_add_entry',\n",
       "  'warning',\n",
       "  'dm-126',\n",
       "  'ext3-fs',\n",
       "  'directory index full',\n",
       "  'same error message',\n",
       "  'node oracle dbase cluster',\n",
       "  'same hitachi array',\n",
       "  'dal',\n",
       "  'ext3_dx_add_entry',\n",
       "  'ext3-fs',\n",
       "  'kernel',\n",
       "  'warning',\n",
       "  'dm-304',\n",
       "  'dcaldd145 server',\n",
       "  'able',\n",
       "  'application',\n",
       "  'core dump report',\n",
       "  'dcaldd145 server',\n",
       "  'rca',\n",
       "  'analyse',\n",
       "  'able',\n",
       "  'application',\n",
       "  'issue',\n",
       "  'tengo mas',\n",
       "  'libres',\n",
       "  'nos muestra error',\n",
       "  'hay mas',\n",
       "  'libres',\n",
       "  'que',\n",
       "  'server nzxpdb451 database',\n",
       "  'directory index full',\n",
       "  'ext4-fs warning',\n",
       "  'kernel',\n",
       "  'nzxpdb451',\n",
       "  'mar',\n",
       "  'device',\n",
       "  'error',\n",
       "  'ext4 file system',\n",
       "  'message',\n",
       "  'device dm-2',\n",
       "  'directory index full',\n",
       "  'ext4-fs warning',\n",
       "  'server',\n",
       "  'message',\n",
       "  'unresponsive',\n",
       "  'systems',\n",
       "  'regular system commands',\n",
       "  'several minutes',\n",
       "  'minutes',\n",
       "  'root',\n",
       "  'unresponsive',\n",
       "  'systems',\n",
       "  'o slowness',\n",
       "  'performance issue',\n",
       "  'lrau1p17',\n",
       "  'i',\n",
       "  'lrau1p18',\n",
       "  'server response slow',\n",
       "  'high i',\n",
       "  'directory index full',\n",
       "  'backup',\n",
       "  'has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super huge_file',\n",
       "  'user_xattr acl filesystem state',\n",
       "  'flex block group size',\n",
       "  'lhm filesystem uuid',\n",
       "  'tune2fs output error system ewookkop001 root@ewookkop001',\n",
       "  'filesystem magic number',\n",
       "  'filesystem os type',\n",
       "  'first meta block group',\n",
       "  'filesystem volume name',\n",
       "  'blocks und größen durchgang',\n",
       "  'samba service',\n",
       "  'port',\n",
       "  'directory index full message',\n",
       "  'directory index full',\n",
       "  'vvsl20315 kernel',\n",
       "  'device dm-3',\n",
       "  'ext4-fs warning',\n",
       "  'sep',\n",
       "  'logs',\n",
       "  'directory index full',\n",
       "  'device dm-5',\n",
       "  'ext4-fs warning',\n",
       "  'ext4_dx_add_entry:2018',\n",
       "  'erro',\n",
       "  'banco de dados está recebendo erro index full',\n",
       "  'comportamento você está enfrentando',\n",
       "  'que você espera encontrar',\n",
       "  'data',\n",
       "  'que tipo',\n",
       "  'df -i',\n",
       "  'dse',\n",
       "  'apliacação',\n",
       "  'df',\n",
       "  'sibxp0022vg',\n",
       "  'file system space issue',\n",
       "  'recent problem',\n",
       "  'generic space issue message',\n",
       "  'directory index full',\n",
       "  'redhat doc https://access.redhat.com/solutions/29894',\n",
       "  'immediate issue',\n",
       "  'system log',\n",
       "  'ext3-fs warning',\n",
       "  'issue',\n",
       "  'large count',\n",
       "  'device dm-2',\n",
       "  'report generations',\n",
       "  'device',\n",
       "  'space',\n",
       "  'rsync',\n",
       "  'enough space',\n",
       "  'destination server',\n",
       "  'space',\n",
       "  'device',\n",
       "  'rsync',\n",
       "  'reason',\n",
       "  'directory index full',\n",
       "  'device dm-4',\n",
       "  'ext3-fs warning',\n",
       "  'ext3_dx_add_entry',\n",
       "  'kernel',\n",
       "  'directory index full',\n",
       "  'device dm-4',\n",
       "  'ext3-fs warning',\n",
       "  'ext3_dx_add_entry',\n",
       "  'same',\n",
       "  'kernel',\n",
       "  'messages',\n",
       "  'error',\n",
       "  'ext4-fs directory index full',\n",
       "  'need root cause',\n",
       "  'time',\n",
       "  'warnings',\n",
       "  'ext4-fs directory index full',\n",
       "  'need root cause',\n",
       "  'time',\n",
       "  'warnings',\n",
       "  'high memory utilization',\n",
       "  'server',\n",
       "  'high memory utilization',\n",
       "  'server',\n",
       "  'directory index full',\n",
       "  'device dm-3',\n",
       "  'messages logs',\n",
       "  'warning',\n",
       "  'directory index full',\n",
       "  'device dm-3',\n",
       "  'disk path dm-3',\n",
       "  'ext4-fs warning',\n",
       "  'long while',\n",
       "  'long filenames',\n",
       "  'app team',\n",
       "  'errors',\n",
       "  'kernel',\n",
       "  'way',\n",
       "  'directory index full',\n",
       "  'ext4-fs warning',\n",
       "  'device',\n",
       "  'kernel',\n",
       "  'betapmdmz4',\n",
       "  'mar',\n",
       "  'directory index full',\n",
       "  'warning messages',\n",
       "  'filesystem',\n",
       "  'dirs',\n",
       "  'files',\n",
       "  'files',\n",
       "  'subfolders',\n",
       "  'error',\n",
       "  'space',\n",
       "  'stress test last weekend',\n",
       "  'k subfolders',\n",
       "  'enough space',\n",
       "  'single directory',\n",
       "  'k folders',\n",
       "  'files',\n",
       "  'space',\n",
       "  'subfolders',\n",
       "  'unstable',\n",
       "  'error']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the match percentage between 'new' cases and rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KCS</th>\n",
       "      <th>case key phrases number</th>\n",
       "      <th>case matched tags number</th>\n",
       "      <th>rule tags number</th>\n",
       "      <th>hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>740323</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740323</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>740323</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>144</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>740323</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>740323</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24651</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>0.028409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24651</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>176</td>\n",
       "      <td>0.017045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24651</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24651</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24651</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "      <td>0.005682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24651</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1614393</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>259</td>\n",
       "      <td>0.015444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1614393</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>259</td>\n",
       "      <td>0.019305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1614393</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>259</td>\n",
       "      <td>0.011583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1614393</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>259</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1614393</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>259</td>\n",
       "      <td>0.007722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1614393</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>0.003861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2065483</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>376</td>\n",
       "      <td>0.013298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2065483</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>376</td>\n",
       "      <td>0.007979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2065483</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>376</td>\n",
       "      <td>0.010638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2065483</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>376</td>\n",
       "      <td>0.010638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2065483</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>376</td>\n",
       "      <td>0.013298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2065483</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>376</td>\n",
       "      <td>0.007979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2065483</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>376</td>\n",
       "      <td>0.015957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2065483</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>376</td>\n",
       "      <td>0.015957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2065483</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>376</td>\n",
       "      <td>0.010638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2065483</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>0.002660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>797553</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0.012048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>797553</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>797553</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3415331</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3446331</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3991451</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>29894</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>578</td>\n",
       "      <td>0.017301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>29894</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>578</td>\n",
       "      <td>0.005190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>29894</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>578</td>\n",
       "      <td>0.003460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>29894</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>578</td>\n",
       "      <td>0.008651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>29894</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>578</td>\n",
       "      <td>0.006920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>29894</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>578</td>\n",
       "      <td>0.001730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>29894</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>578</td>\n",
       "      <td>0.001730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>29894</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>578</td>\n",
       "      <td>0.003460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>29894</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>578</td>\n",
       "      <td>0.010381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>29894</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>578</td>\n",
       "      <td>0.010381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>29894</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>578</td>\n",
       "      <td>0.008651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>29894</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>578</td>\n",
       "      <td>0.008651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>29894</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>578</td>\n",
       "      <td>0.012111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>29894</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>578</td>\n",
       "      <td>0.003460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>29894</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>578</td>\n",
       "      <td>0.001730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        KCS  case key phrases number  case matched tags number  \\\n",
       "0    740323                        8                         1   \n",
       "1    740323                        3                         1   \n",
       "2    740323                       13                         6   \n",
       "3    740323                       12                         0   \n",
       "4    740323                       15                         2   \n",
       "5     24651                       13                         5   \n",
       "6     24651                       12                         3   \n",
       "7     24651                       13                         2   \n",
       "8     24651                       11                         2   \n",
       "9     24651                        5                         1   \n",
       "10    24651                       12                         0   \n",
       "11  1614393                       12                         4   \n",
       "12  1614393                       10                         5   \n",
       "13  1614393                       13                         3   \n",
       "14  1614393                       14                         7   \n",
       "15  1614393                        6                         2   \n",
       "16  1614393                        3                         1   \n",
       "17  2065483                       14                         5   \n",
       "18  2065483                       14                         3   \n",
       "19  2065483                       15                         4   \n",
       "20  2065483                       12                         4   \n",
       "21  2065483                       14                         5   \n",
       "22  2065483                       12                         3   \n",
       "23  2065483                       13                         6   \n",
       "24  2065483                       13                         6   \n",
       "25  2065483                        8                         4   \n",
       "26  2065483                        5                         1   \n",
       "27   797553                        8                         1   \n",
       "28   797553                        3                         0   \n",
       "29   797553                       12                         0   \n",
       "30  3415331                       12                         0   \n",
       "31  3446331                       13                         0   \n",
       "32  3991451                       13                         0   \n",
       "33    29894                       19                        10   \n",
       "34    29894                       14                         3   \n",
       "35    29894                        3                         2   \n",
       "36    29894                       14                         5   \n",
       "37    29894                       13                         4   \n",
       "38    29894                       11                         1   \n",
       "39    29894                       10                         1   \n",
       "40    29894                        5                         2   \n",
       "41    29894                       11                         6   \n",
       "42    29894                       12                         6   \n",
       "43    29894                       11                         5   \n",
       "44    29894                       14                         5   \n",
       "45    29894                       15                         7   \n",
       "46    29894                       11                         2   \n",
       "47    29894                        5                         1   \n",
       "\n",
       "    rule tags number  hit_rate  \n",
       "0                144  0.006944  \n",
       "1                144  0.006944  \n",
       "2                144  0.041667  \n",
       "3                144  0.000000  \n",
       "4                144  0.013889  \n",
       "5                176  0.028409  \n",
       "6                176  0.017045  \n",
       "7                176  0.011364  \n",
       "8                176  0.011364  \n",
       "9                176  0.005682  \n",
       "10               176  0.000000  \n",
       "11               259  0.015444  \n",
       "12               259  0.019305  \n",
       "13               259  0.011583  \n",
       "14               259  0.027027  \n",
       "15               259  0.007722  \n",
       "16               259  0.003861  \n",
       "17               376  0.013298  \n",
       "18               376  0.007979  \n",
       "19               376  0.010638  \n",
       "20               376  0.010638  \n",
       "21               376  0.013298  \n",
       "22               376  0.007979  \n",
       "23               376  0.015957  \n",
       "24               376  0.015957  \n",
       "25               376  0.010638  \n",
       "26               376  0.002660  \n",
       "27                83  0.012048  \n",
       "28                83  0.000000  \n",
       "29                83  0.000000  \n",
       "30                14  0.000000  \n",
       "31                30  0.000000  \n",
       "32                 0  0.000000  \n",
       "33               578  0.017301  \n",
       "34               578  0.005190  \n",
       "35               578  0.003460  \n",
       "36               578  0.008651  \n",
       "37               578  0.006920  \n",
       "38               578  0.001730  \n",
       "39               578  0.001730  \n",
       "40               578  0.003460  \n",
       "41               578  0.010381  \n",
       "42               578  0.010381  \n",
       "43               578  0.008651  \n",
       "44               578  0.008651  \n",
       "45               578  0.012111  \n",
       "46               578  0.003460  \n",
       "47               578  0.001730  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = []\n",
    "for key in kcs_rule_dict.keys():\n",
    "    temp_df = df[df['resource_display_id__c'] == key]\n",
    "    case_count = temp_df.shape[0]\n",
    "    train_count = int(case_count * (4/5))\n",
    "    temp_test_df = temp_df[train_count:]\n",
    "    for description_phrases in temp_test_df['description_key_phrases']:\n",
    "        performance.append({'KCS':key, 'case key phrases number': len(description_phrases), 'case matched tags number': len(list(set(description_phrases) & set(rule_tag_dict[key]))), 'rule tags number': len(rule_tag_dict[key])})\n",
    "      \n",
    "performance_df = pd.DataFrame(performance)\n",
    "def calculate_hit_rate(row):\n",
    "    if row['rule tags number'] == 0:\n",
    "        return 0\n",
    "    return row['case matched tags number']/row['rule tags number']\n",
    "    \n",
    "performance_df['hit_rate'] = performance_df.apply(calculate_hit_rate, axis=1)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The average match percentage is close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009440565418467538"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df['hit_rate'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041666666666666664"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df['hit_rate'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
